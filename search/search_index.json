{"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\-]","pipeline":["stemmer"]},"docs":[{"location":"","title":"DanLing","text":""},{"location":"#introduction","title":"Introduction","text":"<p>DanLing (\u200b\u4e39\u7075\u200b) is a high-level library to help with running neural networks flexibly and transparently.</p> <p>DanLing is meant to be a scaffold for experienced researchers and engineers who know how to define a training loop, but are bored of writing the same boilerplate code, such as DDP, logging, checkpointing, etc., over and over again.</p> <p>Therefore, DanLing does not feature complex Runner designs with many pre-defined methods and complicated hooks. Instead, the Runner of DanLing just initialise the essential parts for you, and you can do whatever you want, however you want.</p> <p>Although many attributes and properties are pre-defined and are expected to be used in DanLing, you have full control over your code.</p> <p>DanLing also provides some utilities, such as <code>NestedTensor</code>, <code>LRScheduler</code>, <code>catch</code>, etc.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the most recent stable version on pypi:</p> Bash<pre><code>pip install danling\n</code></pre> <p>Install the latest version from source:</p> Bash<pre><code>pip install git+https://github.com/ZhiyuanChen/DanLing\n</code></pre> <p>It works the way it should have worked.</p>"},{"location":"#license","title":"License","text":"<p>DanLing is multi-licensed under the following licenses:</p> <ul> <li>The Unlicense</li> <li>GNU Affero General Public License v3.0 or later</li> <li>GNU General Public License v2.0 or later</li> <li>BSD 4-Clause \u201cOriginal\u201d or \u201cOld\u201d License</li> <li>MIT License</li> <li>Apache License 2.0</li> </ul> <p>You can choose any (one or more) of these licenses if you use this work.</p> <p><code>SPDX-License-Identifier: Unlicense OR AGPL-3.0-or-later OR GPL-2.0-or-later OR BSD-4-Clause OR MIT OR Apache-2.0</code></p>"},{"location":"package/","title":"DanLing","text":""},{"location":"package/#danling.AverageMeter","title":"<code>AverageMeter</code>","text":"<p>Computes and stores the average and current value.</p> <p>Attributes:</p> Name Type Description <code>val</code> <code>float</code> <p>Current value.</p> <code>avg</code> <code>float</code> <p>Average value.</p> <code>sum</code> <code>float</code> <p>Sum of values.</p> <code>count</code> <code>int</code> <p>Number of values.</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>class AverageMeter:\n    r\"\"\"\n    Computes and stores the average and current value.\n\n    Attributes:\n        val: Current value.\n        avg: Average value.\n        sum: Sum of values.\n        count: Number of values.\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    val: float = 0\n    avg: float = 0\n    sum: float = 0\n    count: int = 0\n\n    def __init__(self) -&gt; None:\n        self.reset()\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets the meter.\n\n        Examples:\n            &gt;&gt;&gt; meter = AverageMeter()\n            &gt;&gt;&gt; meter.update(0.7)\n            &gt;&gt;&gt; meter.val\n            0.7\n            &gt;&gt;&gt; meter.avg\n            0.7\n            &gt;&gt;&gt; meter.reset()\n            &gt;&gt;&gt; meter.val\n            0\n            &gt;&gt;&gt; meter.avg\n            0\n        \"\"\"\n\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n: int = 1) -&gt; None:\n        r\"\"\"\n        Updates the average and current value in the meter.\n\n        Args:\n            val: Value to be added to the average.\n            n: Number of values to be added.\n\n        Examples:\n            &gt;&gt;&gt; meter = AverageMeter()\n            &gt;&gt;&gt; meter.update(0.7)\n            &gt;&gt;&gt; meter.val\n            0.7\n            &gt;&gt;&gt; meter.avg\n            0.7\n            &gt;&gt;&gt; meter.update(0.9)\n            &gt;&gt;&gt; meter.val\n            0.9\n            &gt;&gt;&gt; meter.avg\n            0.8\n            &gt;&gt;&gt; meter.sum\n            1.6\n            &gt;&gt;&gt; meter.count\n            2\n        \"\"\"\n\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __format__(self, format_spec) -&gt; str:\n        return f\"{self.val.__format__(format_spec)} ({self.avg.__format__(format_spec)})\"\n</code></pre>"},{"location":"package/#danling.AverageMeter.reset","title":"<code>reset()</code>","text":"<p>Resets the meter.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets the meter.\n\n    Examples:\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        0\n    \"\"\"\n\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n</code></pre>"},{"location":"package/#danling.AverageMeter.update","title":"<code>update(val, n=1)</code>","text":"<p>Updates the average and current value in the meter.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <p>Value to be added to the average.</p> required <code>n</code> <code>int</code> <p>Number of values to be added.</p> <code>1</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>def update(self, val, n: int = 1) -&gt; None:\n    r\"\"\"\n    Updates the average and current value in the meter.\n\n    Args:\n        val: Value to be added to the average.\n        n: Number of values to be added.\n\n    Examples:\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n    \"\"\"\n\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n</code></pre>"},{"location":"package/#danling.AverageMeters","title":"<code>AverageMeters</code>","text":"<p>             Bases: <code>DefaultDict</code></p> <p>A <code>DefaultDict</code> for <code>AverageMeter</code>.</p> <p>Examples: Python<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.loss.reset()\n&gt;&gt;&gt; meters.loss.update(0.7)\n&gt;&gt;&gt; meters.loss.val\n0.7\n&gt;&gt;&gt; meters.loss.avg\n0.7\n&gt;&gt;&gt; meters.update(0.9)\n&gt;&gt;&gt; meters.loss.val\n0.9\n&gt;&gt;&gt; meters.loss.avg\n0.8\n&gt;&gt;&gt; meters.loss.sum\n1.6\n&gt;&gt;&gt; meters.loss.count\n2\n&gt;&gt;&gt; meters.reset()\n&gt;&gt;&gt; meters.loss.val\n0\n&gt;&gt;&gt; meters.loss.avg\n0\n</code></pre></p> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>class AverageMeters(DefaultDict):\n    r\"\"\"\n    A `DefaultDict` for `AverageMeter`.\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; meters = AverageMeters()\n    &gt;&gt;&gt; meters.loss.reset()\n    &gt;&gt;&gt; meters.loss.update(0.7)\n    &gt;&gt;&gt; meters.loss.val\n    0.7\n    &gt;&gt;&gt; meters.loss.avg\n    0.7\n    &gt;&gt;&gt; meters.update(0.9)\n    &gt;&gt;&gt; meters.loss.val\n    0.9\n    &gt;&gt;&gt; meters.loss.avg\n    0.8\n    &gt;&gt;&gt; meters.loss.sum\n    1.6\n    &gt;&gt;&gt; meters.loss.count\n    2\n    &gt;&gt;&gt; meters.reset()\n    &gt;&gt;&gt; meters.loss.val\n    0\n    &gt;&gt;&gt; meters.loss.avg\n    0\n\n    ```\n    \"\"\"\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        kwargs.setdefault(\"default_factory\", AverageMeter)\n        super().__init__(*args, **kwargs)\n\n    @property\n    def val(self) -&gt; NestedDict[str, float]:\n        return NestedDict({key: meter.val for key, meter in self.items()})\n\n    @property\n    def avg(self) -&gt; NestedDict[str, float]:\n        return NestedDict({key: meter.avg for key, meter in self.items()})\n\n    @property\n    def sum(self) -&gt; NestedDict[str, float]:\n        return NestedDict({key: meter.sum for key, meter in self.items()})\n\n    @property\n    def count(self) -&gt; NestedDict[str, int]:\n        return NestedDict({key: meter.count for key, meter in self.items()})\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets all meters.\n\n        Examples:\n            &gt;&gt;&gt; meters = AverageMeters()\n            &gt;&gt;&gt; meters.loss.update(0.7)\n            &gt;&gt;&gt; meters.loss.val\n            0.7\n            &gt;&gt;&gt; meters.loss.avg\n            0.7\n            &gt;&gt;&gt; meters.reset()\n            &gt;&gt;&gt; meters.loss.val\n            0\n            &gt;&gt;&gt; meters.loss.avg\n            0\n        \"\"\"\n\n        for meter in self.values():\n            meter.reset()\n\n    def update(self, val, n: int = 1) -&gt; None:  # pylint: disable=W0237\n        r\"\"\"\n        Updates the average and current value in all meters.\n\n        Args:\n            val: Value to be added to the average.\n            n: Number of values to be added.\n\n        Note:\n            This function is **NOT** recommended to use, as it alters all meters in the bank.\n\n        Examples:\n            &gt;&gt;&gt; meters = AverageMeters()\n            &gt;&gt;&gt; meters.loss.update(0.7)\n            &gt;&gt;&gt; meters.loss.val\n            0.7\n            &gt;&gt;&gt; meters.loss.avg\n            0.7\n            &gt;&gt;&gt; meters.update(0.9)\n            &gt;&gt;&gt; meters.loss.val\n            0.9\n            &gt;&gt;&gt; meters.loss.avg\n            0.8\n            &gt;&gt;&gt; meters.loss.sum\n            1.6\n            &gt;&gt;&gt; meters.loss.count\n            2\n        \"\"\"\n\n        for meter in self.values():\n            meter.update(val, n)\n\n    def __format__(self, format_spec) -&gt; str:\n        return \"\\n\".join(f\"{key}: {meter.__format__(format_spec)}\" for key, meter in self.items())\n</code></pre>"},{"location":"package/#danling.AverageMeters.reset","title":"<code>reset()</code>","text":"<p>Resets all meters.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.loss.update(0.7)\n&gt;&gt;&gt; meters.loss.val\n0.7\n&gt;&gt;&gt; meters.loss.avg\n0.7\n&gt;&gt;&gt; meters.reset()\n&gt;&gt;&gt; meters.loss.val\n0\n&gt;&gt;&gt; meters.loss.avg\n0\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets all meters.\n\n    Examples:\n        &gt;&gt;&gt; meters = AverageMeters()\n        &gt;&gt;&gt; meters.loss.update(0.7)\n        &gt;&gt;&gt; meters.loss.val\n        0.7\n        &gt;&gt;&gt; meters.loss.avg\n        0.7\n        &gt;&gt;&gt; meters.reset()\n        &gt;&gt;&gt; meters.loss.val\n        0\n        &gt;&gt;&gt; meters.loss.avg\n        0\n    \"\"\"\n\n    for meter in self.values():\n        meter.reset()\n</code></pre>"},{"location":"package/#danling.AverageMeters.update","title":"<code>update(val, n=1)</code>","text":"<p>Updates the average and current value in all meters.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <p>Value to be added to the average.</p> required <code>n</code> <code>int</code> <p>Number of values to be added.</p> <code>1</code> Note <p>This function is NOT recommended to use, as it alters all meters in the bank.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.loss.update(0.7)\n&gt;&gt;&gt; meters.loss.val\n0.7\n&gt;&gt;&gt; meters.loss.avg\n0.7\n&gt;&gt;&gt; meters.update(0.9)\n&gt;&gt;&gt; meters.loss.val\n0.9\n&gt;&gt;&gt; meters.loss.avg\n0.8\n&gt;&gt;&gt; meters.loss.sum\n1.6\n&gt;&gt;&gt; meters.loss.count\n2\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>def update(self, val, n: int = 1) -&gt; None:  # pylint: disable=W0237\n    r\"\"\"\n    Updates the average and current value in all meters.\n\n    Args:\n        val: Value to be added to the average.\n        n: Number of values to be added.\n\n    Note:\n        This function is **NOT** recommended to use, as it alters all meters in the bank.\n\n    Examples:\n        &gt;&gt;&gt; meters = AverageMeters()\n        &gt;&gt;&gt; meters.loss.update(0.7)\n        &gt;&gt;&gt; meters.loss.val\n        0.7\n        &gt;&gt;&gt; meters.loss.avg\n        0.7\n        &gt;&gt;&gt; meters.update(0.9)\n        &gt;&gt;&gt; meters.loss.val\n        0.9\n        &gt;&gt;&gt; meters.loss.avg\n        0.8\n        &gt;&gt;&gt; meters.loss.sum\n        1.6\n        &gt;&gt;&gt; meters.loss.count\n        2\n    \"\"\"\n\n    for meter in self.values():\n        meter.update(val, n)\n</code></pre>"},{"location":"package/#danling.BaseRunner","title":"<code>BaseRunner</code>","text":"<p>Base class for all runners.</p> <p><code>BaseRunner</code> sets up basic running environment, including <code>seed</code>, <code>deterministic</code>, and <code>logging</code>.</p> <p><code>BaseRunner</code> also provides some basic methods, such as, <code>step</code>, <code>state_dict</code>, <code>save_checkpoint</code>, <code>load_checkpoint</code>.</p> <p><code>BaseRunner</code> defines all basic attributes and relevant properties such as <code>scores</code>, <code>progress</code>, etc.</p> <p>Core:</p> Name Type Description <code>mode</code> <code>(RunnerMode, property)</code> <p>Running mode.</p> <code>state</code> <code>RunnerState</code> <p>Running state. See <code>RunnerState</code> for details.</p> <p>Model:</p> Name Type Description <code>model</code> <code>Callable</code> <code>criterion</code> <code>Callable</code> <code>optimizer</code> <code>Any | None</code> <code>scheduler</code> <code>Any | None</code> <p>Data:</p> Name Type Description <code>datasets</code> <code>FlatDict</code> <p>All datasets, should be in the form of <code>{subset: dataset}</code>. Initialised to <code>FlatDict</code> by default.</p> <code>datasamplers</code> <code>FlatDict</code> <p>All datasamplers, should be in the form of <code>{subset: datasampler}</code>. Initialised to <code>FlatDict</code> by default.</p> <code>dataloaders</code> <code>FlatDict</code> <p>All dataloaders, should be in the form of <code>{subset: dataloader}</code>. Initialised to <code>FlatDict</code> by default.</p> <code>batch_size</code> <code>(int, property)</code> <p>Number of samples per batch in train dataloader or the first dataloader.</p> <code>batch_size_equivalent</code> <code>(int, property)</code> <p>Total batch_size (<code>batch_size * world_size * accum_steps</code>).</p> <p><code>datasets</code>, <code>datasamplers</code>, <code>dataloaders</code> should be a dict with the same keys. Their keys should be <code>split</code> (e.g. <code>train</code>, <code>val</code>, <code>test</code>).</p> <p>Progress:</p> Name Type Description <code>progress</code> <code>(float, property)</code> <p>Running Progress, in <code>range(0, 1)</code>.</p> <p>Results:</p> Name Type Description <code>results</code> <code>NestedDict</code> <p>Results include all metric information of the model. Results should be in the form of <code>{epoch: {subset: {metric: score}}}</code>.</p> <code>latest_result</code> <code>(NestedDict, property)</code> <p>Most recent result, should be in the form of <code>{subset: {metric: score}}</code>.</p> <code>best_result</code> <code>(NestedDict, property)</code> <p>Best result, should be in the form of <code>{subset: {metric: score}}</code>.</p> <code>scores</code> <code>(List[float], property)</code> <p>Score is the core metric that is used to evaluate the performance of the model. Scores should be in the form of <code>{epoch: score}</code>.</p> <code>latest_score</code> <code>(float, property)</code> <p>Most recent score, should be in the form of <code>score</code>.</p> <code>best_score</code> <code>(float, property)</code> <p>Best score, should be in the form of <code>score</code>.</p> <code>score_set</code> <code>Optional[str]</code> <p>The subset to calculate the score. If is <code>None</code>, will use the last set of the result.</p> <code>score_name</code> <code>str</code> <p>The metric name of score. Defaults to <code>\"loss\"</code>.</p> <code>is_best</code> <code>(bool, property)</code> <p>If <code>latest_score == best_score</code>.</p> <p>IO:</p> Name Type Description <code>dir</code> <code>(str, property)</code> <p>Directory of the run. Defaults to <code>os.path.join(self.project_root, f\"{self.name}-{self.id}\")</code>.</p> <code>checkpoint_dir</code> <code>(str, property)</code> <p>Directory of checkpoints.</p> <code>log_path</code> <code>(str, property)</code> <p>Path of log file.</p> <code>checkpoint_dir_name</code> <code>str</code> <p>The name of the directory under <code>runner.dir</code> to save checkpoints. Defaults to <code>\"checkpoints\"</code>.</p> <p>Parallel Training:</p> Name Type Description <code>world_size</code> <code>(int, property)</code> <p>Number of processes.</p> <code>rank</code> <code>(int, property)</code> <p>Process index of all processes.</p> <code>local_rank</code> <code>(int, property)</code> <p>Process index of local processes.</p> <code>distributed</code> <code>(bool, property)</code> <p>If runner is running in distributed mode.</p> <code>is_main_process</code> <code>(bool, property)</code> <p>If current process is the main process of all processes.</p> <code>is_local_main_process</code> <code>(bool, property)</code> <p>If current process is the main process of local processes.</p> <p>logging:</p> Name Type Description <code>meters</code> <code>AverageMeters</code> <p>Average meters. Initialised to <code>AverageMeters</code> by default.</p> <code>metrics</code> <code>Metrics</code> <p>Metrics for evaluating.</p> <code>logger</code> <code>Logger | None</code> <code>writer</code> <code>Any | None</code> See Also <p><code>RunnerState</code>: The runeer base that stores runtime information. <code>BaseRunner</code>: The base runner class.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>class BaseRunner(metaclass=RunnerMeta):  # pylint: disable=too-many-public-methods\n    r\"\"\"\n    Base class for all runners.\n\n    `BaseRunner` sets up basic running environment, including `seed`, `deterministic`, and `logging`.\n\n    `BaseRunner` also provides some basic methods, such as, `step`, `state_dict`, `save_checkpoint`, `load_checkpoint`.\n\n    `BaseRunner` defines all basic attributes and relevant properties such as `scores`, `progress`, etc.\n\n    Attributes: Core:\n        mode (RunnerMode, property): Running mode.\n        state (RunnerState): Running state. See `RunnerState` for details.\n\n    Attributes: Model:\n        model (Callable):\n        criterion (Callable):\n        optimizer:\n        scheduler:\n\n    Attributes: Data:\n        datasets (FlatDict): All datasets, should be in the form of ``{subset: dataset}``.\n            Initialised to `FlatDict` by default.\n        datasamplers (FlatDict): All datasamplers, should be in the form of ``{subset: datasampler}``.\n            Initialised to `FlatDict` by default.\n        dataloaders (FlatDict): All dataloaders, should be in the form of ``{subset: dataloader}``.\n            Initialised to `FlatDict` by default.\n        batch_size (int, property): Number of samples per batch in train dataloader or the first dataloader.\n        batch_size_equivalent (int, property): Total batch_size (`batch_size * world_size * accum_steps`).\n\n    `datasets`, `datasamplers`, `dataloaders` should be a dict with the same keys.\n    Their keys should be `split` (e.g. `train`, `val`, `test`).\n\n    Attributes: Progress:\n        progress (float, property): Running Progress, in `range(0, 1)`.\n\n    Attributes: Results:\n        results (NestedDict): Results include all metric information of the model.\n            Results should be in the form of `{epoch: {subset: {metric: score}}}`.\n        latest_result (NestedDict, property): Most recent result, should be in the form of `{subset: {metric: score}}`.\n        best_result (NestedDict, property): Best result, should be in the form of `{subset: {metric: score}}`.\n        scores (List[float], property): Score is the core metric that is used to evaluate the performance of the model.\n            Scores should be in the form of `{epoch: score}`.\n        latest_score (float, property): Most recent score, should be in the form of `score`.\n        best_score (float, property): Best score, should be in the form of `score`.\n        score_set (Optional[str]): The subset to calculate the score.\n            If is `None`, will use the last set of the result.\n        score_name (str): The metric name of score.\n            Defaults to `\"loss\"`.\n        is_best (bool, property): If `latest_score == best_score`.\n\n    Attributes: IO:\n        dir (str, property): Directory of the run.\n            Defaults to `os.path.join(self.project_root, f\"{self.name}-{self.id}\")`.\n        checkpoint_dir (str, property): Directory of checkpoints.\n        log_path (str, property):  Path of log file.\n        checkpoint_dir_name (str): The name of the directory under `runner.dir` to save checkpoints.\n            Defaults to `\"checkpoints\"`.\n\n    Attributes: Parallel Training:\n        world_size (int, property): Number of processes.\n        rank (int, property): Process index of all processes.\n        local_rank (int, property): Process index of local processes.\n        distributed (bool, property): If runner is running in distributed mode.\n        is_main_process (bool, property): If current process is the main process of all processes.\n        is_local_main_process (bool, property): If current process is the main process of local processes.\n\n    Attributes: logging:\n        meters (AverageMeters): Average meters.\n            Initialised to `AverageMeters` by default.\n        metrics (Metrics): Metrics for evaluating.\n        logger:\n        writer:\n\n    See Also:\n        [`RunnerState`][danling.runner.runner_state.RunnerState]: The runeer base that stores runtime information.\n        [`BaseRunner`][danling.runner.BaseRunner]: The base runner class.\n    \"\"\"\n\n    # DO NOT set default value in class, as they won't be stored in `__dict__`.\n\n    _mode: RunnerMode\n    state: RunnerState\n\n    model: Callable | None = None\n    criterion: Callable | None = None\n    optimizer: Any | None = None\n    scheduler: Any | None = None\n\n    datasets: FlatDict\n    datasamplers: FlatDict\n    dataloaders: FlatDict\n\n    meters: AverageMeters\n    metrics: Metrics | None = None\n    logger: logging.Logger | None = None\n    writer: Any | None = None\n\n    def __init__(self, config: NestedDict) -&gt; None:\n        if \"datasets\" not in self.__dict__:\n            self.datasets = FlatDict()\n        if \"datasamplers\" not in self.__dict__:\n            self.datasamplers = FlatDict()\n        if \"dataloaders\" not in self.__dict__:\n            self.dataloaders = FlatDict()\n        self._mode = RunnerMode.train\n        self.meters = AverageMeters()\n        self.metrics = None\n        # must init state at last to avoid conflict names\n        self.state = RunnerState(config)\n        self.init_distributed()\n        if self.state.seed is not None:\n            self.set_seed()\n        if self.state.deterministic:\n            self.set_deterministic()\n        if os.listdir(self.dir):\n            warn(\n                f\"Directory `{self.dir}` is not empty.\",\n                category=RuntimeWarning,\n                stacklevel=2,\n            )\n        if self.state.log:\n            self.init_logging()\n        self.init_print()\n        if self.state.tensorboard:\n            self.init_tensorboard()\n\n    def __post_init__(self, *args, **kwargs) -&gt; None:\n        pass\n\n    @property\n    def mode(self) -&gt; RunnerMode:\n        return self._mode\n\n    @mode.setter\n    def mode(self, mode: str | RunnerMode) -&gt; None:\n        if isinstance(mode, str):\n            mode = RunnerMode(mode)\n        self._mode = mode\n\n    @cached_property\n    def batch_size(self) -&gt; int:\n        r\"\"\"\n        Batch size.\n\n        Notes:\n            If `train` is in `dataloaders`, then `batch_size` is the batch size of `train`.\n            Otherwise, `batch_size` is the batch size of the first dataloader.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        batch_size = self.state.get(\"dataloader.batch_size\")\n        if batch_size:\n            return batch_size\n        if self.dataloaders:\n            loader = self.dataloaders[\"train\"] if \"train\" in self.dataloaders else next(iter(self.dataloaders.values()))\n            return loader.batch_size\n        raise AttributeError(\"batch_size could not be inferred, since no dataloader found.\")\n\n    @property\n    def batch_size_equivalent(self) -&gt; int:\n        r\"\"\"\n        Actual batch size.\n\n        Returns:\n            (int): `batch_size` * `world_size` * `accum_steps`\n        \"\"\"\n\n        return self.batch_size * self.world_size * self.accum_steps\n\n    @cached_property\n    def total_epochs(self) -&gt; int:\n        if self.state.epoch_end:\n            return self.state.epoch_end - self.state.epoch_begin\n        raise ValueError(\"epoch_end is not specified\")\n\n    @cached_property\n    def total_steps(self) -&gt; int:\n        if self.state.step_end:\n            return self.state.step_end - self.state.step_begin\n        dataset = self.datasets.get(\"train\", next(iter(self.datasets.values())))\n        return self.total_epochs * ceil(len(dataset) / self.batch_size)\n\n    @cached_property\n    def accum_steps(self) -&gt; int:\n        r\"\"\"\n        Accumulated steps.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        return self.state.get(\"accum_steps\", 1)\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Initialise distributed running environment.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @property\n    def device(self) -&gt; Any:\n        r\"\"\"\n        Device of runner.\n        \"\"\"\n\n        return \"cpu\"\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of processes.\n        \"\"\"\n\n        return 1\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index of all processes.\n        \"\"\"\n\n        return 0\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index of local processes.\n        \"\"\"\n\n        return 0\n\n    @property\n    def distributed(self) -&gt; bool:\n        r\"\"\"\n        If runner is running in distributed mode.\n        \"\"\"\n\n        return self.world_size &gt; 1\n\n    @property\n    def is_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process of all processes.\n        \"\"\"\n\n        return self.rank == 0\n\n    @property\n    def is_local_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process of local processes.\n        \"\"\"\n\n        return self.local_rank == 0\n\n    @catch\n    def save(self, obj: Any, file: PathStr, main_process_only: bool = True, *args, **kwargs) -&gt; File:\n        r\"\"\"\n        Save any file with supported extensions.\n\n        `Runner.save` internally calls `dl.save`,\n        but with additional arguments to allow it save only on the main process.\n        Moreover, any error raised by `Runner.save` will be caught and logged.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            return save(obj, file, *args, **kwargs)\n        return file\n\n    @staticmethod\n    def load(file: PathStr, *args, **kwargs) -&gt; Any:\n        r\"\"\"\n        Load any file with supported extensions.\n\n        `Runner.load` is identical to `dl.load`.\n        \"\"\"\n\n        return load(file, *args, **kwargs)\n\n    def dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Convert state to Mapping.\n\n        Args:\n            cls: Target `clc to convert to.\n        \"\"\"\n\n        return self.state.dict(cls)\n\n    @catch\n    def json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n        r\"\"\"\n        Dump Runner State to json file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            return self.state.json(file, *args, **kwargs)\n\n    @classmethod\n    def from_json(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from json file.\n\n        This function calls `self.from_jsons()` to construct object from json string.\n        You may overwrite `from_jsons` in case something is not json serializable.\n        \"\"\"\n\n        with FlatDict.open(file) as fp:\n            return cls.from_jsons(fp.read(), *args, **kwargs)\n\n    def jsons(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner State to json string.\n        \"\"\"\n\n        return self.state.jsons(*args, **kwargs)\n\n    @classmethod\n    def from_jsons(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from json string.\n        \"\"\"\n\n        return cls(Config.from_jsons(string, *args, **kwargs))\n\n    @catch\n    def yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n        r\"\"\"\n        Dump Runner State to yaml file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            return self.state.yaml(file, *args, **kwargs)\n\n    @classmethod\n    def from_yaml(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from yaml file.\n\n        This function calls `self.from_yamls()` to construct object from yaml string.\n        You may overwrite `from_yamls` in case something is not yaml serializable.\n        \"\"\"\n\n        with FlatDict.open(file) as fp:\n            return cls.from_yamls(fp.read(), *args, **kwargs)\n\n    def yamls(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner State to yaml string.\n        \"\"\"\n\n        return self.state.yamls(*args, **kwargs)\n\n    @classmethod\n    def from_yamls(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from yaml string.\n        \"\"\"\n\n        return cls(Config.from_yamls(string, *args, **kwargs))\n\n    @property\n    def progress(self) -&gt; float:\n        r\"\"\"\n        Training Progress.\n\n        Returns:\n            (float):\n\n        Raises:\n            RuntimeError: If no terminal is defined.\n        \"\"\"\n\n        return self.steps / self.total_steps\n\n    @property\n    def best_fn(self) -&gt; Callable:\n        r\"\"\"\n        Function to determine the best score from a list of scores.\n\n        By default, the `best_fn` returns `min` if `self.state.score_name` is `loss`,\n        otherwise, returns `max`.\n\n        Subclass can override this method to accommodate needs, such as `min`.\n\n        Returns:\n            (callable):\n        \"\"\"\n\n        return max if self.state.score_name != \"loss\" else min\n\n    @property\n    def best_index(self) -&gt; int:\n        r\"\"\"\n        Find the best index from all scores.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        if not self.scores:\n            return 0\n        values = list(self.scores.values())\n        return self.best_fn(range(len(values)), key=values.__getitem__)\n\n    @property\n    def latest_result(self) -&gt; NestedDict | None:\n        r\"\"\"\n        Latest result.\n        \"\"\"\n\n        if not self.state.results:\n            return None\n        latest_index = next(reversed(self.state.results if PY38_PLUS else list(self.state.results)))  # type: ignore\n        ret = self.state.results[latest_index].clone()\n        ret[\"index\"] = latest_index\n        return ret\n\n    @property\n    def best_result(self) -&gt; NestedDict | None:\n        r\"\"\"\n        Best result.\n        \"\"\"\n\n        if not self.state.results:\n            return None\n        best_index = self.best_index\n        ret = self.state.results[best_index].clone()\n        ret[\"index\"] = best_index\n        return ret\n\n    @property\n    def scores(self) -&gt; FlatDict | None:\n        r\"\"\"\n        All scores.\n\n        Scores are extracted from results by `score_set` and `runner.state.score_name`,\n        following `[r[score_set][self.state.score_name] for r in self.state.results]`.\n\n        Scores are considered as the index of the performance of the model.\n        It is useful to determine the best model and the best hyper-parameters.\n\n        `score_set` is defined in `self.state.score_set`.\n        If it is not set, `DanLing` will use `val` or `validate` if they appear in the `latest_result`.\n        If `DanLing` still could not find, it will fall back to the second key in the `latest_result`\n        if it contains more that one element, or the first key.\n\n        Note that certain keys are ignored when falling back, they are defined in {IGNORED_SET_NAMES}.\n        \"\"\"\n\n        if not self.state.results:\n            return None\n        subsets = [i for i in self.latest_result.keys() if i not in IGNORED_SET_NAMES]  # type: ignore\n        score_set = self.state.get(\"score_set\")\n        if score_set is None and \"val\" in subsets:\n            score_set = \"val\"\n        if score_set is None and \"validate\" in subsets:\n            score_set = \"validate\"\n        if score_set is None:\n            score_set = subsets[1] if len(subsets) &gt; 1 else subsets[0]\n        return FlatDict({k: v[score_set][self.state.score_name] for k, v in self.state.results.items()})\n\n    @property\n    def latest_score(self) -&gt; float | None:\n        r\"\"\"\n        Latest score.\n        \"\"\"\n\n        if not self.state.results:\n            return None\n        if not PY38_PLUS:\n            return next(reversed(list(self.scores.values())))  # type: ignore\n        return next(reversed(self.scores.values()))  # type: ignore\n\n    @property\n    def best_score(self) -&gt; float | None:\n        r\"\"\"\n        Best score.\n        \"\"\"\n\n        if not self.state.results:\n            return None\n        return self.scores[self.best_index]  # type: ignore\n\n    @property\n    def is_best(self) -&gt; bool:\n        r\"\"\"\n        If current epoch is the best epoch.\n        \"\"\"\n\n        if not self.state.results:\n            return True\n        try:\n            return abs(self.latest_score - self.best_score) &lt; 1e-7  # type: ignore\n        except TypeError:\n            return True\n\n    @cached_property\n    @ensure_dir\n    def dir(self) -&gt; str:\n        r\"\"\"\n        Directory of the run.\n        \"\"\"\n\n        if \"dir\" in self.state:\n            return self.state.dir\n        return os.path.join(self.project_root, f\"{self.name}-{self.id}\")\n\n    @cached_property\n    def log_path(self) -&gt; str:\n        r\"\"\"\n        Path of log file.\n        \"\"\"\n\n        if \"log_path\" in self.state:\n            return self.state.log_path\n        return os.path.join(self.dir, \"run.log\")\n\n    @cached_property\n    @ensure_dir\n    def checkpoint_dir(self) -&gt; str:\n        r\"\"\"\n        Directory of checkpoints.\n        \"\"\"\n\n        if \"checkpoint_dir\" in self.state:\n            return self.state.checkpoint_dir\n        return os.path.join(self.dir, self.checkpoint_dir_name)\n\n    # def __getattribute__(self, name) -&gt; Any:\n    #     if name in (\"__class__\", \"__dict__\"):\n    #         return super().__getattribute__(name)\n    #     if name in self.__dict__:\n    #         return self.__dict__[name]\n    #     if name in dir(self):\n    #         return super().__getattribute__(name)\n    #     if \"state\" in self and name in self.state:\n    #         return self.state[name]\n    #     return super().__getattribute__(name)\n\n    def __getattr__(self, name) -&gt; Any:\n        if \"state\" in self and name in self.state:\n            return self.state[name]\n        return super().__getattribute__(name)\n\n    def __setattr__(self, name, value) -&gt; None:\n        if name in self.__dict__:\n            if isinstance(self.__dict__[name], Variable):\n                self.__dict__[name].set(value)\n            else:\n                self.__dict__[name] = value\n            return\n        if name in dir(self):\n            if isinstance(super().__getattribute__(name), Variable):\n                super().__getattribute__(name).set(value)\n            else:\n                object.__setattr__(self, name, value)\n            return\n        if \"state\" in self and name in self.state:\n            if isinstance(self.state[name], Variable):\n                self.state[name].set(value)\n            else:\n                self.state[name] = value\n            return\n        object.__setattr__(self, name, value)\n\n    def __contains__(self, name) -&gt; bool:\n        return name in dir(self) or (\"state\" in self.__dict__ and name in dir(self.state))\n\n    def __repr__(self):\n        lines = []\n        for key, value in self.__dict__.items():\n            value_str = repr(value)\n            value_str = self._add_indent(value_str)\n            lines.append(\"(\" + key + \"): \" + value_str)\n\n        main_str = self.__class__.__name__ + \"(\"\n        if lines:\n            main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n        main_str += \")\"\n        return main_str\n\n    def _add_indent(self, text):\n        lines = text.split(\"\\n\")\n        # don't do anything for single-line stuff\n        if len(lines) == 1:\n            return text\n        first = lines.pop(0)\n        # add 2 spaces to each line but the first\n        lines = [(2 * \" \") + line for line in lines]\n        lines = \"\\n\".join(lines)\n        lines = first + \"\\n\" + lines\n        return lines\n\n    def init_deepspeed(  # pylint: disable=too-many-branches, too-many-statements\n        self, config: Dict = None  # type: ignore\n    ) -&gt; Dict:\n        r\"\"\"\n        Preprocess DeepSpeed config.\n        \"\"\"\n\n        if config is None:\n            config = self.state.get(\"deepspeed\")\n        if config is None:\n            return {}\n        if isinstance(config, str):\n            config = NestedDict.load(config)\n        if config.get(\"steps_per_print\", \"auto\") == \"auto\":\n            config[\"steps_per_print\"] = self.print_interval\n        if config.get(\"train_micro_batch_size_per_gpu\", \"auto\") == \"auto\":\n            config[\"train_micro_batch_size_per_gpu\"] = self.batch_size\n        if \"amp\" in config:\n            amp = config[\"amp\"]\n            if amp.get(\"enabled\", \"auto\") == \"auto\":\n                amp[\"enabled\"] = \"true\"\n            if amp.get(\"opt_level\", \"auto\") == \"auto\":\n                amp[\"opt_level\"] = \"O1\"\n        if \"zero_optimization\" in config:\n            zero = config[\"zero_optimization\"]\n            if zero.get(\"allgather_bucket_size\") == \"auto\":\n                zero[\"allgather_bucket_size\"] = 1e6\n            if zero.get(\"reduce_bucket_size\") == \"auto\":\n                zero[\"reduce_bucket_size\"] = 1e6\n            if zero.get(\"stage3_max_live_parameters\") == \"auto\":\n                zero[\"stage3_max_live_parameters\"] = 1e8\n            if zero.get(\"stage3_max_live_gradients\") == \"auto\":\n                zero[\"stage3_max_live_gradients\"] = 1e8\n            if zero.get(\"stage3_max_reuse_distance\") == \"auto\":\n                zero[\"stage3_max_reuse_distance\"] = 1e8\n            if zero.get(\"stage3_prefetch_bucket_size\") == \"auto\":\n                zero[\"stage3_prefetch_bucket_size\"] = 1e6\n            if zero.get(\"stage3_param_persistence_threshold\") == \"auto\":\n                zero[\"stage3_param_persistence_threshold\"] = 1e8\n            if \"amp\" in config:\n                if \"fp16\" not in config:\n                    config[\"fp16\"] = {}\n                if config[\"fp16\"].get(\"enabled\", \"auto\"):\n                    config[\"fp16\"][\"enabled\"] = config[\"amp\"][\"enabled\"]\n                warn(\n                    f\"AMP is not compatible with ZeRO. Automatically set 'fp16' to {config['amp']['enabled']}\",\n                    stacklevel=2,\n                )\n                del config[\"amp\"]\n        if \"optimizer\" in config:\n            if \"params\" not in config[\"optimizer\"]:\n                config[\"optimizer\"][\"params\"] = {}\n            optimizer = config[\"optimizer\"][\"params\"]\n            if optimizer.get(\"lr\", \"auto\") == \"auto\":\n                optimizer[\"lr\"] = self.state.get(\"optim.lr\", 1e-3)\n            if optimizer.get(\"weight_decay\", \"auto\") == \"auto\":\n                optimizer[\"weight_decay\"] = self.state.get(\"optim.weight_decay\", 1e-2)\n            if optimizer.get(\"betas\") == \"auto\":\n                optimizer[\"betas\"] = (0.9, 0.999)\n            if optimizer.get(\"eps\") == \"auto\":\n                optimizer[\"eps\"] = 1e-8\n        if \"scheduler\" in config:\n            if \"params\" not in config[\"scheduler\"]:\n                config[\"scheduler\"][\"params\"] = {}\n            scheduler = config[\"scheduler\"][\"params\"]\n            if scheduler.get(\"total_num_steps\", \"auto\") == \"auto\":\n                scheduler[\"total_num_steps\"] = self.total_steps\n            if scheduler.get(\"warmup_num_steps\", \"auto\") == \"auto\":\n                scheduler[\"warmup_num_steps\"] = scheduler[\"total_num_steps\"] // 20\n            if scheduler.get(\"warmup_max_lr\", \"auto\") == \"auto\":\n                if self.optimizer:\n                    scheduler[\"warmup_max_lr\"] = self.optimizer.param_groups[0][\"lr\"]\n                elif \"optimizer\" in config:\n                    scheduler[\"warmup_max_lr\"] = config[\"optimizer\"][\"params\"][\"lr\"]\n                else:\n                    raise ValueError(\"warmup_max_lr is not defined and cannot be inferred\")\n            if scheduler.get(\"warmup_min_lr\", \"auto\") == \"auto\":\n                scheduler[\"warmup_min_lr\"] = 1e-7\n        return config\n\n    @on_main_process\n    def init_logging(self) -&gt; None:\n        r\"\"\"\n        Set up logging.\n        \"\"\"\n\n        os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n        # Why is setting up proper logging so !@?#! ugly?\n        logging.config.dictConfig(\n            {\n                \"version\": 1,\n                \"disable_existing_loggers\": False,\n                \"formatters\": {\n                    \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n                },\n                \"handlers\": {\n                    \"stdout\": {\n                        \"level\": \"INFO\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.StreamHandler\",\n                        \"stream\": \"ext://sys.stdout\",\n                    },\n                    \"logfile\": {\n                        \"level\": \"DEBUG\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.FileHandler\",\n                        \"filename\": self.log_path,\n                        \"mode\": \"a\",\n                    },\n                },\n                \"loggers\": {\n                    \"\": {\n                        \"handlers\": [\"stdout\", \"logfile\"],\n                        \"level\": \"DEBUG\",\n                        \"propagate\": True,\n                    },\n                },\n            }\n        )\n        logging.captureWarnings(True)\n        self.logger = logging.getLogger(\"runner\")\n        self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n\n    def init_print(self, process: int = 0) -&gt; None:\n        r\"\"\"\n        Set up `print`.\n\n        Only print on a specific `process` or when `force = True`.\n\n        Args:\n            process: The process to `print` on.\n\n        Notes\n        -----\n        If `self.state.log = True`, the default `print` function will be override by `logging.info`.\n        \"\"\"\n\n        logger = logging.getLogger(\"print\")\n        logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n        import builtins as __builtin__  # pylint: disable=C0415\n\n        builtin_print = __builtin__.print\n\n        @catch\n        def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=redefined-builtin\n            if self.rank == process or force:\n                if self.state.log:\n                    logger.info(*args, **kwargs)\n                else:\n                    builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n        __builtin__.print = print\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        raise NotImplementedError\n\n    def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Args:\n            seed: Random seed to set.\n                Defaults to `self.state.seed` (`config.seed`).\n\n            bias: Make the seed different for each processes.\n\n                This avoids same data augmentation are applied on every processes.\n\n                Defaults to `self.rank`.\n\n                Set to `False` to disable this feature.\n        \"\"\"\n\n        seed = seed or self.state.seed\n        bias = bias or self.rank\n        if bias:\n            seed += bias\n        if np_random is not None:\n            np_random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def scale_lr(\n        self,\n        lr: float,\n        lr_scale_factor: float | None = None,\n        batch_size_base: int | None = None,\n    ) -&gt; float:\n        r\"\"\"\n        Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n        \"\"\"\n\n        if lr_scale_factor in self.state:\n            lr_scale_factor = self.state.lr_scale_factor\n\n        if lr_scale_factor is None:\n            if batch_size_base is None:\n                batch_size_base = getattr(self, \"batch_size_base\", None)\n                if batch_size_base is None:\n                    raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n            lr_scale_factor = self.batch_size_equivalent / batch_size_base\n        elif batch_size_base is not None:\n            warn(\n                \"batch_size_base will be ignored if lr_scale_factor is specified\", category=RuntimeWarning, stacklevel=2\n            )\n        lr = lr * lr_scale_factor\n        self.state.lr_scale_factor = lr_scale_factor\n        return lr\n\n    def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n        r\"\"\"\n        Backward loss and step optimizer &amp; scheduler.\n\n        This method increment `self.state.steps`.\n\n        This method also increment `self.state.iters` when `batch_size` is specified.\n\n        Args:\n            zero_grad: Whether to zero the gradients.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Return dict of all attributes for checkpoint.\n        \"\"\"\n\n        return cls(self.state)\n\n    @catch\n    @on_main_process\n    def save_checkpoint(self) -&gt; None:\n        r\"\"\"\n        Save checkpoint to `self.checkpoint_dir`.\n\n        The checkpoint will be saved to `self.checkpoint_dir/latest.pth`.\n\n        If `self.state.save_interval` is positive and `self.state.epochs + 1` is a multiple of `save_interval`,\n        the checkpoint will also be copied to `self.checkpoint_dir/epoch-{self.state.epochs}.pth`.\n\n        If `self.is_best` is `True`, the checkpoint will also be copied to `self.checkpoint_dir/best.pth`.\n        \"\"\"\n\n        latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        self.save(self.state_dict(), latest_path)\n        if (\n            hasattr(self, \"save_interval\")\n            and self.save_interval &gt; 0\n            and (self.state.epochs + 1) % self.save_interval == 0\n        ):\n            save_path = os.path.join(self.checkpoint_dir, f\"epoch-{self.state.epochs}.pth\")\n            shutil.copy(latest_path, save_path)\n        if self.is_best:\n            best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n            shutil.copy(latest_path, best_path)\n\n    def load_checkpoint(\n        self,\n        checkpoint: Mapping | bytes | str | os.PathLike | None = None,\n        auto_resume: bool | None = None,\n        override_state: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Load info from checkpoint.\n\n        Args:\n            checkpoint: Checkpoint (or its path) to load.\n                Defaults to `self.state.checkpoint`.\n            auto_resume: Automatically resume from latest checkpoint if exists.\n                Defaults to `False`.\n                If is `True` and `checkpoint` is None, will set it to `self.checkpoint_dir/latest.pth`.\n            override_state: If True, override runner state with checkpoint state.\n                Defaults to `False`.\n            *args: Additional arguments to pass to `self.load`.\n            **kwargs: Additional keyword arguments to pass to `self.load`.\n\n        Raises:\n            FileNotFoundError: If `checkpoint` does not exists.\n\n        See Also:\n            [`from_checkpoint`][danling.BaseRunner.from_checkpoint]: Build runner from checkpoint.\n            [`load_pretrained`][danling.BaseRunner.load_pretrained]: Load parameters from pretrained checkpoint.\n        \"\"\"\n\n        checkpoint = checkpoint if checkpoint is not None else self.state.get(\"checkpoint\")\n        auto_resume = auto_resume if auto_resume is not None else self.state.get(\"auto_resume\", False)\n\n        # TODO: Support loading checkpoints in other format\n        if checkpoint is not None:\n            if auto_resume:\n                warn(\n                    \"latest checkpoint is preempted by value specified in checkpoint\",\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n            if isinstance(checkpoint, (bytes, str, os.PathLike)):\n                if not os.path.exists(checkpoint):\n                    raise FileNotFoundError(f\"checkpoint is set to {checkpoint!r} but does not exist.\")\n                self.state.checkpoint = checkpoint\n                ckpt = self.load(checkpoint, *args, **kwargs)\n            elif isinstance(checkpoint, Mapping):\n                ckpt = checkpoint\n            else:\n                raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint.\")\n        elif auto_resume:\n            checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n            if os.path.exists(checkpoint):\n                self.state.checkpoint = checkpoint\n                ckpt = self.load(checkpoint, *args, **kwargs)\n            else:\n                warn(\"latest checkpoint does not exits\", category=RuntimeWarning, stacklevel=2)\n                return\n        else:\n            raise ValueError(\"checkpoint is not specified and auto_resume is not set to True\")\n\n        # TODO: Wrap state_dict in a dataclass\n        self.state.merge(ckpt[\"runner\"], overwrite=override_state)\n        if self.model is not None and \"model\" in ckpt:\n            model = self.unwrap_model(self.model)\n            model.load_state_dict(ckpt[\"model\"])\n        if self.optimizer is not None and \"optimizer\" in ckpt:\n            self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n        if self.scheduler is not None and \"scheduler\" in ckpt:\n            self.scheduler.load_state_dict(ckpt[\"scheduler\"])\n        self.state.iter_begin = self.state.iters\n        self.state.step_begin = self.state.steps\n        self.state.epoch_begin = self.state.epochs\n\n    @classmethod\n    def from_checkpoint(cls, checkpoint: Mapping | bytes | str | os.PathLike, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Build BaseRunner from checkpoint.\n\n        Args:\n            checkpoint: Checkpoint (or its path) to load.\n            *args: Additional arguments to pass to `cls.load`.\n            **kwargs: Additional keyword arguments to pass to `cls.load`.\n\n        Returns:\n            (BaseRunner):\n        \"\"\"\n\n        if isinstance(checkpoint, (bytes, str, os.PathLike)):\n            ckpt = cls.load(checkpoint, *args, **kwargs)\n        elif isinstance(checkpoint, Mapping):\n            ckpt = checkpoint\n        else:\n            raise ValueError(f\"checkpoint is set to {checkpoint} but is not a valid checkpoint.\")\n        runner = cls(**ckpt[\"runner\"])\n        runner.load_checkpoint(ckpt, override_state=False)\n        return runner\n\n    def load_pretrained(self, checkpoint: Mapping | bytes | str | os.PathLike | None = None, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Load parameters from pretrained checkpoint.\n\n        This method only loads the model weights.\n\n        Args:\n            checkpoint: Pretrained checkpoint (or its path) to load.\n                Defaults to `self.state.pretrained`.\n            *args: Additional arguments to pass to `self.load`.\n            **kwargs: Additional keyword arguments to pass to `self.load`.\n\n        Raises:\n            FileNotFoundError: If `checkpoint` does not exists.\n\n        See Also:\n            [`load_checkpoint`][danling.BaseRunner.load_checkpoint]: Load info from checkpoint.\n        \"\"\"\n\n        # TODO: Support loading checkpoints in other format\n        checkpoint = checkpoint if checkpoint is not None else self.state.get(\"pretrained\")\n        if checkpoint is None:\n            raise ValueError(\"pretrained is not specified\")\n        if isinstance(checkpoint, (bytes, str, os.PathLike)):\n            if not os.path.exists(checkpoint):\n                raise FileNotFoundError(f\"pretrained is set to {checkpoint!r} but does not exist.\")\n            ckpt = self.load(checkpoint, *args, **kwargs)\n        elif isinstance(checkpoint, Mapping):\n            ckpt = checkpoint\n        else:\n            raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint.\")\n        ckpt = ckpt.get(\"model\", ckpt)\n        ckpt = ckpt.get(\"state_dict\", ckpt)\n        model = self.unwrap_model(self.model)\n        model.load_state_dict(ckpt)\n\n    def append_result(self, result: NestedDict, index: int | None = None) -&gt; None:\n        r\"\"\"\n        Append result to `self.state.results`.\n\n        Warnings:\n            `self.state.results` is heavily relied upon for computing metrics.\n\n            Failed to use this method may lead to unexpected behavior.\n        \"\"\"\n\n        if index is None:\n            index = self.state.epochs\n            global __APPEND_RESULT_COUNTER__  # pylint: disable=global-statement\n            __APPEND_RESULT_COUNTER__ += 1\n            if index == 0 and __APPEND_RESULT_COUNTER__ &gt; 1:\n                warn(\n                    \"\"\"\n                    Automatically set index to `self.state.epochs`.\n                    Please ensure `self.state.epochs` updates before calling `append_result`\n                    \"\"\",\n                    category=RuntimeWarning,\n                    stacklevel=2,\n                )\n        if index in self.state.results:\n            self.state.results[index].merge(result)\n        else:\n            self.state.results[index] = result\n\n    def print_result(self) -&gt; None:\n        r\"\"\"\n        Print latest and best result.\n        \"\"\"\n\n        print(f\"results: {self.state.results}\")\n        print(f\"latest result: {self.latest_result}\")\n        print(f\"best result: {self.best_result}\")\n\n    def step_log(self, split: str, iteration: int, length: int | None = None):\n        if length is None:\n            length = len(self.dataloaders[split]) - 1\n        result = self.meters.val\n        if self.metrics is not None:\n            result.merge(self.metrics.val)\n        print(self.format_step_result(result, split, iteration, length))\n        if self.mode == \"train\":\n            self.write_result(result, split)\n        return result\n\n    def format_step_result(self, result: NestedDict, split: str, steps: int, length: int) -&gt; str:\n        result = NestedDict(result).clone()\n        repr_str = \"\"\n        if split is not None:\n            if self.mode == \"train\":\n                repr_str = f\"training on {split} \"\n            elif self.mode == \"eval\":\n                repr_str = f\"evaluating on {split} \"\n            else:\n                repr_str = f\"running in {self.mode} mode on {split} \"\n        repr_str += f\"[{steps}/{length}]\\t\"\n        return repr_str + self.format_result(result)\n\n    def format_epoch_result(self, result: NestedDict, epochs: int | None = None, epoch_end: int | None = None) -&gt; str:\n        result = NestedDict(result).clone()\n        epochs = epochs or self.state.epochs\n        epoch_end = epoch_end or self.state.epoch_end\n        repr_str = f\"epoch [{epochs}/{epoch_end - 1}]\\n\" if epochs is not None and epoch_end else \"\"\n        repr_str += \"\\n\".join([f\"{k}:\\t{self.format_result(v)}\" for k, v in result.items()])\n        return repr_str\n\n    def format_result(self, result):\n        return \"\\t\".join([f\"{k}: {v}\" for k, v in result.items()])\n\n    def write_result(self, result: NestedDict, split: str, steps: int | None = None):\n        if steps is None:\n            steps = self.steps\n        for name, score in result.all_items():\n            name = name.replace(\".\", \"/\")\n            if name == \"loss\" and isinstance(score, AverageMeter):\n                score = score.avg\n            if isinstance(score, Sequence):\n                for i, s in enumerate(score):\n                    self.write_score(f\"{name}/{i}\", s, split, steps)\n            elif isinstance(score, Mapping):\n                for k, s in score.items():\n                    self.write_score(f\"{name}/{k}\", s, split, steps)\n            else:\n                self.write_score(name, score, split, steps)\n\n    def write_score(self, name: str, score: float, split: str, steps: int):\n        if self.writer:\n            self.writer.add_scalar(f\"{split}/{name}\", score, steps)\n\n    @catch\n    @on_main_process\n    def save_result(self) -&gt; None:\n        r\"\"\"\n        Save result to `self.dir`.\n\n        This method will save latest and best result to\n        `self.dir/latest.json` and `self.dir/best.json` respectively.\n        \"\"\"\n\n        results_path = os.path.join(self.dir, \"results.json\")\n        self.save(\n            {\n                \"id\": self.state.id,\n                \"name\": self.state.name,\n                \"results\": self.state.results,\n            },\n            results_path,\n            indent=4,\n        )\n        ret = {\"id\": self.state.id, \"name\": self.state.name}\n        result = self.latest_result\n        if isinstance(result, FlatDict):\n            result = result.dict()\n        # This is slower but ensure id is the first key\n        if result is not None:\n            ret.update(result)\n        latest_path = os.path.join(self.dir, \"latest.json\")\n        self.save(ret, latest_path, indent=4)\n        if self.is_best:\n            best_path = os.path.join(self.dir, \"best.json\")\n            shutil.copy(latest_path, best_path)\n</code></pre>"},{"location":"package/#danling.BaseRunner.accum_steps","title":"<code>accum_steps: int</code>  <code>cached</code> <code>property</code>","text":"<p>Accumulated steps.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"package/#danling.BaseRunner.batch_size","title":"<code>batch_size: int</code>  <code>cached</code> <code>property</code>","text":"<p>Batch size.</p> Notes <p>If <code>train</code> is in <code>dataloaders</code>, then <code>batch_size</code> is the batch size of <code>train</code>. Otherwise, <code>batch_size</code> is the batch size of the first dataloader.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"package/#danling.BaseRunner.batch_size_equivalent","title":"<code>batch_size_equivalent: int</code>  <code>property</code>","text":"<p>Actual batch size.</p> <p>Returns:</p> Type Description <code>int</code> <p><code>batch_size</code> * <code>world_size</code> * <code>accum_steps</code></p>"},{"location":"package/#danling.BaseRunner.best_fn","title":"<code>best_fn: Callable</code>  <code>property</code>","text":"<p>Function to determine the best score from a list of scores.</p> <p>By default, the <code>best_fn</code> returns <code>min</code> if <code>self.state.score_name</code> is <code>loss</code>, otherwise, returns <code>max</code>.</p> <p>Subclass can override this method to accommodate needs, such as <code>min</code>.</p> <p>Returns:</p> Type Description <code>callable</code>"},{"location":"package/#danling.BaseRunner.best_index","title":"<code>best_index: int</code>  <code>property</code>","text":"<p>Find the best index from all scores.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"package/#danling.BaseRunner.best_result","title":"<code>best_result: NestedDict | None</code>  <code>property</code>","text":"<p>Best result.</p>"},{"location":"package/#danling.BaseRunner.best_score","title":"<code>best_score: float | None</code>  <code>property</code>","text":"<p>Best score.</p>"},{"location":"package/#danling.BaseRunner.checkpoint_dir","title":"<code>checkpoint_dir: str</code>  <code>cached</code> <code>property</code>","text":"<p>Directory of checkpoints.</p>"},{"location":"package/#danling.BaseRunner.device","title":"<code>device: Any</code>  <code>property</code>","text":"<p>Device of runner.</p>"},{"location":"package/#danling.BaseRunner.dir","title":"<code>dir: str</code>  <code>cached</code> <code>property</code>","text":"<p>Directory of the run.</p>"},{"location":"package/#danling.BaseRunner.distributed","title":"<code>distributed: bool</code>  <code>property</code>","text":"<p>If runner is running in distributed mode.</p>"},{"location":"package/#danling.BaseRunner.is_best","title":"<code>is_best: bool</code>  <code>property</code>","text":"<p>If current epoch is the best epoch.</p>"},{"location":"package/#danling.BaseRunner.is_local_main_process","title":"<code>is_local_main_process: bool</code>  <code>property</code>","text":"<p>If current process is the main process of local processes.</p>"},{"location":"package/#danling.BaseRunner.is_main_process","title":"<code>is_main_process: bool</code>  <code>property</code>","text":"<p>If current process is the main process of all processes.</p>"},{"location":"package/#danling.BaseRunner.latest_result","title":"<code>latest_result: NestedDict | None</code>  <code>property</code>","text":"<p>Latest result.</p>"},{"location":"package/#danling.BaseRunner.latest_score","title":"<code>latest_score: float | None</code>  <code>property</code>","text":"<p>Latest score.</p>"},{"location":"package/#danling.BaseRunner.local_rank","title":"<code>local_rank: int</code>  <code>property</code>","text":"<p>Process index of local processes.</p>"},{"location":"package/#danling.BaseRunner.log_path","title":"<code>log_path: str</code>  <code>cached</code> <code>property</code>","text":"<p>Path of log file.</p>"},{"location":"package/#danling.BaseRunner.progress","title":"<code>progress: float</code>  <code>property</code>","text":"<p>Training Progress.</p> <p>Returns:</p> Type Description <code>float</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no terminal is defined.</p>"},{"location":"package/#danling.BaseRunner.rank","title":"<code>rank: int</code>  <code>property</code>","text":"<p>Process index of all processes.</p>"},{"location":"package/#danling.BaseRunner.scores","title":"<code>scores: FlatDict | None</code>  <code>property</code>","text":"<p>All scores.</p> <p>Scores are extracted from results by <code>score_set</code> and <code>runner.state.score_name</code>, following <code>[r[score_set][self.state.score_name] for r in self.state.results]</code>.</p> <p>Scores are considered as the index of the performance of the model. It is useful to determine the best model and the best hyper-parameters.</p> <p><code>score_set</code> is defined in <code>self.state.score_set</code>. If it is not set, <code>DanLing</code> will use <code>val</code> or <code>validate</code> if they appear in the <code>latest_result</code>. If <code>DanLing</code> still could not find, it will fall back to the second key in the <code>latest_result</code> if it contains more that one element, or the first key.</p> <p>Note that certain keys are ignored when falling back, they are defined in {IGNORED_SET_NAMES}.</p>"},{"location":"package/#danling.BaseRunner.world_size","title":"<code>world_size: int</code>  <code>property</code>","text":"<p>Number of processes.</p>"},{"location":"package/#danling.BaseRunner.append_result","title":"<code>append_result(result, index=None)</code>","text":"<p>Append result to <code>self.state.results</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def append_result(self, result: NestedDict, index: int | None = None) -&gt; None:\n    r\"\"\"\n    Append result to `self.state.results`.\n\n    Warnings:\n        `self.state.results` is heavily relied upon for computing metrics.\n\n        Failed to use this method may lead to unexpected behavior.\n    \"\"\"\n\n    if index is None:\n        index = self.state.epochs\n        global __APPEND_RESULT_COUNTER__  # pylint: disable=global-statement\n        __APPEND_RESULT_COUNTER__ += 1\n        if index == 0 and __APPEND_RESULT_COUNTER__ &gt; 1:\n            warn(\n                \"\"\"\n                Automatically set index to `self.state.epochs`.\n                Please ensure `self.state.epochs` updates before calling `append_result`\n                \"\"\",\n                category=RuntimeWarning,\n                stacklevel=2,\n            )\n    if index in self.state.results:\n        self.state.results[index].merge(result)\n    else:\n        self.state.results[index] = result\n</code></pre>"},{"location":"package/#danling.BaseRunner.dict","title":"<code>dict(cls=dict)</code>","text":"<p>Convert state to Mapping.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Callable</code> <p>Target `clc to convert to.</p> <code>dict</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Convert state to Mapping.\n\n    Args:\n        cls: Target `clc to convert to.\n    \"\"\"\n\n    return self.state.dict(cls)\n</code></pre>"},{"location":"package/#danling.BaseRunner.from_checkpoint","title":"<code>from_checkpoint(checkpoint, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Build BaseRunner from checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Mapping | bytes | str | PathLike</code> <p>Checkpoint (or its path) to load.</p> required <code>*args</code> <p>Additional arguments to pass to <code>cls.load</code>.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>cls.load</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseRunner</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_checkpoint(cls, checkpoint: Mapping | bytes | str | os.PathLike, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Build BaseRunner from checkpoint.\n\n    Args:\n        checkpoint: Checkpoint (or its path) to load.\n        *args: Additional arguments to pass to `cls.load`.\n        **kwargs: Additional keyword arguments to pass to `cls.load`.\n\n    Returns:\n        (BaseRunner):\n    \"\"\"\n\n    if isinstance(checkpoint, (bytes, str, os.PathLike)):\n        ckpt = cls.load(checkpoint, *args, **kwargs)\n    elif isinstance(checkpoint, Mapping):\n        ckpt = checkpoint\n    else:\n        raise ValueError(f\"checkpoint is set to {checkpoint} but is not a valid checkpoint.\")\n    runner = cls(**ckpt[\"runner\"])\n    runner.load_checkpoint(ckpt, override_state=False)\n    return runner\n</code></pre>"},{"location":"package/#danling.BaseRunner.from_json","title":"<code>from_json(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json file.</p> <p>This function calls <code>self.from_jsons()</code> to construct object from json string. You may overwrite <code>from_jsons</code> in case something is not json serializable.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_json(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from json file.\n\n    This function calls `self.from_jsons()` to construct object from json string.\n    You may overwrite `from_jsons` in case something is not json serializable.\n    \"\"\"\n\n    with FlatDict.open(file) as fp:\n        return cls.from_jsons(fp.read(), *args, **kwargs)\n</code></pre>"},{"location":"package/#danling.BaseRunner.from_jsons","title":"<code>from_jsons(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_jsons(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from json string.\n    \"\"\"\n\n    return cls(Config.from_jsons(string, *args, **kwargs))\n</code></pre>"},{"location":"package/#danling.BaseRunner.from_yaml","title":"<code>from_yaml(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml file.</p> <p>This function calls <code>self.from_yamls()</code> to construct object from yaml string. You may overwrite <code>from_yamls</code> in case something is not yaml serializable.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_yaml(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from yaml file.\n\n    This function calls `self.from_yamls()` to construct object from yaml string.\n    You may overwrite `from_yamls` in case something is not yaml serializable.\n    \"\"\"\n\n    with FlatDict.open(file) as fp:\n        return cls.from_yamls(fp.read(), *args, **kwargs)\n</code></pre>"},{"location":"package/#danling.BaseRunner.from_yamls","title":"<code>from_yamls(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_yamls(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from yaml string.\n    \"\"\"\n\n    return cls(Config.from_yamls(string, *args, **kwargs))\n</code></pre>"},{"location":"package/#danling.BaseRunner.init_deepspeed","title":"<code>init_deepspeed(config=None)</code>","text":"<p>Preprocess DeepSpeed config.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_deepspeed(  # pylint: disable=too-many-branches, too-many-statements\n    self, config: Dict = None  # type: ignore\n) -&gt; Dict:\n    r\"\"\"\n    Preprocess DeepSpeed config.\n    \"\"\"\n\n    if config is None:\n        config = self.state.get(\"deepspeed\")\n    if config is None:\n        return {}\n    if isinstance(config, str):\n        config = NestedDict.load(config)\n    if config.get(\"steps_per_print\", \"auto\") == \"auto\":\n        config[\"steps_per_print\"] = self.print_interval\n    if config.get(\"train_micro_batch_size_per_gpu\", \"auto\") == \"auto\":\n        config[\"train_micro_batch_size_per_gpu\"] = self.batch_size\n    if \"amp\" in config:\n        amp = config[\"amp\"]\n        if amp.get(\"enabled\", \"auto\") == \"auto\":\n            amp[\"enabled\"] = \"true\"\n        if amp.get(\"opt_level\", \"auto\") == \"auto\":\n            amp[\"opt_level\"] = \"O1\"\n    if \"zero_optimization\" in config:\n        zero = config[\"zero_optimization\"]\n        if zero.get(\"allgather_bucket_size\") == \"auto\":\n            zero[\"allgather_bucket_size\"] = 1e6\n        if zero.get(\"reduce_bucket_size\") == \"auto\":\n            zero[\"reduce_bucket_size\"] = 1e6\n        if zero.get(\"stage3_max_live_parameters\") == \"auto\":\n            zero[\"stage3_max_live_parameters\"] = 1e8\n        if zero.get(\"stage3_max_live_gradients\") == \"auto\":\n            zero[\"stage3_max_live_gradients\"] = 1e8\n        if zero.get(\"stage3_max_reuse_distance\") == \"auto\":\n            zero[\"stage3_max_reuse_distance\"] = 1e8\n        if zero.get(\"stage3_prefetch_bucket_size\") == \"auto\":\n            zero[\"stage3_prefetch_bucket_size\"] = 1e6\n        if zero.get(\"stage3_param_persistence_threshold\") == \"auto\":\n            zero[\"stage3_param_persistence_threshold\"] = 1e8\n        if \"amp\" in config:\n            if \"fp16\" not in config:\n                config[\"fp16\"] = {}\n            if config[\"fp16\"].get(\"enabled\", \"auto\"):\n                config[\"fp16\"][\"enabled\"] = config[\"amp\"][\"enabled\"]\n            warn(\n                f\"AMP is not compatible with ZeRO. Automatically set 'fp16' to {config['amp']['enabled']}\",\n                stacklevel=2,\n            )\n            del config[\"amp\"]\n    if \"optimizer\" in config:\n        if \"params\" not in config[\"optimizer\"]:\n            config[\"optimizer\"][\"params\"] = {}\n        optimizer = config[\"optimizer\"][\"params\"]\n        if optimizer.get(\"lr\", \"auto\") == \"auto\":\n            optimizer[\"lr\"] = self.state.get(\"optim.lr\", 1e-3)\n        if optimizer.get(\"weight_decay\", \"auto\") == \"auto\":\n            optimizer[\"weight_decay\"] = self.state.get(\"optim.weight_decay\", 1e-2)\n        if optimizer.get(\"betas\") == \"auto\":\n            optimizer[\"betas\"] = (0.9, 0.999)\n        if optimizer.get(\"eps\") == \"auto\":\n            optimizer[\"eps\"] = 1e-8\n    if \"scheduler\" in config:\n        if \"params\" not in config[\"scheduler\"]:\n            config[\"scheduler\"][\"params\"] = {}\n        scheduler = config[\"scheduler\"][\"params\"]\n        if scheduler.get(\"total_num_steps\", \"auto\") == \"auto\":\n            scheduler[\"total_num_steps\"] = self.total_steps\n        if scheduler.get(\"warmup_num_steps\", \"auto\") == \"auto\":\n            scheduler[\"warmup_num_steps\"] = scheduler[\"total_num_steps\"] // 20\n        if scheduler.get(\"warmup_max_lr\", \"auto\") == \"auto\":\n            if self.optimizer:\n                scheduler[\"warmup_max_lr\"] = self.optimizer.param_groups[0][\"lr\"]\n            elif \"optimizer\" in config:\n                scheduler[\"warmup_max_lr\"] = config[\"optimizer\"][\"params\"][\"lr\"]\n            else:\n                raise ValueError(\"warmup_max_lr is not defined and cannot be inferred\")\n        if scheduler.get(\"warmup_min_lr\", \"auto\") == \"auto\":\n            scheduler[\"warmup_min_lr\"] = 1e-7\n    return config\n</code></pre>"},{"location":"package/#danling.BaseRunner.init_distributed","title":"<code>init_distributed()</code>","text":"<p>Initialise distributed running environment.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_distributed(self) -&gt; None:\n    r\"\"\"\n    Initialise distributed running environment.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"package/#danling.BaseRunner.init_logging","title":"<code>init_logging()</code>","text":"<p>Set up logging.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@on_main_process\ndef init_logging(self) -&gt; None:\n    r\"\"\"\n    Set up logging.\n    \"\"\"\n\n    os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n    # Why is setting up proper logging so !@?#! ugly?\n    logging.config.dictConfig(\n        {\n            \"version\": 1,\n            \"disable_existing_loggers\": False,\n            \"formatters\": {\n                \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n            },\n            \"handlers\": {\n                \"stdout\": {\n                    \"level\": \"INFO\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.StreamHandler\",\n                    \"stream\": \"ext://sys.stdout\",\n                },\n                \"logfile\": {\n                    \"level\": \"DEBUG\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.FileHandler\",\n                    \"filename\": self.log_path,\n                    \"mode\": \"a\",\n                },\n            },\n            \"loggers\": {\n                \"\": {\n                    \"handlers\": [\"stdout\", \"logfile\"],\n                    \"level\": \"DEBUG\",\n                    \"propagate\": True,\n                },\n            },\n        }\n    )\n    logging.captureWarnings(True)\n    self.logger = logging.getLogger(\"runner\")\n    self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n</code></pre>"},{"location":"package/#danling.BaseRunner.init_print","title":"<code>init_print(process=0)</code>","text":"<p>Set up <code>print</code>.</p> <p>Only print on a specific <code>process</code> or when <code>force = True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>process</code> <code>int</code> <p>The process to <code>print</code> on.</p> <code>0</code>"},{"location":"package/#danling.BaseRunner.init_print--notes","title":"Notes","text":"<p>If <code>self.state.log = True</code>, the default <code>print</code> function will be override by <code>logging.info</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_print(self, process: int = 0) -&gt; None:\n    r\"\"\"\n    Set up `print`.\n\n    Only print on a specific `process` or when `force = True`.\n\n    Args:\n        process: The process to `print` on.\n\n    Notes\n    -----\n    If `self.state.log = True`, the default `print` function will be override by `logging.info`.\n    \"\"\"\n\n    logger = logging.getLogger(\"print\")\n    logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n    import builtins as __builtin__  # pylint: disable=C0415\n\n    builtin_print = __builtin__.print\n\n    @catch\n    def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=redefined-builtin\n        if self.rank == process or force:\n            if self.state.log:\n                logger.info(*args, **kwargs)\n            else:\n                builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n    __builtin__.print = print\n</code></pre>"},{"location":"package/#danling.BaseRunner.init_tensorboard","title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"package/#danling.BaseRunner.json","title":"<code>json(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner State to json file.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\ndef json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n    r\"\"\"\n    Dump Runner State to json file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        return self.state.json(file, *args, **kwargs)\n</code></pre>"},{"location":"package/#danling.BaseRunner.jsons","title":"<code>jsons(*args, **kwargs)</code>","text":"<p>Dump Runner State to json string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def jsons(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner State to json string.\n    \"\"\"\n\n    return self.state.jsons(*args, **kwargs)\n</code></pre>"},{"location":"package/#danling.BaseRunner.load","title":"<code>load(file, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load any file with supported extensions.</p> <p><code>Runner.load</code> is identical to <code>dl.load</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@staticmethod\ndef load(file: PathStr, *args, **kwargs) -&gt; Any:\n    r\"\"\"\n    Load any file with supported extensions.\n\n    `Runner.load` is identical to `dl.load`.\n    \"\"\"\n\n    return load(file, *args, **kwargs)\n</code></pre>"},{"location":"package/#danling.BaseRunner.load_checkpoint","title":"<code>load_checkpoint(checkpoint=None, auto_resume=None, override_state=False, *args, **kwargs)</code>","text":"<p>Load info from checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Mapping | bytes | str | PathLike | None</code> <p>Checkpoint (or its path) to load. Defaults to <code>self.state.checkpoint</code>.</p> <code>None</code> <code>auto_resume</code> <code>bool | None</code> <p>Automatically resume from latest checkpoint if exists. Defaults to <code>False</code>. If is <code>True</code> and <code>checkpoint</code> is None, will set it to <code>self.checkpoint_dir/latest.pth</code>.</p> <code>None</code> <code>override_state</code> <code>bool</code> <p>If True, override runner state with checkpoint state. Defaults to <code>False</code>.</p> <code>False</code> <code>*args</code> <p>Additional arguments to pass to <code>self.load</code>.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>self.load</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>checkpoint</code> does not exists.</p> See Also <p><code>from_checkpoint</code>: Build runner from checkpoint. <code>load_pretrained</code>: Load parameters from pretrained checkpoint.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def load_checkpoint(\n    self,\n    checkpoint: Mapping | bytes | str | os.PathLike | None = None,\n    auto_resume: bool | None = None,\n    override_state: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Load info from checkpoint.\n\n    Args:\n        checkpoint: Checkpoint (or its path) to load.\n            Defaults to `self.state.checkpoint`.\n        auto_resume: Automatically resume from latest checkpoint if exists.\n            Defaults to `False`.\n            If is `True` and `checkpoint` is None, will set it to `self.checkpoint_dir/latest.pth`.\n        override_state: If True, override runner state with checkpoint state.\n            Defaults to `False`.\n        *args: Additional arguments to pass to `self.load`.\n        **kwargs: Additional keyword arguments to pass to `self.load`.\n\n    Raises:\n        FileNotFoundError: If `checkpoint` does not exists.\n\n    See Also:\n        [`from_checkpoint`][danling.BaseRunner.from_checkpoint]: Build runner from checkpoint.\n        [`load_pretrained`][danling.BaseRunner.load_pretrained]: Load parameters from pretrained checkpoint.\n    \"\"\"\n\n    checkpoint = checkpoint if checkpoint is not None else self.state.get(\"checkpoint\")\n    auto_resume = auto_resume if auto_resume is not None else self.state.get(\"auto_resume\", False)\n\n    # TODO: Support loading checkpoints in other format\n    if checkpoint is not None:\n        if auto_resume:\n            warn(\n                \"latest checkpoint is preempted by value specified in checkpoint\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        if isinstance(checkpoint, (bytes, str, os.PathLike)):\n            if not os.path.exists(checkpoint):\n                raise FileNotFoundError(f\"checkpoint is set to {checkpoint!r} but does not exist.\")\n            self.state.checkpoint = checkpoint\n            ckpt = self.load(checkpoint, *args, **kwargs)\n        elif isinstance(checkpoint, Mapping):\n            ckpt = checkpoint\n        else:\n            raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint.\")\n    elif auto_resume:\n        checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        if os.path.exists(checkpoint):\n            self.state.checkpoint = checkpoint\n            ckpt = self.load(checkpoint, *args, **kwargs)\n        else:\n            warn(\"latest checkpoint does not exits\", category=RuntimeWarning, stacklevel=2)\n            return\n    else:\n        raise ValueError(\"checkpoint is not specified and auto_resume is not set to True\")\n\n    # TODO: Wrap state_dict in a dataclass\n    self.state.merge(ckpt[\"runner\"], overwrite=override_state)\n    if self.model is not None and \"model\" in ckpt:\n        model = self.unwrap_model(self.model)\n        model.load_state_dict(ckpt[\"model\"])\n    if self.optimizer is not None and \"optimizer\" in ckpt:\n        self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n    if self.scheduler is not None and \"scheduler\" in ckpt:\n        self.scheduler.load_state_dict(ckpt[\"scheduler\"])\n    self.state.iter_begin = self.state.iters\n    self.state.step_begin = self.state.steps\n    self.state.epoch_begin = self.state.epochs\n</code></pre>"},{"location":"package/#danling.BaseRunner.load_pretrained","title":"<code>load_pretrained(checkpoint=None, *args, **kwargs)</code>","text":"<p>Load parameters from pretrained checkpoint.</p> <p>This method only loads the model weights.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Mapping | bytes | str | PathLike | None</code> <p>Pretrained checkpoint (or its path) to load. Defaults to <code>self.state.pretrained</code>.</p> <code>None</code> <code>*args</code> <p>Additional arguments to pass to <code>self.load</code>.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>self.load</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>checkpoint</code> does not exists.</p> See Also <p><code>load_checkpoint</code>: Load info from checkpoint.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def load_pretrained(self, checkpoint: Mapping | bytes | str | os.PathLike | None = None, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Load parameters from pretrained checkpoint.\n\n    This method only loads the model weights.\n\n    Args:\n        checkpoint: Pretrained checkpoint (or its path) to load.\n            Defaults to `self.state.pretrained`.\n        *args: Additional arguments to pass to `self.load`.\n        **kwargs: Additional keyword arguments to pass to `self.load`.\n\n    Raises:\n        FileNotFoundError: If `checkpoint` does not exists.\n\n    See Also:\n        [`load_checkpoint`][danling.BaseRunner.load_checkpoint]: Load info from checkpoint.\n    \"\"\"\n\n    # TODO: Support loading checkpoints in other format\n    checkpoint = checkpoint if checkpoint is not None else self.state.get(\"pretrained\")\n    if checkpoint is None:\n        raise ValueError(\"pretrained is not specified\")\n    if isinstance(checkpoint, (bytes, str, os.PathLike)):\n        if not os.path.exists(checkpoint):\n            raise FileNotFoundError(f\"pretrained is set to {checkpoint!r} but does not exist.\")\n        ckpt = self.load(checkpoint, *args, **kwargs)\n    elif isinstance(checkpoint, Mapping):\n        ckpt = checkpoint\n    else:\n        raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint.\")\n    ckpt = ckpt.get(\"model\", ckpt)\n    ckpt = ckpt.get(\"state_dict\", ckpt)\n    model = self.unwrap_model(self.model)\n    model.load_state_dict(ckpt)\n</code></pre>"},{"location":"package/#danling.BaseRunner.print_result","title":"<code>print_result()</code>","text":"<p>Print latest and best result.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def print_result(self) -&gt; None:\n    r\"\"\"\n    Print latest and best result.\n    \"\"\"\n\n    print(f\"results: {self.state.results}\")\n    print(f\"latest result: {self.latest_result}\")\n    print(f\"best result: {self.best_result}\")\n</code></pre>"},{"location":"package/#danling.BaseRunner.save","title":"<code>save(obj, file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Save any file with supported extensions.</p> <p><code>Runner.save</code> internally calls <code>dl.save</code>, but with additional arguments to allow it save only on the main process. Moreover, any error raised by <code>Runner.save</code> will be caught and logged.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, file: PathStr, main_process_only: bool = True, *args, **kwargs) -&gt; File:\n    r\"\"\"\n    Save any file with supported extensions.\n\n    `Runner.save` internally calls `dl.save`,\n    but with additional arguments to allow it save only on the main process.\n    Moreover, any error raised by `Runner.save` will be caught and logged.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        return save(obj, file, *args, **kwargs)\n    return file\n</code></pre>"},{"location":"package/#danling.BaseRunner.save_checkpoint","title":"<code>save_checkpoint()</code>","text":"<p>Save checkpoint to <code>self.checkpoint_dir</code>.</p> <p>The checkpoint will be saved to <code>self.checkpoint_dir/latest.pth</code>.</p> <p>If <code>self.state.save_interval</code> is positive and <code>self.state.epochs + 1</code> is a multiple of <code>save_interval</code>, the checkpoint will also be copied to <code>self.checkpoint_dir/epoch-{self.state.epochs}.pth</code>.</p> <p>If <code>self.is_best</code> is <code>True</code>, the checkpoint will also be copied to <code>self.checkpoint_dir/best.pth</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_checkpoint(self) -&gt; None:\n    r\"\"\"\n    Save checkpoint to `self.checkpoint_dir`.\n\n    The checkpoint will be saved to `self.checkpoint_dir/latest.pth`.\n\n    If `self.state.save_interval` is positive and `self.state.epochs + 1` is a multiple of `save_interval`,\n    the checkpoint will also be copied to `self.checkpoint_dir/epoch-{self.state.epochs}.pth`.\n\n    If `self.is_best` is `True`, the checkpoint will also be copied to `self.checkpoint_dir/best.pth`.\n    \"\"\"\n\n    latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n    self.save(self.state_dict(), latest_path)\n    if (\n        hasattr(self, \"save_interval\")\n        and self.save_interval &gt; 0\n        and (self.state.epochs + 1) % self.save_interval == 0\n    ):\n        save_path = os.path.join(self.checkpoint_dir, f\"epoch-{self.state.epochs}.pth\")\n        shutil.copy(latest_path, save_path)\n    if self.is_best:\n        best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n        shutil.copy(latest_path, best_path)\n</code></pre>"},{"location":"package/#danling.BaseRunner.save_result","title":"<code>save_result()</code>","text":"<p>Save result to <code>self.dir</code>.</p> <p>This method will save latest and best result to <code>self.dir/latest.json</code> and <code>self.dir/best.json</code> respectively.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_result(self) -&gt; None:\n    r\"\"\"\n    Save result to `self.dir`.\n\n    This method will save latest and best result to\n    `self.dir/latest.json` and `self.dir/best.json` respectively.\n    \"\"\"\n\n    results_path = os.path.join(self.dir, \"results.json\")\n    self.save(\n        {\n            \"id\": self.state.id,\n            \"name\": self.state.name,\n            \"results\": self.state.results,\n        },\n        results_path,\n        indent=4,\n    )\n    ret = {\"id\": self.state.id, \"name\": self.state.name}\n    result = self.latest_result\n    if isinstance(result, FlatDict):\n        result = result.dict()\n    # This is slower but ensure id is the first key\n    if result is not None:\n        ret.update(result)\n    latest_path = os.path.join(self.dir, \"latest.json\")\n    self.save(ret, latest_path, indent=4)\n    if self.is_best:\n        best_path = os.path.join(self.dir, \"best.json\")\n        shutil.copy(latest_path, best_path)\n</code></pre>"},{"location":"package/#danling.BaseRunner.scale_lr","title":"<code>scale_lr(lr, lr_scale_factor=None, batch_size_base=None)</code>","text":"<p>Scale learning rate according to linear scaling rule.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def scale_lr(\n    self,\n    lr: float,\n    lr_scale_factor: float | None = None,\n    batch_size_base: int | None = None,\n) -&gt; float:\n    r\"\"\"\n    Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n    \"\"\"\n\n    if lr_scale_factor in self.state:\n        lr_scale_factor = self.state.lr_scale_factor\n\n    if lr_scale_factor is None:\n        if batch_size_base is None:\n            batch_size_base = getattr(self, \"batch_size_base\", None)\n            if batch_size_base is None:\n                raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n        lr_scale_factor = self.batch_size_equivalent / batch_size_base\n    elif batch_size_base is not None:\n        warn(\n            \"batch_size_base will be ignored if lr_scale_factor is specified\", category=RuntimeWarning, stacklevel=2\n        )\n    lr = lr * lr_scale_factor\n    self.state.lr_scale_factor = lr_scale_factor\n    return lr\n</code></pre>"},{"location":"package/#danling.BaseRunner.set_deterministic","title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"package/#danling.BaseRunner.set_seed","title":"<code>set_seed(seed=None, bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Random seed to set. Defaults to <code>self.state.seed</code> (<code>config.seed</code>).</p> <code>None</code> <code>bias</code> <code>int | None</code> <p>Make the seed different for each processes.</p> <p>This avoids same data augmentation are applied on every processes.</p> <p>Defaults to <code>self.rank</code>.</p> <p>Set to <code>False</code> to disable this feature.</p> <code>None</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Args:\n        seed: Random seed to set.\n            Defaults to `self.state.seed` (`config.seed`).\n\n        bias: Make the seed different for each processes.\n\n            This avoids same data augmentation are applied on every processes.\n\n            Defaults to `self.rank`.\n\n            Set to `False` to disable this feature.\n    \"\"\"\n\n    seed = seed or self.state.seed\n    bias = bias or self.rank\n    if bias:\n        seed += bias\n    if np_random is not None:\n        np_random.seed(seed)\n    random.seed(seed)\n</code></pre>"},{"location":"package/#danling.BaseRunner.state_dict","title":"<code>state_dict(cls=dict)</code>","text":"<p>Return dict of all attributes for checkpoint.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Return dict of all attributes for checkpoint.\n    \"\"\"\n\n    return cls(self.state)\n</code></pre>"},{"location":"package/#danling.BaseRunner.step","title":"<code>step(loss, batch_size=None, zero_grad=True)</code>","text":"<p>Backward loss and step optimizer &amp; scheduler.</p> <p>This method increment <code>self.state.steps</code>.</p> <p>This method also increment <code>self.state.iters</code> when <code>batch_size</code> is specified.</p> <p>Parameters:</p> Name Type Description Default <code>zero_grad</code> <code>bool</code> <p>Whether to zero the gradients.</p> <code>True</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n    r\"\"\"\n    Backward loss and step optimizer &amp; scheduler.\n\n    This method increment `self.state.steps`.\n\n    This method also increment `self.state.iters` when `batch_size` is specified.\n\n    Args:\n        zero_grad: Whether to zero the gradients.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"package/#danling.BaseRunner.yaml","title":"<code>yaml(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner State to yaml file.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\ndef yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n    r\"\"\"\n    Dump Runner State to yaml file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        return self.state.yaml(file, *args, **kwargs)\n</code></pre>"},{"location":"package/#danling.BaseRunner.yamls","title":"<code>yamls(*args, **kwargs)</code>","text":"<p>Dump Runner State to yaml string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def yamls(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner State to yaml string.\n    \"\"\"\n\n    return self.state.yamls(*args, **kwargs)\n</code></pre>"},{"location":"package/#danling.Metrics","title":"<code>Metrics</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Metric class wraps around multiple metrics that share the same states.</p> <p>Typically, there are many metrics that we want to compute for a single task. For example, we usually needs to compute <code>accuracy</code>, <code>auroc</code>, <code>auprc</code> for a classification task. Computing them one by one is inefficient, especially when evaluating in a distributed environment.</p> <p>To solve this problem, Metrics maintains a shared state for multiple metric functions.</p> <p>Attributes:</p> Name Type Description <code>metrics</code> <code>FlatDict[str, Callable]</code> <p>A dictionary of metrics to be computed.</p> <code>input</code> <p>The input tensor of latest batch.</p> <code>target</code> <p>The target tensor of latest batch.</p> <code>inputs</code> <p>All input tensors.</p> <code>targets</code> <p>All target tensors.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>A single mapping of metrics.</p> <code>()</code> <code>**metrics</code> <code>FlatDict[str, Callable]</code> <p>Metrics.</p> <code>{}</code> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>class Metrics(Metric):\n    r\"\"\"\n    Metric class wraps around multiple metrics that share the same states.\n\n    Typically, there are many metrics that we want to compute for a single task.\n    For example, we usually needs to compute `accuracy`, `auroc`, `auprc` for a classification task.\n    Computing them one by one is inefficient, especially when evaluating in a distributed environment.\n\n    To solve this problem, Metrics maintains a shared state for multiple metric functions.\n\n    Attributes:\n        metrics: A dictionary of metrics to be computed.\n        input: The input tensor of latest batch.\n        target: The target tensor of latest batch.\n        inputs: All input tensors.\n        targets: All target tensors.\n\n    Args:\n        *args: A single mapping of metrics.\n        **metrics: Metrics.\n    \"\"\"\n\n    metrics: FlatDict[str, Callable]\n    _input: Tensor\n    _target: Tensor\n    _inputs: list[Tensor]\n    _targets: list[Tensor]\n    _input_buffer: list[Tensor]\n    _target_buffer: list[Tensor]\n    score_name: str\n    best_fn: Callable\n    merge_dict: bool = True\n\n    def __init__(self, *args, merge_dict: bool | None = None, **metrics: FlatDict[str, Callable]):\n        super().__init__()\n        self._add_state(\"_input\", torch.empty(0))\n        self._add_state(\"_target\", torch.empty(0))\n        self._add_state(\"_inputs\", [])\n        self._add_state(\"_targets\", [])\n        self._add_state(\"_input_buffer\", [])\n        self._add_state(\"_target_buffer\", [])\n        self.metrics = FlatDict(*args, **metrics)\n        if merge_dict is not None:\n            self.merge_dict = merge_dict\n\n    @torch.inference_mode()\n    def update(self, input: Any, target: Any) -&gt; None:\n        if isinstance(input, NestedTensor):\n            self._input = input\n            self._input_buffer.extend(input.to(self.device).storage)\n        else:\n            if not isinstance(input, torch.Tensor):\n                input = torch.tensor(input)\n            self._input = input\n            self._input_buffer.append(input.to(self.device))\n        if isinstance(target, NestedTensor):\n            self._target = target\n            self._target_buffer.extend(target.to(self.device).storage)\n        else:\n            if not isinstance(target, torch.Tensor):\n                target = torch.tensor(target)\n            self._target = target\n            self._target_buffer.append(target.to(self.device))\n\n    def compute(self) -&gt; FlatDict[str, float]:\n        return self.comp\n\n    def value(self) -&gt; FlatDict[str, float]:\n        return self.val\n\n    def average(self) -&gt; FlatDict[str, float]:\n        return self.avg\n\n    @property\n    def comp(self) -&gt; FlatDict[str, float]:\n        return self._compute(self._input, self._target)\n\n    @property\n    def val(self) -&gt; FlatDict[str, float]:\n        return self._compute(self.input, self.target)\n\n    @property\n    def avg(self) -&gt; FlatDict[str, float]:\n        return self._compute(self.inputs, self.targets)\n\n    @torch.inference_mode()\n    def _compute(self, input: Tensor, target: Tensor) -&gt; flist | float:\n        if input.numel() == 0 == target.numel():\n            return FlatDict({name: nan for name in self.metrics.keys()})\n        ret = FlatDict()\n        for name, metric in self.metrics.items():\n            score = metric(input, target)\n            if isinstance(score, Tensor):\n                ret[name] = score.item() if score.numel() == 1 else flist(score.tolist())\n            elif isinstance(score, Mapping):\n                if self.merge_dict:\n                    ret.merge(score)\n                else:\n                    for n, s in score:\n                        ret[f\"{name}.{n}\"] = s\n            else:\n                ret[name] = score\n        return ret\n\n    @torch.inference_mode()\n    def merge_state(self, metrics: Iterable):\n        raise NotImplementedError()\n\n    @property\n    @torch.inference_mode()\n    def input(self):\n        if world_size() == 1:\n            return self._input\n        if isinstance(self._input, Tensor):\n            synced_tensor = [torch.zeros_like(self._input) for _ in range(dist.get_world_size())]\n            dist.all_gather(synced_tensor, self._input)\n            return torch.cat(synced_tensor, 0)\n        if isinstance(self._input, NestedTensor):\n            synced_tensors = [None for _ in range(dist.get_world_size())]\n            dist.all_gather_object(synced_tensors, self._input.storage)\n            return NestedTensor([i for j in synced_tensors for i in j])\n        raise ValueError(f\"Expected _input to be a Tensor or a NestedTensor, but got {type(self._input)}\")\n\n    @property\n    @torch.inference_mode()\n    def target(self):\n        if world_size() == 1:\n            return self._target\n        if isinstance(self._target, Tensor):\n            synced_tensor = [torch.zeros_like(self._target) for _ in range(dist.get_world_size())]\n            dist.all_gather(synced_tensor, self._target)\n            return torch.cat(synced_tensor, 0)\n        if isinstance(self._target, NestedTensor):\n            synced_tensors = [None for _ in range(dist.get_world_size())]\n            dist.all_gather_object(synced_tensors, self._target.storage)\n            return NestedTensor([i for j in synced_tensors for i in j])\n        raise ValueError(f\"Expected _target to be a Tensor or a NestedTensor, but got {type(self._target)}\")\n\n    @property\n    @torch.inference_mode()\n    def inputs(self):\n        if not self._inputs and not self._input_buffer:\n            return torch.empty(0)\n        if self._input_buffer:\n            if world_size() &gt; 1:\n                synced_tensors = [None for _ in range(dist.get_world_size())]\n                dist.all_gather_object(synced_tensors, self._input_buffer)\n                self._inputs.extend([i for j in synced_tensors for i in j])\n            else:\n                self._inputs.extend(self._input_buffer)\n            self._input_buffer = []\n        if isinstance(self._input, NestedTensor):\n            return NestedTensor(self._inputs)\n        return torch.cat(self._inputs, 0)\n\n    @property\n    @torch.inference_mode()\n    def targets(self):\n        if not self._targets and not self._target_buffer:\n            return torch.empty(0)\n        if self._target_buffer:\n            if world_size() &gt; 1:\n                synced_tensors = [None for _ in range(dist.get_world_size())]\n                dist.all_gather_object(synced_tensors, self._target_buffer)\n                self._targets.extend([i for j in synced_tensors for i in j])\n            else:\n                self._targets.extend(self._target_buffer)\n            self._target_buffer = []\n        if isinstance(self._target, NestedTensor):\n            return NestedTensor(self._targets)\n        return torch.cat(self._targets, 0)\n\n    def __repr__(self):\n        keys = tuple(i for i in self.metrics.keys())\n        return f\"{self.__class__.__name__}{keys}\"\n\n    def __format__(self, format_spec):\n        val, avg = self.compute(), self.average()\n        return \"\\n\".join(\n            [f\"{key}: {val[key].__format__(format_spec)} ({avg[key].__format__(format_spec)})\" for key in self.metrics]\n        )\n</code></pre>"},{"location":"package/#danling.NestedTensor","title":"<code>NestedTensor</code>","text":"<p>Wrap a sequence of tensors into a single tensor with a mask.</p> <p>In sequence to sequence tasks, elements of a batch are usually not of the same length. This made it tricky to use a single tensor to represent a batch of sequences.</p> <p><code>NestedTensor</code> allows to store a sequence of tensors of different lengths in a single object. It also provides a mask that can be used to retrieve the original sequence of tensors.</p> <p>When calling <code>__getitem__(arg)</code> on a <code>NestedTensor</code>, it has two return type: 1. if arg is <code>int</code> or <code>slice</code>, returns a tuple of two <code>tensor</code>s, representing data and padding mask. 2. if arg is a <code>tuple</code>, return a new <code>NestedTensor</code> with specified shape.</p> <p>Attributes:</p> Text Only<pre><code>_storage: The sequence of tensors.\nbatch_first:  Whether the first dimension of the tensors is the batch dimension.\n\n    If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.\n\n    If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`\npadding_value: The value used to pad the tensors.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Iterable[Tensor]</code> required <code>batch_first</code> <code>bool</code> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensors</code> is not a sequence.</p> <code>ValueError</code> <p>If <code>tensors</code> is empty.</p> Notes <p>We have rewritten the <code>__getattr__</code> function to support as much native tensor operations as possible. However, not all operations are tested.</p> <p>Please file an issue if you find any bugs.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n&gt;&gt;&gt; nested_tensor.dtype\ntorch.int64\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n&gt;&gt;&gt; nested_tensor.to(torch.float).tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]])\n&gt;&gt;&gt; nested_tensor.half().tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]], dtype=torch.float16)\n&gt;&gt;&gt; nested_tensor[:]\n(tensor([[1, 2, 3],\n        [4, 5, 0]]), tensor([[ True,  True,  True],\n        [ True,  True, False]]))\n&gt;&gt;&gt; nested_tensor[1]\n(tensor([4, 5]), tensor([True, True]))\n&gt;&gt;&gt; nested_tensor[:, 1:]\nNestedTensor([[2, 3],\n        [5, 0]])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class NestedTensor:\n    r\"\"\"\n    Wrap a sequence of tensors into a single tensor with a mask.\n\n    In sequence to sequence tasks, elements of a batch are usually not of the same length.\n    This made it tricky to use a single tensor to represent a batch of sequences.\n\n    `NestedTensor` allows to store a sequence of tensors of different lengths in a single object.\n    It also provides a mask that can be used to retrieve the original sequence of tensors.\n\n    When calling `__getitem__(arg)` on a `NestedTensor`, it has two return type:\n    1. if arg is `int` or `slice`, returns a tuple of two `tensor`s, representing data and padding mask.\n    2. if arg is a `tuple`, return a new `NestedTensor` with specified shape.\n\n    Attributes:\n\n        _storage: The sequence of tensors.\n        batch_first:  Whether the first dimension of the tensors is the batch dimension.\n\n            If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.\n\n            If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`\n        padding_value: The value used to pad the tensors.\n\n    Args:\n        tensors:\n        batch_first:\n\n    Raises:\n        ValueError: If `tensors` is not a sequence.\n        ValueError: If `tensors` is empty.\n\n    Notes:\n        We have rewritten the `__getattr__` function to support as much native tensor operations as possible.\n        However, not all operations are tested.\n\n        Please file an issue if you find any bugs.\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.device\n        device(type='cpu')\n        &gt;&gt;&gt; nested_tensor.dtype\n        torch.int64\n        &gt;&gt;&gt; nested_tensor.tensor\n        tensor([[1, 2, 3],\n                [4, 5, 0]])\n        &gt;&gt;&gt; nested_tensor.mask\n        tensor([[ True,  True,  True],\n                [ True,  True, False]])\n        &gt;&gt;&gt; nested_tensor.to(torch.float).tensor\n        tensor([[1., 2., 3.],\n                [4., 5., 0.]])\n        &gt;&gt;&gt; nested_tensor.half().tensor\n        tensor([[1., 2., 3.],\n                [4., 5., 0.]], dtype=torch.float16)\n        &gt;&gt;&gt; nested_tensor[:]\n        (tensor([[1, 2, 3],\n                [4, 5, 0]]), tensor([[ True,  True,  True],\n                [ True,  True, False]]))\n        &gt;&gt;&gt; nested_tensor[1]\n        (tensor([4, 5]), tensor([True, True]))\n        &gt;&gt;&gt; nested_tensor[:, 1:]\n        NestedTensor([[2, 3],\n                [5, 0]])\n    \"\"\"\n\n    _storage: Sequence[Tensor]\n    batch_first: bool = True\n    padding_value: SupportsFloat = 0.0\n    mask_value: bool = False\n\n    def __init__(\n        self,\n        tensors: Iterable[Tensor],\n        batch_first: bool = True,\n        padding_value: SupportsFloat = 0.0,\n        mask_value: bool = False,\n    ) -&gt; None:\n        if not isinstance(tensors, Iterable):\n            raise ValueError(f\"NestedTensor must be initialised with an Iterable, bug got {type(tensors)}.\")\n        tensors = list(tensors)\n        if len(tensors) == 0:\n            raise ValueError(\"NestedTensor must be initialised with a non-empty Iterable.\")\n        if not isinstance(tensors[0], Tensor):\n            tensors = [torch.tensor(tensor) for tensor in tensors]\n        self._storage = tensors\n        self.batch_first = batch_first\n        self.padding_value = padding_value\n        self.mask_value = mask_value\n\n    def storage(self):\n        return self._storage\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Return a single tensor by padding all the tensors.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.tensor\n            tensor([[1, 2, 3],\n                    [4, 5, 0]])\n        \"\"\"\n\n        return self._tensor(tuple(self._storage), self.batch_first, float(self.padding_value))\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Padding mask of `tensor`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.mask\n            tensor([[ True,  True,  True],\n                    [ True,  True, False]])\n        \"\"\"\n\n        return self._mask(tuple(self._storage), self.batch_first, self.mask_value)\n\n    @classmethod\n    def from_tensor_mask(cls, tensor: Tensor, mask: Tensor):\n        r\"\"\"\n        Build a `NestedTensor` object from a padded `Tensor` and corresponding mask `Tensor`.\n\n        Args:\n            tensor: Padded Tensor.\n            mask: Tensor Mask.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; padded_tensor = torch.tensor([[1, 2, 3, 0, 0],\n            ...                                [4, 5, 0, 0, 0],\n            ...                                [6, 7, 8, 9, 0]])\n            &gt;&gt;&gt; mask_tensor = torch.tensor([[1, 1, 1, 0, 0],\n            ...                             [1, 1, 0, 0, 0],\n            ...                             [1, 1, 1, 1, 0]])\n            &gt;&gt;&gt; nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)\n            &gt;&gt;&gt; nested_tensor.tensor\n            tensor([[1, 2, 3, 0],\n                    [4, 5, 0, 0],\n                    [6, 7, 8, 9]])\n        \"\"\"\n\n        if mask.ndim == 2:\n            return cls(t[slice(0, m.sum())] for t, m in zip(tensor, mask))\n        return cls(\n            t[[slice(0, (m.sum(dim=dim) &gt; 0).sum().item()) for dim in reversed(range(m.dim()))]]\n            for t, m in zip(tensor, mask)\n        )\n\n    def nested_like(self, other: Tensor, unsafe: bool = False) -&gt; NestedTensor:\n        r\"\"\"\n        Create a new `NestedTensor` from a `Tensor`.\n        The newly created `NestedTensor` will have the same shape as current `NestedTensor`.\n\n        Args:\n            other: The `Tensor` to be nested.\n            unsafe: Whether to check the shape of `other` and current `NestedTensor`.\n\n        Returns:\n            (NestedTensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; tensor = nested_tensor.tensor\n            &gt;&gt;&gt; new_tensor = nested_tensor.nested_like(tensor)\n            &gt;&gt;&gt; all([(x == y).all() for x, y in zip(nested_tensor.storage(), new_tensor.storage())])\n            True\n            &gt;&gt;&gt; f = nested_tensor.nested_like(torch.randn(2, 2))\n            Traceback (most recent call last):\n            ValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])\n            &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(2, 2), True)\n            &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(3, 3), True)\n            Traceback (most recent call last):\n            ValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\n        \"\"\"  # noqa: E501\n\n        if not unsafe and self.shape != other.shape:\n            raise ValueError(\n                f\"The shape of NestedTensor and input tensor does not match, {self.shape} != {other.shape}\"\n            )\n        if self.size(0) != other.size(0):\n            raise ValueError(\n                f\"The batch size of NestedTensor and input tensor does not match, {self.size(0)} != {other.size(0)}\"\n            )\n        return NestedTensor([o[tuple(slice(0, dim) for dim in t.shape)] for t, o in zip(self._storage, other)])\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of the NestedTensor.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.device\n            device(type='cpu')\n        \"\"\"\n\n        return self._device(tuple(self._storage))\n\n    @property\n    def shape(self) -&gt; torch.Size | int:\n        r\"\"\"\n        Alias for `size()`.\n        \"\"\"\n\n        return self.size()\n\n    @property\n    def ndim(self) -&gt; int:\n        r\"\"\"\n        Alias for `dim()`.\n        \"\"\"\n\n        return self.dim()\n\n    def size(self, dim: int | None = None) -&gt; torch.Size | int:\n        r\"\"\"\n        Returns the size of the self `NestedTensor`.\n\n        Args:\n            dim: If not specified, the returned value is a `torch.Size`, a subclass of `tuple`.\n                If specified, returns an `int` holding the size of that dimension.\n                Defaults to `None`.\n\n        Returns:\n            (torch.Size | int):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.size()\n            torch.Size([2, 3])\n            &gt;&gt;&gt; nested_tensor.size(0)\n            2\n            &gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n            &gt;&gt;&gt; nested_tensor.shape\n            torch.Size([2, 4])\n            &gt;&gt;&gt; nested_tensor.size(1)\n            4\n        \"\"\"\n\n        return self._size(tuple(self._storage), dim, self.batch_first)\n\n    def dim(self) -&gt; int:\n        r\"\"\"\n        Number of dimension of the NestedTensor.\n\n        Returns:\n            (int):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.dim()\n            2\n            &gt;&gt;&gt; nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))\n            &gt;&gt;&gt; nested_tensor.ndim\n            2\n        \"\"\"\n\n        return self._dim(tuple(self._storage))\n\n    def tolist(self) -&gt; list:\n        return [t.tolist() for t in self._storage]\n\n    def where(self, condition, other) -&gt; NestedTensor:\n        r\"\"\"\n        Return a NestedTensor of elements selected from either self or other, depending on condition.\n\n        Returns:\n            (NestedTensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.size()\n            torch.Size([2, 3])\n            &gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n            &gt;&gt;&gt; nested_tensor.size()\n            torch.Size([2, 4])\n        \"\"\"\n\n        if isinstance(condition, NestedTensor) and isinstance(other, NestedTensor):\n            return NestedTensor(\n                [x.where(c, y) for x, c, y in zip(self._storage, condition._storage, other._storage)], **self._state\n            )\n        if isinstance(condition, NestedTensor):\n            return NestedTensor([x.where(c, other) for x, c in zip(self._storage, condition._storage)], **self._state)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x.where(condition, y) for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor(x.where(condition, other) for x in self._storage)\n\n    def __abs__(self):\n        return NestedTensor([abs(value) for value in self._storage], **self._state)\n\n    def __add__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x + y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value + other for value in self._storage], **self._state)\n\n    def __radd__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y + x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other + value for value in self._storage], **self._state)\n\n    def __iadd__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x += y\n        else:\n            for value in self._storage:\n                value += other\n        return self\n\n    def __and__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x &amp; y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value &amp; other for value in self._storage], **self._state)\n\n    def __rand__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y &amp; x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other &amp; value for value in self._storage], **self._state)\n\n    def __iand__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x &amp;= y\n        else:\n            for value in self._storage:\n                value &amp;= other\n        return self\n\n    def __floordiv__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x // y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value // other for value in self._storage], **self._state)\n\n    def __rfloordiv__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y // x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other // value for value in self._storage], **self._state)\n\n    def __ifloordiv__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x //= y\n        else:\n            for value in self._storage:\n                value //= other\n        return self\n\n    def __mod__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x % y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value % other for value in self._storage], **self._state)\n\n    def __rmod__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y % x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other % value for value in self._storage], **self._state)\n\n    def __imod__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x %= y\n        else:\n            for value in self._storage:\n                value %= other\n        return self\n\n    def __mul__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x * y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value * other for value in self._storage], **self._state)\n\n    def __rmul__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y * x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other * value for value in self._storage], **self._state)\n\n    def __imul__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x *= y\n        else:\n            for value in self._storage:\n                value *= other\n        return self\n\n    def __matmul__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x @ y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value @ other for value in self._storage], **self._state)\n\n    def __rmatmul__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y @ x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other @ value for value in self._storage], **self._state)\n\n    def __imatmul__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x @= y\n        else:\n            for value in self._storage:\n                value @= other\n        return self\n\n    def __pow__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x**y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value**other for value in self._storage], **self._state)\n\n    def __rpow__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y**x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other**value for value in self._storage], **self._state)\n\n    def __ipow__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x *= y\n        else:\n            for value in self._storage:\n                value *= other\n        return self\n\n    def __truediv__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x / y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value / other for value in self._storage], **self._state)\n\n    def __rtruediv__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y / x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other / value for value in self._storage], **self._state)\n\n    def __itruediv__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x /= y\n        else:\n            for value in self._storage:\n                value /= other\n        return self\n\n    def __sub__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x - y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value - other for value in self._storage], **self._state)\n\n    def __rsub__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y - x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other - value for value in self._storage], **self._state)\n\n    def __isub__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x -= y\n        else:\n            for value in self._storage:\n                value -= other\n        return self\n\n    def __getitem__(self, index: int | slice | tuple) -&gt; tuple[Tensor, Tensor] | NestedTensor:\n        if isinstance(index, tuple):\n            return NestedTensor([t[index[0]][index[1:]] for t in self._storage])\n        if isinstance(index, (int, slice)):\n            ret = self._storage[index]\n            if isinstance(ret, Tensor):\n                return ret, torch.ones_like(ret, dtype=torch.bool)\n            return self.tensor, self.mask\n        raise ValueError(f\"Unsupported index type {type(index)}\")\n\n    def __getattr__(self, name) -&gt; Any:\n        if not self._storage:\n            raise ValueError(f\"Unable to get {name} from an empty {self.__class__.__name__}\")\n        ret = [getattr(i, name) for i in self._storage]\n        elem = ret[0]\n        if isinstance(elem, Tensor):\n            return NestedTensor(ret, **self._state)\n        if callable(elem):\n            return NestedTensorFuncWrapper(ret, state=self._state)\n        if elem.__hash__ is not None and len(set(ret)) == 1:\n            return elem\n        return ret\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        if func not in NestedTensorFunc or not all(issubclass(t, (torch.Tensor, NestedTensor)) for t in types):\n            args = [a.tensor if hasattr(a, \"tensor\") else a for a in args]\n            return func(*args, **kwargs)\n        return NestedTensorFunc[func](*args, **kwargs)\n\n    def __len__(self) -&gt; int:\n        return len(self._storage)\n\n    def __eq__(  # type: ignore[override]\n        self, other: Tensor | NestedTensor | SupportsFloat\n    ) -&gt; bool | Tensor | NestedTensor:\n        if isinstance(other, NestedTensor):\n            return NestedTensor(i == j for i, j in zip(self._storage, other._storage))\n        if isinstance(other, Tensor):\n            return self.tensor == other\n        if isinstance(other, SupportsFloat):\n            return NestedTensor([x == other for x in self._storage], **self._state)\n        return False\n\n    @property\n    def _state(self) -&gt; Mapping:\n        return {k: v for k, v in self.__dict__.items() if k != \"_storage\"}\n\n    def __state__(self) -&gt; Mapping:\n        return self.__dict__\n\n    def __setstate__(self, state: Mapping) -&gt; None:\n        self.__dict__.update(state)\n\n    def __repr__(self):\n        return self.__class__.__name__ + repr(self.tensor)[len(self.tensor.__class__.__name__) :]  # noqa: E203\n\n    @staticmethod\n    @lru_cache(maxsize=1)\n    def _tensor(storage, batch_first, padding_value: float = 0) -&gt; Tensor:\n        if storage[0].dim() == 0:\n            return torch.stack(storage, dim=0)\n        return pad_tensor(storage, batch_first=batch_first, padding_value=padding_value)\n\n    @staticmethod\n    @lru_cache(maxsize=1)\n    def _mask(storage, batch_first, mask_value: bool = False) -&gt; Tensor:\n        if storage[0].dim() == 0:\n            return torch.full(len(storage), fill_value=not mask_value, dtype=torch.bool, device=storage[0].device)\n        return mask_tensor(storage, batch_first=batch_first, mask_value=mask_value)\n\n    @staticmethod\n    @lru_cache(maxsize=1)\n    def _device(storage) -&gt; torch.device:\n        return storage[0].device\n\n    @staticmethod\n    @lru_cache(maxsize=1)\n    def _size(storage, dim: int | None = None, batch_first: bool = True) -&gt; torch.Size | int:\n        if dim is not None:\n            if dim == 0:\n                return len(storage)\n            return max(t.size(dim - 1) for t in storage)\n        if max(t.dim() for t in storage) == 0:\n            return torch.Size((len(storage),))\n        ndim = max(t.dim() for t in storage)\n        size = [max(t.shape[i] if i &lt; len(t.shape) else 0 for t in storage) for i in range(ndim)]\n        size.insert(0 if batch_first else 1, len(storage))\n        return torch.Size(size)\n\n    @staticmethod\n    @lru_cache(maxsize=1)\n    def _dim(storage) -&gt; int:\n        return max(t.dim() for t in storage) + 1\n</code></pre>"},{"location":"package/#danling.NestedTensor.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Device of the NestedTensor.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n</code></pre>"},{"location":"package/#danling.NestedTensor.mask","title":"<code>mask: Tensor</code>  <code>property</code>","text":"<p>Padding mask of <code>tensor</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n</code></pre>"},{"location":"package/#danling.NestedTensor.ndim","title":"<code>ndim: int</code>  <code>property</code>","text":"<p>Alias for <code>dim()</code>.</p>"},{"location":"package/#danling.NestedTensor.shape","title":"<code>shape: torch.Size | int</code>  <code>property</code>","text":"<p>Alias for <code>size()</code>.</p>"},{"location":"package/#danling.NestedTensor.tensor","title":"<code>tensor: Tensor</code>  <code>property</code>","text":"<p>Return a single tensor by padding all the tensors.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre>"},{"location":"package/#danling.NestedTensor.dim","title":"<code>dim()</code>","text":"<p>Number of dimension of the NestedTensor.</p> <p>Returns:</p> Type Description <code>int</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.dim()\n2\n&gt;&gt;&gt; nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))\n&gt;&gt;&gt; nested_tensor.ndim\n2\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def dim(self) -&gt; int:\n    r\"\"\"\n    Number of dimension of the NestedTensor.\n\n    Returns:\n        (int):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.dim()\n        2\n        &gt;&gt;&gt; nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))\n        &gt;&gt;&gt; nested_tensor.ndim\n        2\n    \"\"\"\n\n    return self._dim(tuple(self._storage))\n</code></pre>"},{"location":"package/#danling.NestedTensor.from_tensor_mask","title":"<code>from_tensor_mask(tensor, mask)</code>  <code>classmethod</code>","text":"<p>Build a <code>NestedTensor</code> object from a padded <code>Tensor</code> and corresponding mask <code>Tensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Padded Tensor.</p> required <code>mask</code> <code>Tensor</code> <p>Tensor Mask.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; padded_tensor = torch.tensor([[1, 2, 3, 0, 0],\n...                                [4, 5, 0, 0, 0],\n...                                [6, 7, 8, 9, 0]])\n&gt;&gt;&gt; mask_tensor = torch.tensor([[1, 1, 1, 0, 0],\n...                             [1, 1, 0, 0, 0],\n...                             [1, 1, 1, 1, 0]])\n&gt;&gt;&gt; nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3, 0],\n        [4, 5, 0, 0],\n        [6, 7, 8, 9]])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@classmethod\ndef from_tensor_mask(cls, tensor: Tensor, mask: Tensor):\n    r\"\"\"\n    Build a `NestedTensor` object from a padded `Tensor` and corresponding mask `Tensor`.\n\n    Args:\n        tensor: Padded Tensor.\n        mask: Tensor Mask.\n\n    Returns:\n        (torch.Tensor):\n\n    Examples:\n        &gt;&gt;&gt; padded_tensor = torch.tensor([[1, 2, 3, 0, 0],\n        ...                                [4, 5, 0, 0, 0],\n        ...                                [6, 7, 8, 9, 0]])\n        &gt;&gt;&gt; mask_tensor = torch.tensor([[1, 1, 1, 0, 0],\n        ...                             [1, 1, 0, 0, 0],\n        ...                             [1, 1, 1, 1, 0]])\n        &gt;&gt;&gt; nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)\n        &gt;&gt;&gt; nested_tensor.tensor\n        tensor([[1, 2, 3, 0],\n                [4, 5, 0, 0],\n                [6, 7, 8, 9]])\n    \"\"\"\n\n    if mask.ndim == 2:\n        return cls(t[slice(0, m.sum())] for t, m in zip(tensor, mask))\n    return cls(\n        t[[slice(0, (m.sum(dim=dim) &gt; 0).sum().item()) for dim in reversed(range(m.dim()))]]\n        for t, m in zip(tensor, mask)\n    )\n</code></pre>"},{"location":"package/#danling.NestedTensor.nested_like","title":"<code>nested_like(other, unsafe=False)</code>","text":"<p>Create a new <code>NestedTensor</code> from a <code>Tensor</code>. The newly created <code>NestedTensor</code> will have the same shape as current <code>NestedTensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Tensor</code> <p>The <code>Tensor</code> to be nested.</p> required <code>unsafe</code> <code>bool</code> <p>Whether to check the shape of <code>other</code> and current <code>NestedTensor</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>NestedTensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; tensor = nested_tensor.tensor\n&gt;&gt;&gt; new_tensor = nested_tensor.nested_like(tensor)\n&gt;&gt;&gt; all([(x == y).all() for x, y in zip(nested_tensor.storage(), new_tensor.storage())])\nTrue\n&gt;&gt;&gt; f = nested_tensor.nested_like(torch.randn(2, 2))\nTraceback (most recent call last):\nValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])\n&gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(2, 2), True)\n&gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(3, 3), True)\nTraceback (most recent call last):\nValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def nested_like(self, other: Tensor, unsafe: bool = False) -&gt; NestedTensor:\n    r\"\"\"\n    Create a new `NestedTensor` from a `Tensor`.\n    The newly created `NestedTensor` will have the same shape as current `NestedTensor`.\n\n    Args:\n        other: The `Tensor` to be nested.\n        unsafe: Whether to check the shape of `other` and current `NestedTensor`.\n\n    Returns:\n        (NestedTensor):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; tensor = nested_tensor.tensor\n        &gt;&gt;&gt; new_tensor = nested_tensor.nested_like(tensor)\n        &gt;&gt;&gt; all([(x == y).all() for x, y in zip(nested_tensor.storage(), new_tensor.storage())])\n        True\n        &gt;&gt;&gt; f = nested_tensor.nested_like(torch.randn(2, 2))\n        Traceback (most recent call last):\n        ValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])\n        &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(2, 2), True)\n        &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(3, 3), True)\n        Traceback (most recent call last):\n        ValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\n    \"\"\"  # noqa: E501\n\n    if not unsafe and self.shape != other.shape:\n        raise ValueError(\n            f\"The shape of NestedTensor and input tensor does not match, {self.shape} != {other.shape}\"\n        )\n    if self.size(0) != other.size(0):\n        raise ValueError(\n            f\"The batch size of NestedTensor and input tensor does not match, {self.size(0)} != {other.size(0)}\"\n        )\n    return NestedTensor([o[tuple(slice(0, dim) for dim in t.shape)] for t, o in zip(self._storage, other)])\n</code></pre>"},{"location":"package/#danling.NestedTensor.size","title":"<code>size(dim=None)</code>","text":"<p>Returns the size of the self <code>NestedTensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int | None</code> <p>If not specified, the returned value is a <code>torch.Size</code>, a subclass of <code>tuple</code>. If specified, returns an <code>int</code> holding the size of that dimension. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Size | int</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.size(0)\n2\n&gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 4])\n&gt;&gt;&gt; nested_tensor.size(1)\n4\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def size(self, dim: int | None = None) -&gt; torch.Size | int:\n    r\"\"\"\n    Returns the size of the self `NestedTensor`.\n\n    Args:\n        dim: If not specified, the returned value is a `torch.Size`, a subclass of `tuple`.\n            If specified, returns an `int` holding the size of that dimension.\n            Defaults to `None`.\n\n    Returns:\n        (torch.Size | int):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.size(0)\n        2\n        &gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 4])\n        &gt;&gt;&gt; nested_tensor.size(1)\n        4\n    \"\"\"\n\n    return self._size(tuple(self._storage), dim, self.batch_first)\n</code></pre>"},{"location":"package/#danling.NestedTensor.where","title":"<code>where(condition, other)</code>","text":"<p>Return a NestedTensor of elements selected from either self or other, depending on condition.</p> <p>Returns:</p> Type Description <code>NestedTensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 4])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def where(self, condition, other) -&gt; NestedTensor:\n    r\"\"\"\n    Return a NestedTensor of elements selected from either self or other, depending on condition.\n\n    Returns:\n        (NestedTensor):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 4])\n    \"\"\"\n\n    if isinstance(condition, NestedTensor) and isinstance(other, NestedTensor):\n        return NestedTensor(\n            [x.where(c, y) for x, c, y in zip(self._storage, condition._storage, other._storage)], **self._state\n        )\n    if isinstance(condition, NestedTensor):\n        return NestedTensor([x.where(c, other) for x, c in zip(self._storage, condition._storage)], **self._state)\n    if isinstance(other, NestedTensor):\n        return NestedTensor([x.where(condition, y) for x, y in zip(self._storage, other._storage)], **self._state)\n    return NestedTensor(x.where(condition, other) for x in self._storage)\n</code></pre>"},{"location":"package/#danling.PNTensor","title":"<code>PNTensor</code>","text":"<p>             Bases: <code>Tensor</code></p> <p>Wrapper for tensors to be converted to <code>NestedTensor</code>.</p> <p><code>PNTensor</code> is a subclass of <code>torch.Tensor</code>. It implements two additional methods as <code>NestedTensor</code>: <code>tensor</code> and <code>mask</code>.</p> <p>Although it is possible to construct <code>NestedTensor</code> in dataset, the best practice is to do so in <code>collate_fn</code>. However, it is hard to tell if a batch of <code>Tensor</code> should be stacked or converted to <code>NestedTensor</code>.</p> <p><code>PNTensor</code> is introduced overcome this limitation.</p> <p>Convert tensors that will be converted to <code>NestedTensor</code> to a <code>PNTensor</code>, and all you need to do is to convert <code>PNTensor</code> to <code>NestedTensor</code> in <code>collate_fn</code>.</p> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class PNTensor(Tensor):\n    r\"\"\"\n    Wrapper for tensors to be converted to `NestedTensor`.\n\n    `PNTensor` is a subclass of `torch.Tensor`.\n    It implements two additional methods as `NestedTensor`: `tensor` and `mask`.\n\n    Although it is possible to construct `NestedTensor` in dataset,\n    the best practice is to do so in `collate_fn`.\n    However, it is hard to tell if a batch of `Tensor` should be stacked or converted to `NestedTensor`.\n\n    `PNTensor` is introduced overcome this limitation.\n\n    Convert tensors that will be converted to `NestedTensor` to a `PNTensor`,\n    and all you need to do is to convert `PNTensor` to `NestedTensor` in `collate_fn`.\n    \"\"\"\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Identical to `self`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n            &gt;&gt;&gt; pn_tensor = PNTensor(tensor)\n            &gt;&gt;&gt; (tensor == pn_tensor).all()\n            PNTensor(True)\n            &gt;&gt;&gt; (tensor == pn_tensor.tensor).all()\n            PNTensor(True)\n        \"\"\"\n\n        return self\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Identical to `torch.ones_like(self)`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n            &gt;&gt;&gt; pn_tensor = PNTensor(tensor)\n            &gt;&gt;&gt; (pn_tensor.mask == torch.ones_like(pn_tensor)).all()\n            PNTensor(True)\n        \"\"\"\n\n        return torch.ones_like(self)\n\n    def new_empty(self, *args, **kwargs):\n        return PNTensor(super().new_empty(*args, **kwargs))\n</code></pre>"},{"location":"package/#danling.PNTensor.mask","title":"<code>mask: Tensor</code>  <code>property</code>","text":"<p>Identical to <code>torch.ones_like(self)</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; pn_tensor = PNTensor(tensor)\n&gt;&gt;&gt; (pn_tensor.mask == torch.ones_like(pn_tensor)).all()\nPNTensor(True)\n</code></pre>"},{"location":"package/#danling.PNTensor.tensor","title":"<code>tensor: Tensor</code>  <code>property</code>","text":"<p>Identical to <code>self</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; pn_tensor = PNTensor(tensor)\n&gt;&gt;&gt; (tensor == pn_tensor).all()\nPNTensor(True)\n&gt;&gt;&gt; (tensor == pn_tensor.tensor).all()\nPNTensor(True)\n</code></pre>"},{"location":"package/#danling.TorchRunner","title":"<code>TorchRunner</code>","text":"<p>             Bases: <code>BaseRunner</code></p> <p>Set up everything for running a job.</p> <p><code>TorchRunner</code> uses <code>accelerate</code> as distributed backend to provide seamless distributed training experience.</p> <p><code>TorchRunner</code> will automatically <code>prepare</code> everything, including <code>model</code>, <code>criterion</code>, <code>optimizer</code>, <code>scheduler</code>, and <code>dataloaders</code> for distribute training, mixed precision, and deepspeed (optional).</p> <p>In fact, you don\u2019t even need to create <code>dataloaders</code>, just define <code>datasets</code> and <code>TorchRunner</code> will create <code>dataloaders</code> for you. <code>TorchRunner</code> will inspect the <code>train</code> flag in corresponding dataset to automatically set <code>shuffle</code>.</p> <p>Attributes:</p> Name Type Description <code>accelerator</code> <code>Accelerator</code> <code>accelerate</code> <code>dict</code> <p>Arguments to pass when building accelerator. Defaults to <code>{}</code>.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>class TorchRunner(BaseRunner):  # pylint: disable=too-many-public-methods\n    r\"\"\"\n    Set up everything for running a job.\n\n    `TorchRunner` uses [`accelerate`][accelerate] as distributed backend to\n    provide seamless distributed training experience.\n\n    `TorchRunner` will automatically [`prepare`][accelerate.Accelerator.prepare] everything,\n    including `model`, `criterion`, `optimizer`, `scheduler`, and `dataloaders` for distribute training,\n    mixed precision, and deepspeed (optional).\n\n    In fact, you don't even need to create `dataloaders`, just define\n    `datasets` and `TorchRunner` will create `dataloaders` for you.\n    `TorchRunner` will inspect the `train` flag in corresponding dataset to\n    automatically set `shuffle`.\n\n    Attributes:\n        accelerator (Accelerator):\n        accelerate: Arguments to pass when building accelerator. Defaults to `{}`.\n    \"\"\"\n\n    accelerator: Accelerator\n    accelerate: dict\n\n    model: nn.Module\n    criterion: nn.Module\n    optimizer: optim.Optimizer\n    scheduler: optim.lr_scheduler._LRScheduler\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        if len(args) != 1 or kwargs:\n            message = (\n                \"Passing multiple args &amp; kwargs to build Runner is deprecated and will be removed in DanLing v0.3.\\n\"\n                \"Please only pass a config dict instead.\"\n            )\n            warn(message, DeprecationWarning, stacklevel=2)\n            config = NestedDict(*args, **kwargs)\n        else:\n            config = args[0]\n        if \"accelerate\" not in self:  # class attributes\n            self.accelerate = {}\n        self.accelerate.update(config.get(\"accelerate\", {}))\n        super().__init__(config)\n\n    def __post_init__(self, *args, **kwargs) -&gt; None:\n        self._prepare()\n\n    def _prepare(self):\n        if self.datasets:\n            datasets = {k: d for k, d in self.datasets.items() if k not in self.dataloaders}\n            dataloader_kwargs = self.state.get(\"dataloader\", {})\n            for k, d in datasets.items():\n                shuffle = dataloader_kwargs.shuffle if \"shuffle\" in dataloader_kwargs else getattr(d, \"train\", True)\n                self.dataloaders[k] = utils.data.DataLoader(d, shuffle=shuffle, **dataloader_kwargs)\n        objects = [self.model, self.criterion, self.optimizer, self.scheduler]\n        num_objects = len(objects)\n        dataloader_names = []\n        for name, dataloader in self.dataloaders.items():\n            dataloader_names.append(name)\n            objects.append(dataloader)\n        objects = self.prepare(*objects)\n        self.model, self.criterion, self.optimizer, self.scheduler = objects[:num_objects]\n        if len(objects) != len(dataloader_names) + num_objects:\n            raise ValueError(\"Number of dataloaders does not match.\")\n        for name, dataloader in zip(dataloader_names, objects[num_objects:]):\n            self.dataloaders[name] = dataloader\n\n    @property\n    def deepspeed(self) -&gt; dict | None:\n        if \"accelerator\" not in self:\n            raise ValueError(\"accelerator is not used\")\n        if self.accelerator.state.deepspeed_plugin is not None:\n            return self.accelerator.state.deepspeed_plugin.deepspeed_config\n        return None\n\n    def train(self, train_splits: list[str] | None = None, eval_splits: list[str] | None = None) -&gt; NestedDict:\n        r\"\"\"\n        Perform training on `split`.\n\n        Args:\n            train_splits (list[str]): list of split to run train.\n                Defaults to `[\"train\"]`.\n            eval_splits (list[str]): list of split to run evaluate.\n                Defaults to `self.dataloaders` except for those in `train_splits`.\n\n        Return:\n            NestedDict: train results\n        \"\"\"\n\n        early_stop_counter = 0\n        if train_splits is None:\n            train_splits = [\"train\"]\n        if eval_splits is None:\n            eval_splits = [s for s in self.dataloaders if s not in train_splits]\n        self.state.epoch_begin = self.state.epochs\n        print(f\"Begin training from {self.state.epoch_begin} to {self.state.epoch_end}\")\n        print(f\"Training splits: {train_splits}\")\n        print(f\"Evaluation splits: {eval_splits}\")\n        patience = self.state.get(\"patience\", float(\"inf\"))\n        for epochs in range(self.state.epoch_begin, self.state.epoch_end):  # type: ignore\n            self.state.epochs = epochs\n            result = NestedDict()\n            result.setattr(\"convert_mapping\", True)\n            for split in train_splits:\n                result[split] = self.train_epoch(split)\n            for split in eval_splits:\n                result[split] = self.evaluate_epoch(split)\n            self.append_result(result)\n            print(self.format_epoch_result(result))\n            self.save_result()\n            self.save_checkpoint()\n            \"\"\"@nni.report_intermediate_result(self.latest_score)\"\"\"\n            early_stop_counter = 0 if self.is_best else early_stop_counter + 1\n            if early_stop_counter &gt; patience:\n                print(\"early stop\")\n                break\n        \"\"\"@nni.report_final_result(self.latest_score)\"\"\"\n        return self.results\n\n    def train_epoch(self, split: str = \"train\") -&gt; NestedDict:\n        r\"\"\"\n        Train one epoch on `split`.\n\n        Args:\n            split (str): split to run train\n\n        Return:\n            NestedDict: train result\n        \"\"\"\n\n        self.mode = \"train\"  # type: ignore\n        loader = self.dataloaders[split]\n        length = len(loader) - 1\n        last_print_iteration = -1\n        self.meters.reset()\n        if self.metrics is not None:\n            self.metrics.reset()\n        batch_time = time()\n        if hasattr(loader.batch_sampler, \"set_epoch\"):\n            loader.batch_sampler.set_epoch(self.epochs)\n        if hasattr(loader.sampler, \"set_epoch\"):\n            loader.sampler.set_epoch(self.epochs)\n\n        for iteration, data in enumerate(loader):\n            with self.autocast(), self.accumulate():\n                input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n                target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n                pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n                loss = self.criterion(pred, target)\n                if self.metrics is not None:\n                    self.metrics.update(pred, target)\n                self.step(loss)\n\n            if self.print_interval &gt; 0 and (\n                iteration &gt; 0 and iteration % self.print_interval == 0 or iteration == length\n            ):\n                interval = iteration - last_print_iteration\n                if self.device == torch.device(\"cuda\"):\n                    torch.cuda.synchronize()\n                self.meters.time.update((time() - batch_time) / interval)\n                batch_time = time()\n                reduced_loss = self.reduce(loss).item()\n                self.meters.loss.update(reduced_loss)\n                self.step_log(split, iteration, length)\n                last_print_iteration = iteration\n\n        result = self.meters.avg\n        if self.metrics is not None:\n            result.merge(self.metrics.avg)\n        return result\n\n    def evaluate(self, eval_splits: list[str] | None = None) -&gt; NestedDict:\n        r\"\"\"\n        Perform evaluation on `eval_splits`.\n\n        Args:\n            eval_splits (list[str]): list of split to run evaluate.\n                Defaults to `[\"eval\"]`.\n\n        Return:\n            NestedDict: evaluation result\n        \"\"\"\n\n        if eval_splits is None:\n            eval_splits = [\"eval\"]\n\n        print(\"Begin evaluation\")\n        print(f\"Evaluation splits: {eval_splits}\")\n        result = NestedDict()\n        result.setattr(\"convert_mapping\", True)\n        for split in eval_splits:\n            result[split] = self.evaluate_epoch(split=split)\n        print(self.format_epoch_result(result))\n        return result\n\n    @torch.inference_mode()\n    def evaluate_epoch(self, split: str = \"val\") -&gt; NestedDict:\n        r\"\"\"\n        Evaluate one epoch on `split`.\n\n        Args:\n            split (str): split to run evaluate\n\n        Return:\n            NestedDict: evaluation result\n        \"\"\"\n\n        self.mode = \"eval\"  # type: ignore\n        loader = self.dataloaders[split]\n        length = len(loader) - 1\n        last_print_iteration = -1\n        self.meters.reset()\n        if self.metrics is not None:\n            self.metrics.reset()\n        batch_time = time()\n\n        for iteration, data in enumerate(loader):\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            loss = self.criterion(pred, target)\n            if self.metrics is not None:\n                self.metrics.update(pred, target)\n\n            if self.print_interval &gt; 0 and (\n                iteration &gt; 0 and iteration % self.print_interval == 0 or iteration == length\n            ):\n                interval = iteration - last_print_iteration\n                if self.device == torch.device(\"cuda\"):\n                    torch.cuda.synchronize()\n                self.meters.time.update((time() - batch_time) / interval)\n                batch_time = time()\n                reduced_loss = self.reduce(loss).item()\n                self.meters.loss.update(reduced_loss)\n                self.step_log(split, iteration, length)\n                last_print_iteration = iteration\n\n        result = self.meters.avg\n        if self.metrics is not None:\n            result.merge(self.metrics.avg)\n        self.write_result(result, split, self.state.epochs)\n        return result\n\n    @torch.inference_mode()\n    def inference(self, split: str = \"inf\") -&gt; list:\n        r\"\"\"\n        Perform inference on `split`.\n\n        Args:\n            split (str): split to run inference\n\n        Return:\n            Tensor: inference outputs\n        \"\"\"\n\n        self.mode = \"inf\"  # type: ignore\n        loader = self.dataloaders[split]\n        self.meters.reset()\n        output = []\n        for _, data in tqdm(enumerate(loader), total=len(loader)):\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            output.extend(pred.tolist())\n\n        if self.distributed:\n            torch.cuda.synchronize()\n            output = self.gather_for_metrics(output)\n        return output\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Set up distributed training.\n\n        Initialise process group and set up DDP variables.\n        \"\"\"\n\n        if os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\").lower() == \"true\":\n            deepspeed_config = self.state.get(\"deepspeed\", os.environ.get(\"ACCELERATE_DEEPSPEED_CONFIG_FILE\"))\n            self.accelerate[\"deepspeed_plugin\"] = DeepSpeedPlugin(hf_ds_config=self.init_deepspeed(deepspeed_config))\n        self.accelerator = Accelerator(**self.accelerate)\n        if self.distributed:\n            object_list = [self.state.id]\n            dist.broadcast_object_list(object_list)\n            self.state.id = object_list[0]\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n        if \"log_dir\" not in kwargs:\n            kwargs[\"log_dir\"] = self.dir\n\n        self.writer = SummaryWriter(*args, **kwargs)\n        self.writer.add_scalar = catch(OSError, verbose=False)(self.writer.add_scalar)\n\n    def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Args:\n            seed: Random seed to set.\n                Defaults to `self.state.seed` (`config.seed`).\n\n            bias: Make the seed different for each processes.\n                This is used to ensure the data augmentation are applied differently on every processes.\n                Defaults to `self.rank`.\n                Set to `False` to disable this feature.\n        \"\"\"\n\n        seed = seed or self.state.seed\n        if self.distributed:\n            object_list = [seed]\n            dist.broadcast_object_list(object_list)\n            seed = object_list[0]\n        bias = bias or self.rank\n        if bias:\n            seed += bias\n        self.state.seed = seed\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        if np_random is not None:\n            np_random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n        if torch.__version__ &gt;= \"1.8.0\":\n            torch.use_deterministic_algorithms(True)\n\n    def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n        r\"\"\"\n        Backward loss and step optimizer &amp; scheduler.\n\n        This method increment `self.state.steps`.\n\n        This method also increment `self.state.iters` when `batch_size` is specified.\n\n        Args:\n            zero_grad: Whether to zero the gradients.\n        \"\"\"\n\n        self.accelerator.backward(loss)\n        if self.sync_gradients:\n            if self.state.get(\"max_grad_value\") is not None:\n                self.clip_grad_value_(self.model.parameters(), self.state.get(\"max_grad_value\"))\n            if self.state.get(\"max_grad_norm\") is not None:\n                self.clip_grad_norm_(self.model.parameters(), self.state.get(\"max_grad_norm\"))\n        if self.optimizer is not None:\n            self.optimizer.step()\n            if zero_grad:\n                self.optimizer.zero_grad()\n        if self.scheduler is not None:\n            self.scheduler.step()\n        self.state.steps += 1\n        if batch_size is None:\n            batch_size = self.batch_size_equivalent\n        self.state.iters += batch_size\n        # TODO: Support `drop_last = False`\n        # self.state.iters += self.batch_size_equivalent\n\n    def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Return dict of all attributes for checkpoint.\n        \"\"\"\n\n        if self.model is None:\n            raise ValueError(\"Model must be defined when calling state_dict\")\n        model = self.accelerator.unwrap_model(self.model)\n        return cls(\n            runner=self.state.dict(),\n            model=model.state_dict(),\n            optimizer=self.optimizer.state_dict() if self.optimizer else None,\n            scheduler=self.scheduler.state_dict() if self.scheduler else None,\n        )\n\n    def prepare(self, *args, device_placement: list[bool] | None = None) -&gt; None:\n        r\"\"\"\n        Prepare all objects passed in `args` for distributed training and mixed precision,\n        then return them in the same order.\n        \"\"\"\n\n        return self.accelerator.prepare(*args, device_placement=device_placement)\n\n    def accumulate(self, model: nn.Module | None = None):\n        r\"\"\"\n        Context manager that enables gradient accumulate.\n        \"\"\"\n\n        model = model or self.model\n        return self.accelerator.accumulate(model)\n\n    def autocast(self):\n        r\"\"\"\n        Context manager that enables auto-casting for the forward pass (and maybe backward pass).\n        \"\"\"\n\n        return self.accelerator.autocast()\n\n    def backward(self, loss) -&gt; None:\n        r\"\"\"\n        Backward loss to compute gradients.\n        \"\"\"\n\n        return self.accelerator.backward(loss)\n\n    def unwrap_model(self, model: nn.Module | None = None) -&gt; nn.Module:\n        r\"\"\"\n        Unwrap DDP model.\n\n        Args:\n            model (Optional[nn.Module]):\n                Defaults to `self.model`.\n        \"\"\"\n\n        if model is not None:\n            model = self.model\n        if self.accelerator is not None:\n            return self.accelerator.unwrap_model(model)\n        if self.distributed:\n            return model.module\n        return model\n\n    @property\n    def mode(self) -&gt; RunnerMode:\n        return self._mode\n\n    @mode.setter\n    def mode(self, mode: str | RunnerMode) -&gt; None:\n        if isinstance(mode, str):\n            mode = RunnerMode(mode)\n        self._mode = mode\n        if self.model is not None:\n            self.model.train(mode == RunnerMode.train)\n\n    @property\n    def batch_size(self) -&gt; int:\n        r\"\"\"\n        Batch size.\n\n        Notes:\n            If `train` is in `dataloaders`, then `batch_size` is the batch size of `train`.\n            Otherwise, `batch_size` is the batch size of the first dataloader.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        batch_size = self.state.get(\"dataloader.batch_size\")\n        if batch_size:\n            return batch_size\n        if self.dataloaders:\n            loader = self.dataloaders.get(\"train\", next(iter(self.dataloaders.values())))\n            if loader.batch_size:\n                return loader.batch_size\n            batch_sampler = loader.batch_sampler if loader.batch_sampler is not None else loader.sampler\n            return batch_sampler.batch_size\n        raise AttributeError(\"batch_size could not be inferred, since no dataloader found.\")\n\n    @property\n    def accum_steps(self) -&gt; int:\n        r\"\"\"\n        Gradient accumulation steps.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        return self.accelerator.gradient_accumulation_steps\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of runner.\n        \"\"\"\n\n        return self.accelerator.device\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n        \"\"\"\n\n        return self.accelerator.num_processes\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n        \"\"\"\n\n        return self.accelerator.process_index\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n        \"\"\"\n\n        return self.accelerator.local_process_index\n\n    def gather(self, tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Gather tensor.\n        \"\"\"\n\n        return self.accelerator.gather(tensor)\n\n    def reduce(self, tensor, reduction: str = \"sum\") -&gt; torch.Tensor:\n        r\"\"\"\n        Reduce tensor.\n        \"\"\"\n\n        return self.accelerator.reduce(tensor, reduction=reduction)\n\n    def __getattr__(self, name: str) -&gt; Any:\n        with suppress(AttributeError):\n            return super().__getattr__(name)\n        if \"accelerator\" in self.__dict__ and hasattr(self.accelerator, name):\n            return getattr(self.accelerator, name)\n        raise super().__getattribute__(name)\n</code></pre>"},{"location":"package/#danling.TorchRunner.accum_steps","title":"<code>accum_steps: int</code>  <code>property</code>","text":"<p>Gradient accumulation steps.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"package/#danling.TorchRunner.batch_size","title":"<code>batch_size: int</code>  <code>property</code>","text":"<p>Batch size.</p> Notes <p>If <code>train</code> is in <code>dataloaders</code>, then <code>batch_size</code> is the batch size of <code>train</code>. Otherwise, <code>batch_size</code> is the batch size of the first dataloader.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"package/#danling.TorchRunner.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Device of runner.</p>"},{"location":"package/#danling.TorchRunner.local_rank","title":"<code>local_rank: int</code>  <code>property</code>","text":"<p>Process index in local processes.</p>"},{"location":"package/#danling.TorchRunner.rank","title":"<code>rank: int</code>  <code>property</code>","text":"<p>Process index in all processes.</p>"},{"location":"package/#danling.TorchRunner.world_size","title":"<code>world_size: int</code>  <code>property</code>","text":"<p>Number of Processes.</p>"},{"location":"package/#danling.TorchRunner.accumulate","title":"<code>accumulate(model=None)</code>","text":"<p>Context manager that enables gradient accumulate.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def accumulate(self, model: nn.Module | None = None):\n    r\"\"\"\n    Context manager that enables gradient accumulate.\n    \"\"\"\n\n    model = model or self.model\n    return self.accelerator.accumulate(model)\n</code></pre>"},{"location":"package/#danling.TorchRunner.autocast","title":"<code>autocast()</code>","text":"<p>Context manager that enables auto-casting for the forward pass (and maybe backward pass).</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def autocast(self):\n    r\"\"\"\n    Context manager that enables auto-casting for the forward pass (and maybe backward pass).\n    \"\"\"\n\n    return self.accelerator.autocast()\n</code></pre>"},{"location":"package/#danling.TorchRunner.backward","title":"<code>backward(loss)</code>","text":"<p>Backward loss to compute gradients.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def backward(self, loss) -&gt; None:\n    r\"\"\"\n    Backward loss to compute gradients.\n    \"\"\"\n\n    return self.accelerator.backward(loss)\n</code></pre>"},{"location":"package/#danling.TorchRunner.evaluate","title":"<code>evaluate(eval_splits=None)</code>","text":"<p>Perform evaluation on <code>eval_splits</code>.</p> <p>Parameters:</p> Name Type Description Default <code>eval_splits</code> <code>list[str]</code> <p>list of split to run evaluate. Defaults to <code>[\"eval\"]</code>.</p> <code>None</code> Return Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def evaluate(self, eval_splits: list[str] | None = None) -&gt; NestedDict:\n    r\"\"\"\n    Perform evaluation on `eval_splits`.\n\n    Args:\n        eval_splits (list[str]): list of split to run evaluate.\n            Defaults to `[\"eval\"]`.\n\n    Return:\n        NestedDict: evaluation result\n    \"\"\"\n\n    if eval_splits is None:\n        eval_splits = [\"eval\"]\n\n    print(\"Begin evaluation\")\n    print(f\"Evaluation splits: {eval_splits}\")\n    result = NestedDict()\n    result.setattr(\"convert_mapping\", True)\n    for split in eval_splits:\n        result[split] = self.evaluate_epoch(split=split)\n    print(self.format_epoch_result(result))\n    return result\n</code></pre>"},{"location":"package/#danling.TorchRunner.evaluate_epoch","title":"<code>evaluate_epoch(split='val')</code>","text":"<p>Evaluate one epoch on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run evaluate</p> <code>'val'</code> Return Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@torch.inference_mode()\ndef evaluate_epoch(self, split: str = \"val\") -&gt; NestedDict:\n    r\"\"\"\n    Evaluate one epoch on `split`.\n\n    Args:\n        split (str): split to run evaluate\n\n    Return:\n        NestedDict: evaluation result\n    \"\"\"\n\n    self.mode = \"eval\"  # type: ignore\n    loader = self.dataloaders[split]\n    length = len(loader) - 1\n    last_print_iteration = -1\n    self.meters.reset()\n    if self.metrics is not None:\n        self.metrics.reset()\n    batch_time = time()\n\n    for iteration, data in enumerate(loader):\n        input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n        target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n        pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n        loss = self.criterion(pred, target)\n        if self.metrics is not None:\n            self.metrics.update(pred, target)\n\n        if self.print_interval &gt; 0 and (\n            iteration &gt; 0 and iteration % self.print_interval == 0 or iteration == length\n        ):\n            interval = iteration - last_print_iteration\n            if self.device == torch.device(\"cuda\"):\n                torch.cuda.synchronize()\n            self.meters.time.update((time() - batch_time) / interval)\n            batch_time = time()\n            reduced_loss = self.reduce(loss).item()\n            self.meters.loss.update(reduced_loss)\n            self.step_log(split, iteration, length)\n            last_print_iteration = iteration\n\n    result = self.meters.avg\n    if self.metrics is not None:\n        result.merge(self.metrics.avg)\n    self.write_result(result, split, self.state.epochs)\n    return result\n</code></pre>"},{"location":"package/#danling.TorchRunner.gather","title":"<code>gather(tensor)</code>","text":"<p>Gather tensor.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def gather(self, tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Gather tensor.\n    \"\"\"\n\n    return self.accelerator.gather(tensor)\n</code></pre>"},{"location":"package/#danling.TorchRunner.inference","title":"<code>inference(split='inf')</code>","text":"<p>Perform inference on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run inference</p> <code>'inf'</code> Return Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@torch.inference_mode()\ndef inference(self, split: str = \"inf\") -&gt; list:\n    r\"\"\"\n    Perform inference on `split`.\n\n    Args:\n        split (str): split to run inference\n\n    Return:\n        Tensor: inference outputs\n    \"\"\"\n\n    self.mode = \"inf\"  # type: ignore\n    loader = self.dataloaders[split]\n    self.meters.reset()\n    output = []\n    for _, data in tqdm(enumerate(loader), total=len(loader)):\n        input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n        pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n        output.extend(pred.tolist())\n\n    if self.distributed:\n        torch.cuda.synchronize()\n        output = self.gather_for_metrics(output)\n    return output\n</code></pre>"},{"location":"package/#danling.TorchRunner.init_distributed","title":"<code>init_distributed()</code>","text":"<p>Set up distributed training.</p> <p>Initialise process group and set up DDP variables.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def init_distributed(self) -&gt; None:\n    r\"\"\"\n    Set up distributed training.\n\n    Initialise process group and set up DDP variables.\n    \"\"\"\n\n    if os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\").lower() == \"true\":\n        deepspeed_config = self.state.get(\"deepspeed\", os.environ.get(\"ACCELERATE_DEEPSPEED_CONFIG_FILE\"))\n        self.accelerate[\"deepspeed_plugin\"] = DeepSpeedPlugin(hf_ds_config=self.init_deepspeed(deepspeed_config))\n    self.accelerator = Accelerator(**self.accelerate)\n    if self.distributed:\n        object_list = [self.state.id]\n        dist.broadcast_object_list(object_list)\n        self.state.id = object_list[0]\n</code></pre>"},{"location":"package/#danling.TorchRunner.init_tensorboard","title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n    self.writer.add_scalar = catch(OSError, verbose=False)(self.writer.add_scalar)\n</code></pre>"},{"location":"package/#danling.TorchRunner.prepare","title":"<code>prepare(*args, device_placement=None)</code>","text":"<p>Prepare all objects passed in <code>args</code> for distributed training and mixed precision, then return them in the same order.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def prepare(self, *args, device_placement: list[bool] | None = None) -&gt; None:\n    r\"\"\"\n    Prepare all objects passed in `args` for distributed training and mixed precision,\n    then return them in the same order.\n    \"\"\"\n\n    return self.accelerator.prepare(*args, device_placement=device_placement)\n</code></pre>"},{"location":"package/#danling.TorchRunner.reduce","title":"<code>reduce(tensor, reduction='sum')</code>","text":"<p>Reduce tensor.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def reduce(self, tensor, reduction: str = \"sum\") -&gt; torch.Tensor:\n    r\"\"\"\n    Reduce tensor.\n    \"\"\"\n\n    return self.accelerator.reduce(tensor, reduction=reduction)\n</code></pre>"},{"location":"package/#danling.TorchRunner.set_deterministic","title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    if torch.__version__ &gt;= \"1.8.0\":\n        torch.use_deterministic_algorithms(True)\n</code></pre>"},{"location":"package/#danling.TorchRunner.set_seed","title":"<code>set_seed(seed=None, bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Random seed to set. Defaults to <code>self.state.seed</code> (<code>config.seed</code>).</p> <code>None</code> <code>bias</code> <code>int | None</code> <p>Make the seed different for each processes. This is used to ensure the data augmentation are applied differently on every processes. Defaults to <code>self.rank</code>. Set to <code>False</code> to disable this feature.</p> <code>None</code> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Args:\n        seed: Random seed to set.\n            Defaults to `self.state.seed` (`config.seed`).\n\n        bias: Make the seed different for each processes.\n            This is used to ensure the data augmentation are applied differently on every processes.\n            Defaults to `self.rank`.\n            Set to `False` to disable this feature.\n    \"\"\"\n\n    seed = seed or self.state.seed\n    if self.distributed:\n        object_list = [seed]\n        dist.broadcast_object_list(object_list)\n        seed = object_list[0]\n    bias = bias or self.rank\n    if bias:\n        seed += bias\n    self.state.seed = seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    if np_random is not None:\n        np_random.seed(seed)\n    random.seed(seed)\n</code></pre>"},{"location":"package/#danling.TorchRunner.state_dict","title":"<code>state_dict(cls=dict)</code>","text":"<p>Return dict of all attributes for checkpoint.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Return dict of all attributes for checkpoint.\n    \"\"\"\n\n    if self.model is None:\n        raise ValueError(\"Model must be defined when calling state_dict\")\n    model = self.accelerator.unwrap_model(self.model)\n    return cls(\n        runner=self.state.dict(),\n        model=model.state_dict(),\n        optimizer=self.optimizer.state_dict() if self.optimizer else None,\n        scheduler=self.scheduler.state_dict() if self.scheduler else None,\n    )\n</code></pre>"},{"location":"package/#danling.TorchRunner.step","title":"<code>step(loss, batch_size=None, zero_grad=True)</code>","text":"<p>Backward loss and step optimizer &amp; scheduler.</p> <p>This method increment <code>self.state.steps</code>.</p> <p>This method also increment <code>self.state.iters</code> when <code>batch_size</code> is specified.</p> <p>Parameters:</p> Name Type Description Default <code>zero_grad</code> <code>bool</code> <p>Whether to zero the gradients.</p> <code>True</code> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n    r\"\"\"\n    Backward loss and step optimizer &amp; scheduler.\n\n    This method increment `self.state.steps`.\n\n    This method also increment `self.state.iters` when `batch_size` is specified.\n\n    Args:\n        zero_grad: Whether to zero the gradients.\n    \"\"\"\n\n    self.accelerator.backward(loss)\n    if self.sync_gradients:\n        if self.state.get(\"max_grad_value\") is not None:\n            self.clip_grad_value_(self.model.parameters(), self.state.get(\"max_grad_value\"))\n        if self.state.get(\"max_grad_norm\") is not None:\n            self.clip_grad_norm_(self.model.parameters(), self.state.get(\"max_grad_norm\"))\n    if self.optimizer is not None:\n        self.optimizer.step()\n        if zero_grad:\n            self.optimizer.zero_grad()\n    if self.scheduler is not None:\n        self.scheduler.step()\n    self.state.steps += 1\n    if batch_size is None:\n        batch_size = self.batch_size_equivalent\n    self.state.iters += batch_size\n</code></pre>"},{"location":"package/#danling.TorchRunner.train","title":"<code>train(train_splits=None, eval_splits=None)</code>","text":"<p>Perform training on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>train_splits</code> <code>list[str]</code> <p>list of split to run train. Defaults to <code>[\"train\"]</code>.</p> <code>None</code> <code>eval_splits</code> <code>list[str]</code> <p>list of split to run evaluate. Defaults to <code>self.dataloaders</code> except for those in <code>train_splits</code>.</p> <code>None</code> Return Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def train(self, train_splits: list[str] | None = None, eval_splits: list[str] | None = None) -&gt; NestedDict:\n    r\"\"\"\n    Perform training on `split`.\n\n    Args:\n        train_splits (list[str]): list of split to run train.\n            Defaults to `[\"train\"]`.\n        eval_splits (list[str]): list of split to run evaluate.\n            Defaults to `self.dataloaders` except for those in `train_splits`.\n\n    Return:\n        NestedDict: train results\n    \"\"\"\n\n    early_stop_counter = 0\n    if train_splits is None:\n        train_splits = [\"train\"]\n    if eval_splits is None:\n        eval_splits = [s for s in self.dataloaders if s not in train_splits]\n    self.state.epoch_begin = self.state.epochs\n    print(f\"Begin training from {self.state.epoch_begin} to {self.state.epoch_end}\")\n    print(f\"Training splits: {train_splits}\")\n    print(f\"Evaluation splits: {eval_splits}\")\n    patience = self.state.get(\"patience\", float(\"inf\"))\n    for epochs in range(self.state.epoch_begin, self.state.epoch_end):  # type: ignore\n        self.state.epochs = epochs\n        result = NestedDict()\n        result.setattr(\"convert_mapping\", True)\n        for split in train_splits:\n            result[split] = self.train_epoch(split)\n        for split in eval_splits:\n            result[split] = self.evaluate_epoch(split)\n        self.append_result(result)\n        print(self.format_epoch_result(result))\n        self.save_result()\n        self.save_checkpoint()\n        \"\"\"@nni.report_intermediate_result(self.latest_score)\"\"\"\n        early_stop_counter = 0 if self.is_best else early_stop_counter + 1\n        if early_stop_counter &gt; patience:\n            print(\"early stop\")\n            break\n    \"\"\"@nni.report_final_result(self.latest_score)\"\"\"\n    return self.results\n</code></pre>"},{"location":"package/#danling.TorchRunner.train_epoch","title":"<code>train_epoch(split='train')</code>","text":"<p>Train one epoch on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run train</p> <code>'train'</code> Return Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def train_epoch(self, split: str = \"train\") -&gt; NestedDict:\n    r\"\"\"\n    Train one epoch on `split`.\n\n    Args:\n        split (str): split to run train\n\n    Return:\n        NestedDict: train result\n    \"\"\"\n\n    self.mode = \"train\"  # type: ignore\n    loader = self.dataloaders[split]\n    length = len(loader) - 1\n    last_print_iteration = -1\n    self.meters.reset()\n    if self.metrics is not None:\n        self.metrics.reset()\n    batch_time = time()\n    if hasattr(loader.batch_sampler, \"set_epoch\"):\n        loader.batch_sampler.set_epoch(self.epochs)\n    if hasattr(loader.sampler, \"set_epoch\"):\n        loader.sampler.set_epoch(self.epochs)\n\n    for iteration, data in enumerate(loader):\n        with self.autocast(), self.accumulate():\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            loss = self.criterion(pred, target)\n            if self.metrics is not None:\n                self.metrics.update(pred, target)\n            self.step(loss)\n\n        if self.print_interval &gt; 0 and (\n            iteration &gt; 0 and iteration % self.print_interval == 0 or iteration == length\n        ):\n            interval = iteration - last_print_iteration\n            if self.device == torch.device(\"cuda\"):\n                torch.cuda.synchronize()\n            self.meters.time.update((time() - batch_time) / interval)\n            batch_time = time()\n            reduced_loss = self.reduce(loss).item()\n            self.meters.loss.update(reduced_loss)\n            self.step_log(split, iteration, length)\n            last_print_iteration = iteration\n\n    result = self.meters.avg\n    if self.metrics is not None:\n        result.merge(self.metrics.avg)\n    return result\n</code></pre>"},{"location":"package/#danling.TorchRunner.unwrap_model","title":"<code>unwrap_model(model=None)</code>","text":"<p>Unwrap DDP model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[Module]</code> <p>Defaults to <code>self.model</code>.</p> <code>None</code> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def unwrap_model(self, model: nn.Module | None = None) -&gt; nn.Module:\n    r\"\"\"\n    Unwrap DDP model.\n\n    Args:\n        model (Optional[nn.Module]):\n            Defaults to `self.model`.\n    \"\"\"\n\n    if model is not None:\n        model = self.model\n    if self.accelerator is not None:\n        return self.accelerator.unwrap_model(model)\n    if self.distributed:\n        return model.module\n    return model\n</code></pre>"},{"location":"package/#danling.catch","title":"<code>catch(error=Exception, exclude=None, callback=print_exc, *callback_args, **callback_kwargs)</code>","text":"<p>Decorator to catch <code>error</code> except for <code>exclude</code>. Detailed traceback will be printed to <code>stderr</code>.</p> <p><code>catch</code> is extremely useful for unfatal errors. For example, <code>Runner</code> saves checkpoint regularly, however, this might break running if the space is full. Decorating <code>save</code> method with <code>catch</code> will allow you to catch these errors and continue your running.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exceptions</code> <p>Exceptions to be caught.</p> <code>Exception</code> <code>exclude</code> <code>Exceptions | None</code> <p>Exceptions to be excluded.</p> <code>None</code> <code>callback</code> <code>Callable</code> <p>Callback to be called when an error occurs. The first four arguments to <code>callback</code> are <code>exc</code>, <code>func</code>, <code>args</code>, <code>kwargs</code>. Additional arguments should be passed with <code>*callback_args</code> and <code>**callback_kwargs</code>.</p> <code>print_exc</code> <code>callback_args</code> <p>Arguments to be passed to <code>callback</code>.</p> <code>()</code> <code>callback_kwargs</code> <p>Keyword arguments to be passed to <code>callback</code>.</p> <code>{}</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; def file_not_found(*args, **kwargs):\n...     return exclude\n...     raise FileNotFoundError\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; file_not_found()\n&gt;&gt;&gt; raise ValueError\n&gt;&gt;&gt; assert 1 == 2\n</code></pre> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>@flexible_decorator\ndef catch(  # pylint: disable=keyword-arg-before-vararg\n    error: Exceptions = Exception,\n    exclude: Exceptions | None = None,\n    callback: Callable = print_exc,\n    *callback_args,\n    **callback_kwargs,\n):\n    r\"\"\"\n    Decorator to catch `error` except for `exclude`.\n    Detailed traceback will be printed to `stderr`.\n\n    `catch` is extremely useful for unfatal errors.\n    For example, `Runner` saves checkpoint regularly, however, this might break running if the space is full.\n    Decorating `save` method with `catch` will allow you to catch these errors and continue your running.\n\n    Args:\n        error: Exceptions to be caught.\n        exclude: Exceptions to be excluded.\n        callback: Callback to be called when an error occurs.\n            The first four arguments to `callback` are `exc`, `func`, `args`, `kwargs`.\n            Additional arguments should be passed with `*callback_args` and `**callback_kwargs`.\n        callback_args: Arguments to be passed to `callback`.\n        callback_kwargs: Keyword arguments to be passed to `callback`.\n\n    Examples:\n        &gt;&gt;&gt; def file_not_found(*args, **kwargs):\n        ...     return exclude\n        ...     raise FileNotFoundError\n\n        &gt;&gt;&gt; file_not_found()\n        &gt;&gt;&gt; raise ValueError\n        &gt;&gt;&gt; assert 1 == 2\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):  # pylint: disable=inconsistent-return-statements\n            try:\n                return func(*args, **kwargs)\n            except error as exc:  # pylint: disable=broad-exception-caught\n                if exclude is not None and isinstance(exc, exclude):\n                    raise exc\n                callback(exc, func, args, kwargs, *callback_args, **callback_kwargs)\n\n        return wrapper\n\n    decorator.__doc__ = catch.__doc__\n\n    return decorator\n</code></pre>"},{"location":"package/#danling.debug","title":"<code>debug(enable=True, error=Exception, exclude=None)</code>","text":"<p>Contextmanager to enter debug mode on <code>error</code> except for <code>exclude</code>.</p> <p><code>debug</code> is intended to be used to catch the error and enter debug mode. Since it is mainly for development purposed, we include an <code>enable</code> args so that it can be deactivated.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Whether to enable the contextmanager. Defaults to <code>True</code>.</p> <code>True</code> <code>error</code> <code>Exceptions</code> <p>The error to catch. Defaults to <code>Exception</code>.</p> <code>Exception</code> <code>exclude</code> <code>Optional[Exceptions]</code> <p>The error to exclude. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>danling/utils/contextmanagers.py</code> Python<pre><code>@contextmanager\ndef debug(\n    enable: bool = True,\n    error: Exceptions = Exception,\n    exclude: Optional[Exceptions] = None,\n):\n    \"\"\"\n    Contextmanager to enter debug mode on `error` except for `exclude`.\n\n    `debug` is intended to be used to catch the error and enter debug mode.\n    Since it is mainly for development purposed, we include an `enable` args so that it can be deactivated.\n\n    Args:\n        enable: Whether to enable the contextmanager.\n            Defaults to `True`.\n        error: The error to catch.\n            Defaults to `Exception`.\n        exclude: The error to exclude.\n            Defaults to `None`.\n    \"\"\"\n\n    if not enable:\n        yield\n        return\n    try:\n        yield\n    except error as exc:  # pylint: disable=broad-exception-caught\n        if exclude is not None and isinstance(exc, exclude):\n            raise exc\n        _, m, tb = sys.exc_info()\n        print(repr(m), file=sys.stderr)\n        pdb.post_mortem(tb)\n    finally:\n        pass\n</code></pre>"},{"location":"package/#danling.ensure_dir","title":"<code>ensure_dir(func)</code>","text":"<p>Decorator to ensure a directory property exists.</p> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>def ensure_dir(func):\n    r\"\"\"\n    Decorator to ensure a directory property exists.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        path = abspath(func(*args, **kwargs))\n        makedirs(path, exist_ok=True)\n        return path\n\n    return wrapper\n</code></pre>"},{"location":"package/#danling.flexible_decorator","title":"<code>flexible_decorator(maybe_decorator=None)</code>","text":"<p>Decorator to allow bracket-less when no arguments are passed.</p> <p>Examples:</p> <p>For decorator defined as follows:</p> Python Console Session<pre><code>&gt;&gt;&gt; @flexible_decorator\n... def decorator(*args, **kwargs):\n...     def wrapper(func, *args, **kwargs):\n...         pass\n...     return wrapper\n</code></pre> <p>The following two are equivalent:</p> Python Console Session<pre><code>&gt;&gt;&gt; @decorator\n... def func(*args, **kwargs):\n...     pass\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; @decorator()\n... def func(*args, **kwargs):\n...     pass\n</code></pre> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>def flexible_decorator(maybe_decorator: Optional[Callable] = None):\n    r\"\"\"\n    Decorator to allow bracket-less when no arguments are passed.\n\n    Examples:\n        For decorator defined as follows:\n\n        &gt;&gt;&gt; @flexible_decorator\n        ... def decorator(*args, **kwargs):\n        ...     def wrapper(func, *args, **kwargs):\n        ...         pass\n        ...     return wrapper\n\n        The following two are equivalent:\n\n        &gt;&gt;&gt; @decorator\n        ... def func(*args, **kwargs):\n        ...     pass\n\n        &gt;&gt;&gt; @decorator()\n        ... def func(*args, **kwargs):\n        ...     pass\n    \"\"\"\n\n    def decorator(func: Callable):\n        @wraps(decorator)\n        def wrapper(*args, **kwargs):\n            if len(args) == 1 and isfunction(args[0]):\n                return func(**kwargs)(args[0])\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    if maybe_decorator is None:\n        return decorator\n    return decorator(maybe_decorator)\n</code></pre>"},{"location":"package/#danling.is_json_serializable","title":"<code>is_json_serializable(obj)</code>","text":"<p>Check if <code>obj</code> is JSON serializable.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def is_json_serializable(obj: Any) -&gt; bool:\n    r\"\"\"\n    Check if `obj` is JSON serializable.\n    \"\"\"\n    try:\n        json.dumps(obj)\n        return True\n    except (TypeError, OverflowError):\n        return False\n</code></pre>"},{"location":"package/#danling.load","title":"<code>load(file, *args, **kwargs)</code>","text":"<p>Load any file with supported extensions.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def load(file: PathStr, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n    r\"\"\"\n    Load any file with supported extensions.\n    \"\"\"\n    if not os.path.isfile(file):\n        raise ValueError(f\"Trying to load {file!r} but it is not a file.\")\n    extension = os.path.splitext(file)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if not TORCH_AVAILABLE:\n            raise ImportError(f\"Trying to load {file!r} but torch is not installed.\")\n        return torch.load(file, *args, **kwargs)\n    if extension in NUMPY:\n        if not NUMPY_AVAILABLE:\n            raise ImportError(f\"Trying to load {file!r} but numpy is not installed.\")\n        return numpy.load(file, *args, **kwargs)\n    if extension in CSV:\n        if not PANDAS_AVAILABLE:\n            raise ImportError(f\"Trying to load {file!r} but pandas is not installed.\")\n        return pandas.read_csv(file, *args, **kwargs)\n    if extension in JSON:\n        with open(file) as fp:\n            return json.load(fp, *args, **kwargs)  # type: ignore\n    if extension in YAML:\n        with open(file) as fp:\n            return yaml.load(fp, *args, **kwargs)  # type: ignore\n    if extension in PICKLE:\n        with open(file, \"rb\") as fp:\n            return pickle.load(fp, *args, **kwargs)  # type: ignore\n    raise ValueError(f\"Tying to load {file!r} with unsupported extension={extension!r}\")\n</code></pre>"},{"location":"package/#danling.method_cache","title":"<code>method_cache(*cache_args, **lru_kwargs)</code>","text":"<p>Decorator to cache the result of an instance method.</p> <p><code>functools.lru_cache</code> uses a strong reference to the instance, which will make the instance immortal and break the garbage collection.</p> <p><code>method_cache</code> uses a weak reference to the instance and works fine.</p> <p>https://rednafi.github.io/reflections/dont-wrap-instance-methods-with-functoolslru_cache-decorator-in-python.html</p> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>def method_cache(*cache_args, **lru_kwargs):\n    r\"\"\"\n    Decorator to cache the result of an instance method.\n\n    `functools.lru_cache` uses a strong reference to the instance,\n    which will make the instance immortal and break the garbage collection.\n\n    `method_cache` uses a weak reference to the instance and works fine.\n\n    https://rednafi.github.io/reflections/dont-wrap-instance-methods-with-functoolslru_cache-decorator-in-python.html\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            self_ref = ref(self)\n\n            @wraps(func)\n            @lru_cache(*cache_args, **lru_kwargs)\n            def cached_method(*args, **kwargs):\n                return func(self_ref(), *args, **kwargs)\n\n            setattr(self, func.__name__, cached_method)\n            return cached_method(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"package/#danling.save","title":"<code>save(obj, file, *args, **kwargs)</code>","text":"<p>Save any file with supported extensions.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def save(obj: Any, file: PathStr, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; File:\n    r\"\"\"\n    Save any file with supported extensions.\n    \"\"\"\n    extension = os.path.splitext(file)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if not TORCH_AVAILABLE:\n            raise ImportError(f\"Trying to save {obj} to {file!r} but torch is not installed.\")\n        torch.save(obj, file, *args, **kwargs)\n    elif extension in NUMPY:\n        if not NUMPY_AVAILABLE:\n            raise ImportError(f\"Trying to save {obj} to {file!r} but numpy is not installed.\")\n        numpy.save(file, obj, *args, **kwargs)\n    elif extension in CSV:\n        if isinstance(obj, pandas.DataFrame):\n            obj.to_csv(file, *args, **kwargs)\n        else:\n            raise NotImplementedError(f\"Trying to save {obj} to {file!r} but is not supported\")\n    elif extension in JSON:\n        if isinstance(obj, FlatDict):\n            obj.json(file)\n        else:\n            with open(file, \"w\") as fp:\n                json.dump(obj, fp, *args, **kwargs)  # type: ignore\n    elif extension in YAML:\n        if isinstance(obj, FlatDict):\n            obj.yaml(file)\n        else:\n            with open(file, \"w\") as fp:\n                yaml.dump(obj, fp, *args, **kwargs)  # type: ignore\n    elif extension in PICKLE:\n        with open(file, \"wb\") as fp:\n            pickle.dump(obj, fp, *args, **kwargs)  # type: ignore\n    else:\n        raise ValueError(f\"Tying to save {obj} to {file!r} with unsupported extension={extension!r}\")\n    return file\n</code></pre>"},{"location":"blog/","title":"DanLing","text":""},{"location":"metrics/average_meter/","title":"AverageMeter","text":"<p>Computes and stores the average and current value.</p> <p>Attributes:</p> Name Type Description <code>val</code> <code>float</code> <p>Current value.</p> <code>avg</code> <code>float</code> <p>Average value.</p> <code>sum</code> <code>float</code> <p>Sum of values.</p> <code>count</code> <code>int</code> <p>Number of values.</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>class AverageMeter:\n    r\"\"\"\n    Computes and stores the average and current value.\n\n    Attributes:\n        val: Current value.\n        avg: Average value.\n        sum: Sum of values.\n        count: Number of values.\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    val: float = 0\n    avg: float = 0\n    sum: float = 0\n    count: int = 0\n\n    def __init__(self) -&gt; None:\n        self.reset()\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets the meter.\n\n        Examples:\n            &gt;&gt;&gt; meter = AverageMeter()\n            &gt;&gt;&gt; meter.update(0.7)\n            &gt;&gt;&gt; meter.val\n            0.7\n            &gt;&gt;&gt; meter.avg\n            0.7\n            &gt;&gt;&gt; meter.reset()\n            &gt;&gt;&gt; meter.val\n            0\n            &gt;&gt;&gt; meter.avg\n            0\n        \"\"\"\n\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n: int = 1) -&gt; None:\n        r\"\"\"\n        Updates the average and current value in the meter.\n\n        Args:\n            val: Value to be added to the average.\n            n: Number of values to be added.\n\n        Examples:\n            &gt;&gt;&gt; meter = AverageMeter()\n            &gt;&gt;&gt; meter.update(0.7)\n            &gt;&gt;&gt; meter.val\n            0.7\n            &gt;&gt;&gt; meter.avg\n            0.7\n            &gt;&gt;&gt; meter.update(0.9)\n            &gt;&gt;&gt; meter.val\n            0.9\n            &gt;&gt;&gt; meter.avg\n            0.8\n            &gt;&gt;&gt; meter.sum\n            1.6\n            &gt;&gt;&gt; meter.count\n            2\n        \"\"\"\n\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __format__(self, format_spec) -&gt; str:\n        return f\"{self.val.__format__(format_spec)} ({self.avg.__format__(format_spec)})\"\n</code></pre> <p>             Bases: <code>DefaultDict</code></p> <p>A <code>DefaultDict</code> for <code>AverageMeter</code>.</p> <p>Examples: Python<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.loss.reset()\n&gt;&gt;&gt; meters.loss.update(0.7)\n&gt;&gt;&gt; meters.loss.val\n0.7\n&gt;&gt;&gt; meters.loss.avg\n0.7\n&gt;&gt;&gt; meters.update(0.9)\n&gt;&gt;&gt; meters.loss.val\n0.9\n&gt;&gt;&gt; meters.loss.avg\n0.8\n&gt;&gt;&gt; meters.loss.sum\n1.6\n&gt;&gt;&gt; meters.loss.count\n2\n&gt;&gt;&gt; meters.reset()\n&gt;&gt;&gt; meters.loss.val\n0\n&gt;&gt;&gt; meters.loss.avg\n0\n</code></pre></p> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>class AverageMeters(DefaultDict):\n    r\"\"\"\n    A `DefaultDict` for `AverageMeter`.\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; meters = AverageMeters()\n    &gt;&gt;&gt; meters.loss.reset()\n    &gt;&gt;&gt; meters.loss.update(0.7)\n    &gt;&gt;&gt; meters.loss.val\n    0.7\n    &gt;&gt;&gt; meters.loss.avg\n    0.7\n    &gt;&gt;&gt; meters.update(0.9)\n    &gt;&gt;&gt; meters.loss.val\n    0.9\n    &gt;&gt;&gt; meters.loss.avg\n    0.8\n    &gt;&gt;&gt; meters.loss.sum\n    1.6\n    &gt;&gt;&gt; meters.loss.count\n    2\n    &gt;&gt;&gt; meters.reset()\n    &gt;&gt;&gt; meters.loss.val\n    0\n    &gt;&gt;&gt; meters.loss.avg\n    0\n\n    ```\n    \"\"\"\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        kwargs.setdefault(\"default_factory\", AverageMeter)\n        super().__init__(*args, **kwargs)\n\n    @property\n    def val(self) -&gt; NestedDict[str, float]:\n        return NestedDict({key: meter.val for key, meter in self.items()})\n\n    @property\n    def avg(self) -&gt; NestedDict[str, float]:\n        return NestedDict({key: meter.avg for key, meter in self.items()})\n\n    @property\n    def sum(self) -&gt; NestedDict[str, float]:\n        return NestedDict({key: meter.sum for key, meter in self.items()})\n\n    @property\n    def count(self) -&gt; NestedDict[str, int]:\n        return NestedDict({key: meter.count for key, meter in self.items()})\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets all meters.\n\n        Examples:\n            &gt;&gt;&gt; meters = AverageMeters()\n            &gt;&gt;&gt; meters.loss.update(0.7)\n            &gt;&gt;&gt; meters.loss.val\n            0.7\n            &gt;&gt;&gt; meters.loss.avg\n            0.7\n            &gt;&gt;&gt; meters.reset()\n            &gt;&gt;&gt; meters.loss.val\n            0\n            &gt;&gt;&gt; meters.loss.avg\n            0\n        \"\"\"\n\n        for meter in self.values():\n            meter.reset()\n\n    def update(self, val, n: int = 1) -&gt; None:  # pylint: disable=W0237\n        r\"\"\"\n        Updates the average and current value in all meters.\n\n        Args:\n            val: Value to be added to the average.\n            n: Number of values to be added.\n\n        Note:\n            This function is **NOT** recommended to use, as it alters all meters in the bank.\n\n        Examples:\n            &gt;&gt;&gt; meters = AverageMeters()\n            &gt;&gt;&gt; meters.loss.update(0.7)\n            &gt;&gt;&gt; meters.loss.val\n            0.7\n            &gt;&gt;&gt; meters.loss.avg\n            0.7\n            &gt;&gt;&gt; meters.update(0.9)\n            &gt;&gt;&gt; meters.loss.val\n            0.9\n            &gt;&gt;&gt; meters.loss.avg\n            0.8\n            &gt;&gt;&gt; meters.loss.sum\n            1.6\n            &gt;&gt;&gt; meters.loss.count\n            2\n        \"\"\"\n\n        for meter in self.values():\n            meter.update(val, n)\n\n    def __format__(self, format_spec) -&gt; str:\n        return \"\\n\".join(f\"{key}: {meter.__format__(format_spec)}\" for key, meter in self.items())\n</code></pre>"},{"location":"metrics/average_meter/#danling.metrics.AverageMeter.reset","title":"<code>reset()</code>","text":"<p>Resets the meter.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets the meter.\n\n    Examples:\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        0\n    \"\"\"\n\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n</code></pre>"},{"location":"metrics/average_meter/#danling.metrics.AverageMeter.update","title":"<code>update(val, n=1)</code>","text":"<p>Updates the average and current value in the meter.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <p>Value to be added to the average.</p> required <code>n</code> <code>int</code> <p>Number of values to be added.</p> <code>1</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>def update(self, val, n: int = 1) -&gt; None:\n    r\"\"\"\n    Updates the average and current value in the meter.\n\n    Args:\n        val: Value to be added to the average.\n        n: Number of values to be added.\n\n    Examples:\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n    \"\"\"\n\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n</code></pre>"},{"location":"metrics/average_meter/#danling.metrics.AverageMeters.reset","title":"<code>reset()</code>","text":"<p>Resets all meters.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.loss.update(0.7)\n&gt;&gt;&gt; meters.loss.val\n0.7\n&gt;&gt;&gt; meters.loss.avg\n0.7\n&gt;&gt;&gt; meters.reset()\n&gt;&gt;&gt; meters.loss.val\n0\n&gt;&gt;&gt; meters.loss.avg\n0\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets all meters.\n\n    Examples:\n        &gt;&gt;&gt; meters = AverageMeters()\n        &gt;&gt;&gt; meters.loss.update(0.7)\n        &gt;&gt;&gt; meters.loss.val\n        0.7\n        &gt;&gt;&gt; meters.loss.avg\n        0.7\n        &gt;&gt;&gt; meters.reset()\n        &gt;&gt;&gt; meters.loss.val\n        0\n        &gt;&gt;&gt; meters.loss.avg\n        0\n    \"\"\"\n\n    for meter in self.values():\n        meter.reset()\n</code></pre>"},{"location":"metrics/average_meter/#danling.metrics.AverageMeters.update","title":"<code>update(val, n=1)</code>","text":"<p>Updates the average and current value in all meters.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <p>Value to be added to the average.</p> required <code>n</code> <code>int</code> <p>Number of values to be added.</p> <code>1</code> Note <p>This function is NOT recommended to use, as it alters all meters in the bank.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.loss.update(0.7)\n&gt;&gt;&gt; meters.loss.val\n0.7\n&gt;&gt;&gt; meters.loss.avg\n0.7\n&gt;&gt;&gt; meters.update(0.9)\n&gt;&gt;&gt; meters.loss.val\n0.9\n&gt;&gt;&gt; meters.loss.avg\n0.8\n&gt;&gt;&gt; meters.loss.sum\n1.6\n&gt;&gt;&gt; meters.loss.count\n2\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>def update(self, val, n: int = 1) -&gt; None:  # pylint: disable=W0237\n    r\"\"\"\n    Updates the average and current value in all meters.\n\n    Args:\n        val: Value to be added to the average.\n        n: Number of values to be added.\n\n    Note:\n        This function is **NOT** recommended to use, as it alters all meters in the bank.\n\n    Examples:\n        &gt;&gt;&gt; meters = AverageMeters()\n        &gt;&gt;&gt; meters.loss.update(0.7)\n        &gt;&gt;&gt; meters.loss.val\n        0.7\n        &gt;&gt;&gt; meters.loss.avg\n        0.7\n        &gt;&gt;&gt; meters.update(0.9)\n        &gt;&gt;&gt; meters.loss.val\n        0.9\n        &gt;&gt;&gt; meters.loss.avg\n        0.8\n        &gt;&gt;&gt; meters.loss.sum\n        1.6\n        &gt;&gt;&gt; meters.loss.count\n        2\n    \"\"\"\n\n    for meter in self.values():\n        meter.update(val, n)\n</code></pre>"},{"location":"metrics/average_meters/","title":"AverageMeters","text":""},{"location":"metrics/average_meters/#danling.metrics.average_meters.AverageMeter","title":"<code>AverageMeter</code>","text":"<p>Computes and stores the average and current value.</p> <p>Attributes:</p> Name Type Description <code>val</code> <code>float</code> <p>Current value.</p> <code>avg</code> <code>float</code> <p>Average value.</p> <code>sum</code> <code>float</code> <p>Sum of values.</p> <code>count</code> <code>int</code> <p>Number of values.</p> Python<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>class AverageMeter:\n    r\"\"\"\n    Computes and stores the average and current value.\n\n    Attributes:\n        val: Current value.\n        avg: Average value.\n        sum: Sum of values.\n        count: Number of values.\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; meter = AverageMeter()\n    &gt;&gt;&gt; meter.update(0.7)\n    &gt;&gt;&gt; meter.val\n    0.7\n    &gt;&gt;&gt; meter.avg\n    0.7\n    &gt;&gt;&gt; meter.update(0.9)\n    &gt;&gt;&gt; meter.val\n    0.9\n    &gt;&gt;&gt; meter.avg\n    0.8\n    &gt;&gt;&gt; meter.sum\n    1.6\n    &gt;&gt;&gt; meter.count\n    2\n    &gt;&gt;&gt; meter.reset()\n    &gt;&gt;&gt; meter.val\n    0\n    &gt;&gt;&gt; meter.avg\n    0\n\n    ```\n    \"\"\"\n\n    val: float = 0\n    avg: float = 0\n    sum: float = 0\n    count: int = 0\n\n    def __init__(self) -&gt; None:\n        self.reset()\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets the meter.\n\n        Examples:\n            &gt;&gt;&gt; meter = AverageMeter()\n            &gt;&gt;&gt; meter.update(0.7)\n            &gt;&gt;&gt; meter.val\n            0.7\n            &gt;&gt;&gt; meter.avg\n            0.7\n            &gt;&gt;&gt; meter.reset()\n            &gt;&gt;&gt; meter.val\n            0\n            &gt;&gt;&gt; meter.avg\n            0\n        \"\"\"\n\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n: int = 1) -&gt; None:\n        r\"\"\"\n        Updates the average and current value in the meter.\n\n        Args:\n            val: Value to be added to the average.\n            n: Number of values to be added.\n\n        Examples:\n            &gt;&gt;&gt; meter = AverageMeter()\n            &gt;&gt;&gt; meter.update(0.7)\n            &gt;&gt;&gt; meter.val\n            0.7\n            &gt;&gt;&gt; meter.avg\n            0.7\n            &gt;&gt;&gt; meter.update(0.9)\n            &gt;&gt;&gt; meter.val\n            0.9\n            &gt;&gt;&gt; meter.avg\n            0.8\n            &gt;&gt;&gt; meter.sum\n            1.6\n            &gt;&gt;&gt; meter.count\n            2\n        \"\"\"\n\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __format__(self, format_spec) -&gt; str:\n        return f\"{self.val.__format__(format_spec)} ({self.avg.__format__(format_spec)})\"\n</code></pre>"},{"location":"metrics/average_meters/#danling.metrics.average_meters.AverageMeter.reset","title":"<code>reset()</code>","text":"<p>Resets the meter.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\n0\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets the meter.\n\n    Examples:\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        0\n    \"\"\"\n\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n</code></pre>"},{"location":"metrics/average_meters/#danling.metrics.average_meters.AverageMeter.update","title":"<code>update(val, n=1)</code>","text":"<p>Updates the average and current value in the meter.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <p>Value to be added to the average.</p> required <code>n</code> <code>int</code> <p>Number of values to be added.</p> <code>1</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>def update(self, val, n: int = 1) -&gt; None:\n    r\"\"\"\n    Updates the average and current value in the meter.\n\n    Args:\n        val: Value to be added to the average.\n        n: Number of values to be added.\n\n    Examples:\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n    \"\"\"\n\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n</code></pre>"},{"location":"metrics/average_meters/#danling.metrics.average_meters.AverageMeters","title":"<code>AverageMeters</code>","text":"<p>             Bases: <code>DefaultDict</code></p> <p>A <code>DefaultDict</code> for <code>AverageMeter</code>.</p> <p>Examples: Python<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.loss.reset()\n&gt;&gt;&gt; meters.loss.update(0.7)\n&gt;&gt;&gt; meters.loss.val\n0.7\n&gt;&gt;&gt; meters.loss.avg\n0.7\n&gt;&gt;&gt; meters.update(0.9)\n&gt;&gt;&gt; meters.loss.val\n0.9\n&gt;&gt;&gt; meters.loss.avg\n0.8\n&gt;&gt;&gt; meters.loss.sum\n1.6\n&gt;&gt;&gt; meters.loss.count\n2\n&gt;&gt;&gt; meters.reset()\n&gt;&gt;&gt; meters.loss.val\n0\n&gt;&gt;&gt; meters.loss.avg\n0\n</code></pre></p> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>class AverageMeters(DefaultDict):\n    r\"\"\"\n    A `DefaultDict` for `AverageMeter`.\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; meters = AverageMeters()\n    &gt;&gt;&gt; meters.loss.reset()\n    &gt;&gt;&gt; meters.loss.update(0.7)\n    &gt;&gt;&gt; meters.loss.val\n    0.7\n    &gt;&gt;&gt; meters.loss.avg\n    0.7\n    &gt;&gt;&gt; meters.update(0.9)\n    &gt;&gt;&gt; meters.loss.val\n    0.9\n    &gt;&gt;&gt; meters.loss.avg\n    0.8\n    &gt;&gt;&gt; meters.loss.sum\n    1.6\n    &gt;&gt;&gt; meters.loss.count\n    2\n    &gt;&gt;&gt; meters.reset()\n    &gt;&gt;&gt; meters.loss.val\n    0\n    &gt;&gt;&gt; meters.loss.avg\n    0\n\n    ```\n    \"\"\"\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        kwargs.setdefault(\"default_factory\", AverageMeter)\n        super().__init__(*args, **kwargs)\n\n    @property\n    def val(self) -&gt; NestedDict[str, float]:\n        return NestedDict({key: meter.val for key, meter in self.items()})\n\n    @property\n    def avg(self) -&gt; NestedDict[str, float]:\n        return NestedDict({key: meter.avg for key, meter in self.items()})\n\n    @property\n    def sum(self) -&gt; NestedDict[str, float]:\n        return NestedDict({key: meter.sum for key, meter in self.items()})\n\n    @property\n    def count(self) -&gt; NestedDict[str, int]:\n        return NestedDict({key: meter.count for key, meter in self.items()})\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets all meters.\n\n        Examples:\n            &gt;&gt;&gt; meters = AverageMeters()\n            &gt;&gt;&gt; meters.loss.update(0.7)\n            &gt;&gt;&gt; meters.loss.val\n            0.7\n            &gt;&gt;&gt; meters.loss.avg\n            0.7\n            &gt;&gt;&gt; meters.reset()\n            &gt;&gt;&gt; meters.loss.val\n            0\n            &gt;&gt;&gt; meters.loss.avg\n            0\n        \"\"\"\n\n        for meter in self.values():\n            meter.reset()\n\n    def update(self, val, n: int = 1) -&gt; None:  # pylint: disable=W0237\n        r\"\"\"\n        Updates the average and current value in all meters.\n\n        Args:\n            val: Value to be added to the average.\n            n: Number of values to be added.\n\n        Note:\n            This function is **NOT** recommended to use, as it alters all meters in the bank.\n\n        Examples:\n            &gt;&gt;&gt; meters = AverageMeters()\n            &gt;&gt;&gt; meters.loss.update(0.7)\n            &gt;&gt;&gt; meters.loss.val\n            0.7\n            &gt;&gt;&gt; meters.loss.avg\n            0.7\n            &gt;&gt;&gt; meters.update(0.9)\n            &gt;&gt;&gt; meters.loss.val\n            0.9\n            &gt;&gt;&gt; meters.loss.avg\n            0.8\n            &gt;&gt;&gt; meters.loss.sum\n            1.6\n            &gt;&gt;&gt; meters.loss.count\n            2\n        \"\"\"\n\n        for meter in self.values():\n            meter.update(val, n)\n\n    def __format__(self, format_spec) -&gt; str:\n        return \"\\n\".join(f\"{key}: {meter.__format__(format_spec)}\" for key, meter in self.items())\n</code></pre>"},{"location":"metrics/average_meters/#danling.metrics.average_meters.AverageMeters.reset","title":"<code>reset()</code>","text":"<p>Resets all meters.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.loss.update(0.7)\n&gt;&gt;&gt; meters.loss.val\n0.7\n&gt;&gt;&gt; meters.loss.avg\n0.7\n&gt;&gt;&gt; meters.reset()\n&gt;&gt;&gt; meters.loss.val\n0\n&gt;&gt;&gt; meters.loss.avg\n0\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets all meters.\n\n    Examples:\n        &gt;&gt;&gt; meters = AverageMeters()\n        &gt;&gt;&gt; meters.loss.update(0.7)\n        &gt;&gt;&gt; meters.loss.val\n        0.7\n        &gt;&gt;&gt; meters.loss.avg\n        0.7\n        &gt;&gt;&gt; meters.reset()\n        &gt;&gt;&gt; meters.loss.val\n        0\n        &gt;&gt;&gt; meters.loss.avg\n        0\n    \"\"\"\n\n    for meter in self.values():\n        meter.reset()\n</code></pre>"},{"location":"metrics/average_meters/#danling.metrics.average_meters.AverageMeters.update","title":"<code>update(val, n=1)</code>","text":"<p>Updates the average and current value in all meters.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <p>Value to be added to the average.</p> required <code>n</code> <code>int</code> <p>Number of values to be added.</p> <code>1</code> Note <p>This function is NOT recommended to use, as it alters all meters in the bank.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.loss.update(0.7)\n&gt;&gt;&gt; meters.loss.val\n0.7\n&gt;&gt;&gt; meters.loss.avg\n0.7\n&gt;&gt;&gt; meters.update(0.9)\n&gt;&gt;&gt; meters.loss.val\n0.9\n&gt;&gt;&gt; meters.loss.avg\n0.8\n&gt;&gt;&gt; meters.loss.sum\n1.6\n&gt;&gt;&gt; meters.loss.count\n2\n</code></pre> Source code in <code>danling/metrics/average_meters.py</code> Python<pre><code>def update(self, val, n: int = 1) -&gt; None:  # pylint: disable=W0237\n    r\"\"\"\n    Updates the average and current value in all meters.\n\n    Args:\n        val: Value to be added to the average.\n        n: Number of values to be added.\n\n    Note:\n        This function is **NOT** recommended to use, as it alters all meters in the bank.\n\n    Examples:\n        &gt;&gt;&gt; meters = AverageMeters()\n        &gt;&gt;&gt; meters.loss.update(0.7)\n        &gt;&gt;&gt; meters.loss.val\n        0.7\n        &gt;&gt;&gt; meters.loss.avg\n        0.7\n        &gt;&gt;&gt; meters.update(0.9)\n        &gt;&gt;&gt; meters.loss.val\n        0.9\n        &gt;&gt;&gt; meters.loss.avg\n        0.8\n        &gt;&gt;&gt; meters.loss.sum\n        1.6\n        &gt;&gt;&gt; meters.loss.count\n        2\n    \"\"\"\n\n    for meter in self.values():\n        meter.update(val, n)\n</code></pre>"},{"location":"metrics/metrics/","title":"Metrics","text":""},{"location":"metrics/metrics/#danling.metrics.metrics.Metrics","title":"<code>Metrics</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Metric class wraps around multiple metrics that share the same states.</p> <p>Typically, there are many metrics that we want to compute for a single task. For example, we usually needs to compute <code>accuracy</code>, <code>auroc</code>, <code>auprc</code> for a classification task. Computing them one by one is inefficient, especially when evaluating in a distributed environment.</p> <p>To solve this problem, Metrics maintains a shared state for multiple metric functions.</p> <p>Attributes:</p> Name Type Description <code>metrics</code> <code>FlatDict[str, Callable]</code> <p>A dictionary of metrics to be computed.</p> <code>input</code> <p>The input tensor of latest batch.</p> <code>target</code> <p>The target tensor of latest batch.</p> <code>inputs</code> <p>All input tensors.</p> <code>targets</code> <p>All target tensors.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>A single mapping of metrics.</p> <code>()</code> <code>**metrics</code> <code>FlatDict[str, Callable]</code> <p>Metrics.</p> <code>{}</code> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>class Metrics(Metric):\n    r\"\"\"\n    Metric class wraps around multiple metrics that share the same states.\n\n    Typically, there are many metrics that we want to compute for a single task.\n    For example, we usually needs to compute `accuracy`, `auroc`, `auprc` for a classification task.\n    Computing them one by one is inefficient, especially when evaluating in a distributed environment.\n\n    To solve this problem, Metrics maintains a shared state for multiple metric functions.\n\n    Attributes:\n        metrics: A dictionary of metrics to be computed.\n        input: The input tensor of latest batch.\n        target: The target tensor of latest batch.\n        inputs: All input tensors.\n        targets: All target tensors.\n\n    Args:\n        *args: A single mapping of metrics.\n        **metrics: Metrics.\n    \"\"\"\n\n    metrics: FlatDict[str, Callable]\n    _input: Tensor\n    _target: Tensor\n    _inputs: list[Tensor]\n    _targets: list[Tensor]\n    _input_buffer: list[Tensor]\n    _target_buffer: list[Tensor]\n    score_name: str\n    best_fn: Callable\n    merge_dict: bool = True\n\n    def __init__(self, *args, merge_dict: bool | None = None, **metrics: FlatDict[str, Callable]):\n        super().__init__()\n        self._add_state(\"_input\", torch.empty(0))\n        self._add_state(\"_target\", torch.empty(0))\n        self._add_state(\"_inputs\", [])\n        self._add_state(\"_targets\", [])\n        self._add_state(\"_input_buffer\", [])\n        self._add_state(\"_target_buffer\", [])\n        self.metrics = FlatDict(*args, **metrics)\n        if merge_dict is not None:\n            self.merge_dict = merge_dict\n\n    @torch.inference_mode()\n    def update(self, input: Any, target: Any) -&gt; None:\n        if isinstance(input, NestedTensor):\n            self._input = input\n            self._input_buffer.extend(input.to(self.device).storage)\n        else:\n            if not isinstance(input, torch.Tensor):\n                input = torch.tensor(input)\n            self._input = input\n            self._input_buffer.append(input.to(self.device))\n        if isinstance(target, NestedTensor):\n            self._target = target\n            self._target_buffer.extend(target.to(self.device).storage)\n        else:\n            if not isinstance(target, torch.Tensor):\n                target = torch.tensor(target)\n            self._target = target\n            self._target_buffer.append(target.to(self.device))\n\n    def compute(self) -&gt; FlatDict[str, float]:\n        return self.comp\n\n    def value(self) -&gt; FlatDict[str, float]:\n        return self.val\n\n    def average(self) -&gt; FlatDict[str, float]:\n        return self.avg\n\n    @property\n    def comp(self) -&gt; FlatDict[str, float]:\n        return self._compute(self._input, self._target)\n\n    @property\n    def val(self) -&gt; FlatDict[str, float]:\n        return self._compute(self.input, self.target)\n\n    @property\n    def avg(self) -&gt; FlatDict[str, float]:\n        return self._compute(self.inputs, self.targets)\n\n    @torch.inference_mode()\n    def _compute(self, input: Tensor, target: Tensor) -&gt; flist | float:\n        if input.numel() == 0 == target.numel():\n            return FlatDict({name: nan for name in self.metrics.keys()})\n        ret = FlatDict()\n        for name, metric in self.metrics.items():\n            score = metric(input, target)\n            if isinstance(score, Tensor):\n                ret[name] = score.item() if score.numel() == 1 else flist(score.tolist())\n            elif isinstance(score, Mapping):\n                if self.merge_dict:\n                    ret.merge(score)\n                else:\n                    for n, s in score:\n                        ret[f\"{name}.{n}\"] = s\n            else:\n                ret[name] = score\n        return ret\n\n    @torch.inference_mode()\n    def merge_state(self, metrics: Iterable):\n        raise NotImplementedError()\n\n    @property\n    @torch.inference_mode()\n    def input(self):\n        if world_size() == 1:\n            return self._input\n        if isinstance(self._input, Tensor):\n            synced_tensor = [torch.zeros_like(self._input) for _ in range(dist.get_world_size())]\n            dist.all_gather(synced_tensor, self._input)\n            return torch.cat(synced_tensor, 0)\n        if isinstance(self._input, NestedTensor):\n            synced_tensors = [None for _ in range(dist.get_world_size())]\n            dist.all_gather_object(synced_tensors, self._input.storage)\n            return NestedTensor([i for j in synced_tensors for i in j])\n        raise ValueError(f\"Expected _input to be a Tensor or a NestedTensor, but got {type(self._input)}\")\n\n    @property\n    @torch.inference_mode()\n    def target(self):\n        if world_size() == 1:\n            return self._target\n        if isinstance(self._target, Tensor):\n            synced_tensor = [torch.zeros_like(self._target) for _ in range(dist.get_world_size())]\n            dist.all_gather(synced_tensor, self._target)\n            return torch.cat(synced_tensor, 0)\n        if isinstance(self._target, NestedTensor):\n            synced_tensors = [None for _ in range(dist.get_world_size())]\n            dist.all_gather_object(synced_tensors, self._target.storage)\n            return NestedTensor([i for j in synced_tensors for i in j])\n        raise ValueError(f\"Expected _target to be a Tensor or a NestedTensor, but got {type(self._target)}\")\n\n    @property\n    @torch.inference_mode()\n    def inputs(self):\n        if not self._inputs and not self._input_buffer:\n            return torch.empty(0)\n        if self._input_buffer:\n            if world_size() &gt; 1:\n                synced_tensors = [None for _ in range(dist.get_world_size())]\n                dist.all_gather_object(synced_tensors, self._input_buffer)\n                self._inputs.extend([i for j in synced_tensors for i in j])\n            else:\n                self._inputs.extend(self._input_buffer)\n            self._input_buffer = []\n        if isinstance(self._input, NestedTensor):\n            return NestedTensor(self._inputs)\n        return torch.cat(self._inputs, 0)\n\n    @property\n    @torch.inference_mode()\n    def targets(self):\n        if not self._targets and not self._target_buffer:\n            return torch.empty(0)\n        if self._target_buffer:\n            if world_size() &gt; 1:\n                synced_tensors = [None for _ in range(dist.get_world_size())]\n                dist.all_gather_object(synced_tensors, self._target_buffer)\n                self._targets.extend([i for j in synced_tensors for i in j])\n            else:\n                self._targets.extend(self._target_buffer)\n            self._target_buffer = []\n        if isinstance(self._target, NestedTensor):\n            return NestedTensor(self._targets)\n        return torch.cat(self._targets, 0)\n\n    def __repr__(self):\n        keys = tuple(i for i in self.metrics.keys())\n        return f\"{self.__class__.__name__}{keys}\"\n\n    def __format__(self, format_spec):\n        val, avg = self.compute(), self.average()\n        return \"\\n\".join(\n            [f\"{key}: {val[key].__format__(format_spec)} ({avg[key].__format__(format_spec)})\" for key in self.metrics]\n        )\n</code></pre>"},{"location":"metrics/metrics/#danling.metrics.metrics.ScoreMetrics","title":"<code>ScoreMetrics</code>","text":"<p>             Bases: <code>Metrics</code></p> <p><code>ScoreMetrics</code> is a subclass of Metrics that supports scoring.</p> <p>Score is a single value that best represents the performance of the model. It is the core metrics that we use to compare different models. For example, in classification, we usually use auroc as the score.</p> <p><code>ScoreMetrics</code> requires two additional arguments: <code>score_name</code> and <code>best_fn</code>. <code>score_name</code> is the name of the metric that we use to compute the score. <code>best_fn</code> is a function that takes a list of values and returns the best value. <code>best_fn</code> is only not used by <code>ScoreMetrics</code>, it is meant to be accessed by other classes.</p> <p>Attributes:</p> Name Type Description <code>score_name</code> <code>str</code> <p>The name of the metric that we use to compute the score.</p> <code>best_fn</code> <code>Callable</code> <p>A function that takes a list of values and returns the best value.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>A single mapping of metrics.</p> <code>()</code> <code>score_name</code> <code>str | None</code> <p>The name of the metric that we use to compute the score. Defaults to the first metric.</p> <code>None</code> <code>best_fn</code> <code>Callable | None</code> <p>A function that takes a list of values and returns the best value. Defaults to <code>max</code>.</p> <code>max</code> <code>**metrics</code> <code>FlatDict[str, Callable]</code> <p>Metrics.</p> <code>{}</code> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>class ScoreMetrics(Metrics):  # pylint: disable=abstract-method\n    r\"\"\"\n    `ScoreMetrics` is a subclass of Metrics that supports scoring.\n\n    Score is a single value that best represents the performance of the model.\n    It is the core metrics that we use to compare different models.\n    For example, in classification, we usually use auroc as the score.\n\n    `ScoreMetrics` requires two additional arguments: `score_name` and `best_fn`.\n    `score_name` is the name of the metric that we use to compute the score.\n    `best_fn` is a function that takes a list of values and returns the best value.\n    `best_fn` is only not used by `ScoreMetrics`, it is meant to be accessed by other classes.\n\n    Attributes:\n        score_name: The name of the metric that we use to compute the score.\n        best_fn: A function that takes a list of values and returns the best value.\n\n    Args:\n        *args: A single mapping of metrics.\n        score_name: The name of the metric that we use to compute the score. Defaults to the first metric.\n        best_fn: A function that takes a list of values and returns the best value. Defaults to `max`.\n        **metrics: Metrics.\n    \"\"\"\n\n    score_name: str\n    best_fn: Callable\n\n    def __init__(\n        self, *args, score_name: str | None = None, best_fn: Callable | None = max, **metrics: FlatDict[str, Callable]\n    ):\n        super().__init__(*args, **metrics)\n        self.score_name = score_name or next(iter(self.metrics.keys()))\n        self.metric = self.metrics[self.score_name]\n        self.best_fn = best_fn or max\n\n    def score(self, scope: str) -&gt; float | flist:\n        if scope == \"batch\":\n            return self.batch_score()\n        if scope == \"average\":\n            return self.average_score()\n        raise ValueError(f\"Unknown scope: {scope}\")\n\n    def batch_score(self) -&gt; float | flist:\n        return self.calculate(self.metric, self.input, self.target)\n\n    def average_score(self) -&gt; float | flist:\n        return self.calculate(self.metric, self.inputs, self.targets)\n</code></pre>"},{"location":"metrics/metrics/#danling.metrics.metrics.world_size","title":"<code>world_size()</code>","text":"<p>Return the number of processes in the current process group.</p> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>def world_size() -&gt; int:\n    r\"\"\"Return the number of processes in the current process group.\"\"\"\n    if dist.is_available() and dist.is_initialized():\n        return dist.get_world_size()\n    return 1\n</code></pre>"},{"location":"optim/lr_scheduler/","title":"LRScheduler","text":"<p>             Bases: <code>_LRScheduler</code></p> <p>General learning rate scheduler.</p> <p>PyTorch LRScheduler is hard to extend. This class is a wrapper of PyTorch LRScheduler, which provides a more general interface. You only needs to add a new method which calculates a learning rate ratio (range from 0 to 1) with total progress (range from 0 to 1), and everything else will be done automatically.</p> <p>Moreover, this class has warmup and cooldown built-in. By default, the first 5% and last 20% of training steps will be warmup and cooldown respectively. You can alternate by passing <code>warmup_steps</code> and <code>cooldown_steps</code>, or disable them by setting them to 0.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>Wrapped optimizer.</p> required <code>total_steps</code> <code>int</code> <p>Total number of steps.</p> required <code>final_lr_ratio</code> <code>float</code> <p>Final learning rate ratio to initial learning rate. Defaults to 100.</p> <code>100</code> <code>final_lr</code> <code>Optional[float]</code> <p>Final learning rate. Deprecated, use <code>final_lr_ratio</code> instead. Defaults to None.</p> <code>None</code> <code>min_lr</code> <code>float</code> <p>Minimal learning rate. Defaults to 1e-9.</p> <code>1e-09</code> <code>strategy</code> <code>str</code> <p>Scaling strategy. Defaults to \u201ccosine\u201d.</p> <code>'cosine'</code> <code>warmup_steps</code> <code>Optional[int]</code> <p>Number of warmup steps. Defaults to <code>steps // 20</code>.</p> <code>None</code> <code>cooldown_steps</code> <code>Optional[int]</code> <p>Number of cooldown steps. Defaults to <code>steps // 5</code>.</p> <code>None</code> <code>last_epoch</code> <code>int</code> <p>The index of last epoch. Defaults to -1.</p> <code>-1</code> <code>method</code> <code>str</code> <p>Method to calculate learning rate given ratio, should be one of \u201cpercentile\u201d or \u201clinear\u201d. Defaults to \u201cpercentile\u201d.</p> <code>'percentile'</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from danling.optim import LRScheduler\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import optim\n&gt;&gt;&gt; optimizer = optim.SGD([{'params': torch.tensor([0])}], lr=1, momentum=0.9)\n&gt;&gt;&gt; scheduler = LRScheduler(optimizer, total_steps=5, final_lr_ratio=1e-5, strategy='linear')\n&gt;&gt;&gt; lrs = []\n&gt;&gt;&gt; for epoch in range(5):\n...     lrs.append(scheduler.get_lr()[0])\n...     scheduler.step()\n&gt;&gt;&gt; [round(lr, 10) for lr in lrs]\n[0.1, 0.01, 0.001, 0.0001, 1e-09]\n&gt;&gt;&gt; scheduler = LRScheduler(optimizer, total_steps=5, final_lr_ratio=1e-5, strategy='cosine')\n&gt;&gt;&gt; lrs = []\n&gt;&gt;&gt; for epoch in range(5):\n...     lrs.append(scheduler.get_lr()[0])\n...     scheduler.step()\n&gt;&gt;&gt; [round(lr, 10) for lr in lrs]\n[0.3330753446, 0.0187302031, 0.000533897, 3.00232e-05, 1e-09]\n</code></pre> Source code in <code>danling/optim/lr_scheduler/lr_scheduler.py</code> Python<pre><code>class LRScheduler(lr_scheduler._LRScheduler):  # pylint: disable=protected-access\n    r\"\"\"\n    General learning rate scheduler.\n\n    PyTorch LRScheduler is hard to extend.\n    This class is a wrapper of PyTorch LRScheduler, which provides a more general interface.\n    You only needs to add a new method which calculates a learning rate ratio (range from 0 to 1)\n    with total progress (range from 0 to 1), and everything else will be done automatically.\n\n    Moreover, this class has warmup and cooldown built-in.\n    By default, the first 5% and last 20% of training steps will be warmup and cooldown respectively.\n    You can alternate by passing `warmup_steps` and `cooldown_steps`, or disable them by setting them to 0.\n\n    Args:\n        optimizer: Wrapped optimizer.\n        total_steps: Total number of steps.\n        final_lr_ratio: Final learning rate ratio to initial learning rate.\n            Defaults to 100.\n        final_lr: Final learning rate. Deprecated, use `final_lr_ratio` instead.\n            Defaults to None.\n        min_lr: Minimal learning rate.\n            Defaults to 1e-9.\n        strategy: Scaling strategy.\n            Defaults to \"cosine\".\n        warmup_steps: Number of warmup steps.\n            Defaults to `steps // 20`.\n        cooldown_steps: Number of cooldown steps.\n            Defaults to `steps // 5`.\n        last_epoch: The index of last epoch.\n            Defaults to -1.\n        method: Method to calculate learning rate given ratio, should be one of \"percentile\" or \"linear\".\n            Defaults to \"percentile\".\n\n    Examples:\n        &gt;&gt;&gt; from danling.optim import LRScheduler\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from torch import optim\n        &gt;&gt;&gt; optimizer = optim.SGD([{'params': torch.tensor([0])}], lr=1, momentum=0.9)\n        &gt;&gt;&gt; scheduler = LRScheduler(optimizer, total_steps=5, final_lr_ratio=1e-5, strategy='linear')\n        &gt;&gt;&gt; lrs = []\n        &gt;&gt;&gt; for epoch in range(5):\n        ...     lrs.append(scheduler.get_lr()[0])\n        ...     scheduler.step()\n        &gt;&gt;&gt; [round(lr, 10) for lr in lrs]\n        [0.1, 0.01, 0.001, 0.0001, 1e-09]\n        &gt;&gt;&gt; scheduler = LRScheduler(optimizer, total_steps=5, final_lr_ratio=1e-5, strategy='cosine')\n        &gt;&gt;&gt; lrs = []\n        &gt;&gt;&gt; for epoch in range(5):\n        ...     lrs.append(scheduler.get_lr()[0])\n        ...     scheduler.step()\n        &gt;&gt;&gt; [round(lr, 10) for lr in lrs]\n        [0.3330753446, 0.0187302031, 0.000533897, 3.00232e-05, 1e-09]\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer: Optimizer,\n        total_steps: int,\n        final_lr_ratio: float = 100,\n        final_lr: Optional[float] = None,\n        min_lr: float = 1e-9,\n        strategy: str = \"cosine\",\n        warmup_steps: Optional[int] = None,\n        cooldown_steps: Optional[int] = None,\n        last_epoch: int = -1,\n        method: str = \"percentile\",\n    ):\n        if total_steps &lt;= 0:\n            raise ValueError(f\"Total steps must be positive, but got {total_steps}\")\n        if warmup_steps is None:\n            warmup_steps = total_steps // 20\n        elif warmup_steps &gt; total_steps:\n            raise ValueError(f\"Warmup steps must be less than total steps, but got {warmup_steps} &gt; {total_steps}\")\n        elif warmup_steps &lt; 0:\n            raise ValueError(f\"Warmup steps must be positive, but got {warmup_steps}\")\n        if cooldown_steps is None:\n            cooldown_steps = total_steps // 5\n        elif cooldown_steps &gt; total_steps:\n            raise ValueError(f\"Cooldown steps must be less than total steps, but got {cooldown_steps} &gt; {total_steps}\")\n        elif cooldown_steps &lt; 0:\n            raise ValueError(f\"Cooldown steps must be positive, but got {cooldown_steps}\")\n        if final_lr_ratio &lt; 0:\n            raise ValueError(f\"`final_lr_ratio` must be positive, but got {final_lr_ratio}\")\n        if min_lr &lt; 0:\n            raise ValueError(f\"`min_lr` must be positive, but got {min_lr}\")\n        self.strategies = {\n            k: v for k, v in self.__class__.__dict__.items() if callable(v) and (not k.startswith(\"_\") or k in \"get_lr\")\n        }\n        if strategy not in self.strategies:\n            raise ValueError(f\"Scaling strategy must be one of {self.strategies.keys()}, but got {strategy}\")\n        self.total_steps = total_steps\n        if final_lr is not None:\n            warn(\n                \"Argument `final_lr` is deprecated, use `final_lr_ratio` instead\",\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n        self.final_lr = final_lr\n        self.final_lr_ratio = final_lr_ratio\n        self.min_lr = min_lr\n        self.strategy = strategy\n        self.method = method\n        self.warmup_steps = warmup_steps\n        self.cooldown_steps = cooldown_steps\n        self.cooldown_steps_begin = self.total_steps - self.cooldown_steps\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self) -&gt; List[float]:\n        step_count = self._step_count\n        if step_count &gt; self.total_steps + 1 or step_count &lt; 1:\n            warn(\n                f\"Step count {step_count} is out of range [1, {self.total_steps + 1}]\",\n                category=RuntimeWarning,\n                stacklevel=2,\n            )\n        return [self._get_lr(lr, step_count) for lr in self.base_lrs]\n\n    def _get_lr(\n        self,\n        lr: float,\n        step_count: Optional[int] = None,\n        progress: Optional[float] = None,\n        warmup_ratio: Optional[float] = None,\n        cooldown_ratio: Optional[float] = None,\n        method: Optional[str] = None,\n    ) -&gt; float:\n        method = method or self.method\n        step_count = step_count or self._step_count\n        progress = progress or min(max(step_count / self.total_steps, 0.0), 1.0)\n        final_lr = self.final_lr or lr * self.final_lr_ratio\n        ratio = getattr(self, self.strategy)(progress)\n        if method == \"percentile\":\n            lr *= pow(final_lr / lr, ratio)\n        elif method == \"numerical\":\n            lr = (1 - ratio) * (lr - final_lr) + final_lr\n        else:\n            raise ValueError(f\"Method must be one of ['percentile', 'numerical'], but got {method}\")\n        if self.warmup_steps &gt; step_count &gt; 0:\n            warmup_ratio = warmup_ratio or step_count / self.warmup_steps\n            lr = warmup_ratio * (lr - self.min_lr) + self.min_lr\n        elif self.cooldown_steps &gt; 0 and step_count &gt; self.cooldown_steps_begin:\n            cooldown_ratio = cooldown_ratio or 1 - (step_count - self.cooldown_steps_begin) / self.cooldown_steps\n            lr = cooldown_ratio * (lr - self.min_lr) + self.min_lr\n        return max(self.min_lr, lr)\n\n    def linear(self, progress: float) -&gt; float:\n        return progress\n\n    def cosine(self, progress: float) -&gt; float:\n        return 1 - ((1 + cos(pi * progress)) / 2)\n\n    def constant(self, progress: float) -&gt; float:  # pylint: disable=unused-argument\n        return 0.0\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}({self.strategy}, method={self.method}, \"\n            f\"final_lr_ratio={self.final_lr_ratio}, total_steps={self.total_steps}, \"\n            f\"warmup_steps={self.warmup_steps}, cooldown_steps={self.cooldown_steps})\"\n        )\n</code></pre>"},{"location":"runner/","title":"Runner","text":"<p>The Runner of DanLing sets up the basic environment for running neural networks.</p>"},{"location":"runner/#components","title":"Components","text":"<p>For cross-platform compatibilities, DanLing features a two-level Runner + RunnerState system.</p>"},{"location":"runner/#platformrunner","title":"PlatformRunner","text":"<p>PlatformRunner implements platform-specific features like <code>step</code> and <code>prepare</code>.</p> <p>The Runner contains all runtime information that is irrelevant to the checkpoint (e.g. <code>world_size</code>, <code>rank</code>, etc.). All other information should be saved in <code>RunnerState</code>.</p> <p>Currently, only <code>TorchRunner</code> is supported.</p>"},{"location":"runner/#baserunner","title":"<code>BaseRunner</code>","text":"<p><code>BaseRunner</code> defines shared attributes and implements platform-agnostic features, including <code>init_logging</code>, <code>results</code> and <code>scores</code>.</p>"},{"location":"runner/#runnerstate","title":"<code>RunnerState</code>","text":"<p><code>RunnerState</code> stores the state of a run (e.g. <code>epochs</code>, <code>run_id</code>, <code>network</code>, etc.).</p> <p>With <code>RunnerState</code> and corresponding weights, you can resume a run from any point. Therefore, all members in <code>RunnerState</code> will be saved in the checkpoint, and thus should be json serialisable.</p>"},{"location":"runner/#experiments-management","title":"Experiments Management","text":"<p>DanLing Runner is designed for a 3.5-level experiments management system: Project, Group, Experiment, and, Run.</p>"},{"location":"runner/#project","title":"Project","text":"<p>A project corresponds to your project.</p> <p>Generally speaking, there should be only one project for each repository.</p> <p><code>project_root</code> is the root directory of all experiments of a certain project, and should be consistent across the project.</p>"},{"location":"runner/#group","title":"Group","text":"<p>A group groups multiple experiments with similar characteristics.</p> <p>For example, if you run multiple experiments on learning rate, you may want to group them into a group.</p> <p>Note that Group is a virtual level (which is why it only counts 0.5) and does not correspond to anything. There are no attributes/properties for groups.</p>"},{"location":"runner/#experiment","title":"Experiment","text":"<p>An experiment is the basic unit of experiments.</p> <p>Each experiment corresponds to a certain commit, which means the code should be consistent across the experiment.</p> <p>DanLing will automatically generate <code>experiment_id</code> and <code>experiment_uuid</code> based on git revision. They are unique for each commit.</p> <p>You may also set a catchy custom <code>experiment_name</code> to identify each experiment.</p>"},{"location":"runner/#run","title":"Run","text":"<p>A run is the basic unit of runnings.</p> <p>Run corresponds to a certain run of an experiment, each run may have different hyperparameters.</p> <p>DanLing will automatically generate <code>run_id</code> and <code>run_uuid</code> based on <code>experiment_uuid</code> and provided config. They are unique for each commit and config.</p> <p>You may also set a catchy custom <code>run_name</code> to identify each experiment.</p>"},{"location":"runner/#identifiers","title":"Identifiers","text":"<p>DanLing has two properties built-in to help you identify each run.</p> <ul> <li><code>id</code> by default is the join of <code>experiment_id</code>, <code>run_id</code>, and <code>uuid</code>. It is automatically generated hex-strings and is unique for each run.</li> <li><code>name</code> by default is <code>experiment_name-run_name</code>. It is manually specified and easy to read. Note that <code>name</code> is not guaranteed to be unique.</li> </ul>"},{"location":"runner/#directories","title":"Directories","text":"<p>To help you manage your experiments, DanLing will automatically generate directories for you.</p> <p><code>dir</code> is the directory of a certain run, defaults to <code>{dir/name-id}</code>. All run files should be under this directory.</p> <p>In particular, <code>checkpoint_dir</code>, which defaults to <code>dir/checkpoint_dir_name</code> contains all checkpoint files.</p> <p>As a result, your <code>project_root</code> should looks like following:</p> Bash<pre><code>- {project_root}\n-     |- {name}-{id} (equivalents to {experiment_name}-{run_name}-{experiment_id}-{run_id}-{uuid})\n-       |\n-       |- {checkpoint_dir_name}\n-       |    |\n-       |    |- best.pth\n-       |    |- latest.pth\n-       |    |- epoch-10.pth\n-       |\n-       |- run.log\n-       |- runner.yaml\n-       |- results.json\n-       |- latest.json\n-       |- best.json\n</code></pre>"},{"location":"runner/base_runner/","title":"BaseRunner","text":"<p>Base class for all runners.</p> <p><code>BaseRunner</code> sets up basic running environment, including <code>seed</code>, <code>deterministic</code>, and <code>logging</code>.</p> <p><code>BaseRunner</code> also provides some basic methods, such as, <code>step</code>, <code>state_dict</code>, <code>save_checkpoint</code>, <code>load_checkpoint</code>.</p> <p><code>BaseRunner</code> defines all basic attributes and relevant properties such as <code>scores</code>, <code>progress</code>, etc.</p> <p>Core:</p> Name Type Description <code>mode</code> <code>(RunnerMode, property)</code> <p>Running mode.</p> <code>state</code> <code>RunnerState</code> <p>Running state. See <code>RunnerState</code> for details.</p> <p>Model:</p> Name Type Description <code>model</code> <code>Callable</code> <code>criterion</code> <code>Callable</code> <code>optimizer</code> <code>Any | None</code> <code>scheduler</code> <code>Any | None</code> <p>Data:</p> Name Type Description <code>datasets</code> <code>FlatDict</code> <p>All datasets, should be in the form of <code>{subset: dataset}</code>. Initialised to <code>FlatDict</code> by default.</p> <code>datasamplers</code> <code>FlatDict</code> <p>All datasamplers, should be in the form of <code>{subset: datasampler}</code>. Initialised to <code>FlatDict</code> by default.</p> <code>dataloaders</code> <code>FlatDict</code> <p>All dataloaders, should be in the form of <code>{subset: dataloader}</code>. Initialised to <code>FlatDict</code> by default.</p> <code>batch_size</code> <code>(int, property)</code> <p>Number of samples per batch in train dataloader or the first dataloader.</p> <code>batch_size_equivalent</code> <code>(int, property)</code> <p>Total batch_size (<code>batch_size * world_size * accum_steps</code>).</p> <p><code>datasets</code>, <code>datasamplers</code>, <code>dataloaders</code> should be a dict with the same keys. Their keys should be <code>split</code> (e.g. <code>train</code>, <code>val</code>, <code>test</code>).</p> <p>Progress:</p> Name Type Description <code>progress</code> <code>(float, property)</code> <p>Running Progress, in <code>range(0, 1)</code>.</p> <p>Results:</p> Name Type Description <code>results</code> <code>NestedDict</code> <p>Results include all metric information of the model. Results should be in the form of <code>{epoch: {subset: {metric: score}}}</code>.</p> <code>latest_result</code> <code>(NestedDict, property)</code> <p>Most recent result, should be in the form of <code>{subset: {metric: score}}</code>.</p> <code>best_result</code> <code>(NestedDict, property)</code> <p>Best result, should be in the form of <code>{subset: {metric: score}}</code>.</p> <code>scores</code> <code>(List[float], property)</code> <p>Score is the core metric that is used to evaluate the performance of the model. Scores should be in the form of <code>{epoch: score}</code>.</p> <code>latest_score</code> <code>(float, property)</code> <p>Most recent score, should be in the form of <code>score</code>.</p> <code>best_score</code> <code>(float, property)</code> <p>Best score, should be in the form of <code>score</code>.</p> <code>score_set</code> <code>Optional[str]</code> <p>The subset to calculate the score. If is <code>None</code>, will use the last set of the result.</p> <code>score_name</code> <code>str</code> <p>The metric name of score. Defaults to <code>\"loss\"</code>.</p> <code>is_best</code> <code>(bool, property)</code> <p>If <code>latest_score == best_score</code>.</p> <p>IO:</p> Name Type Description <code>dir</code> <code>(str, property)</code> <p>Directory of the run. Defaults to <code>os.path.join(self.project_root, f\"{self.name}-{self.id}\")</code>.</p> <code>checkpoint_dir</code> <code>(str, property)</code> <p>Directory of checkpoints.</p> <code>log_path</code> <code>(str, property)</code> <p>Path of log file.</p> <code>checkpoint_dir_name</code> <code>str</code> <p>The name of the directory under <code>runner.dir</code> to save checkpoints. Defaults to <code>\"checkpoints\"</code>.</p> <p>Parallel Training:</p> Name Type Description <code>world_size</code> <code>(int, property)</code> <p>Number of processes.</p> <code>rank</code> <code>(int, property)</code> <p>Process index of all processes.</p> <code>local_rank</code> <code>(int, property)</code> <p>Process index of local processes.</p> <code>distributed</code> <code>(bool, property)</code> <p>If runner is running in distributed mode.</p> <code>is_main_process</code> <code>(bool, property)</code> <p>If current process is the main process of all processes.</p> <code>is_local_main_process</code> <code>(bool, property)</code> <p>If current process is the main process of local processes.</p> <p>logging:</p> Name Type Description <code>meters</code> <code>AverageMeters</code> <p>Average meters. Initialised to <code>AverageMeters</code> by default.</p> <code>metrics</code> <code>Metrics</code> <p>Metrics for evaluating.</p> <code>logger</code> <code>Logger | None</code> <code>writer</code> <code>Any | None</code> See Also <p><code>RunnerState</code>: The runeer base that stores runtime information. <code>BaseRunner</code>: The base runner class.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>class BaseRunner(metaclass=RunnerMeta):  # pylint: disable=too-many-public-methods\n    r\"\"\"\n    Base class for all runners.\n\n    `BaseRunner` sets up basic running environment, including `seed`, `deterministic`, and `logging`.\n\n    `BaseRunner` also provides some basic methods, such as, `step`, `state_dict`, `save_checkpoint`, `load_checkpoint`.\n\n    `BaseRunner` defines all basic attributes and relevant properties such as `scores`, `progress`, etc.\n\n    Attributes: Core:\n        mode (RunnerMode, property): Running mode.\n        state (RunnerState): Running state. See `RunnerState` for details.\n\n    Attributes: Model:\n        model (Callable):\n        criterion (Callable):\n        optimizer:\n        scheduler:\n\n    Attributes: Data:\n        datasets (FlatDict): All datasets, should be in the form of ``{subset: dataset}``.\n            Initialised to `FlatDict` by default.\n        datasamplers (FlatDict): All datasamplers, should be in the form of ``{subset: datasampler}``.\n            Initialised to `FlatDict` by default.\n        dataloaders (FlatDict): All dataloaders, should be in the form of ``{subset: dataloader}``.\n            Initialised to `FlatDict` by default.\n        batch_size (int, property): Number of samples per batch in train dataloader or the first dataloader.\n        batch_size_equivalent (int, property): Total batch_size (`batch_size * world_size * accum_steps`).\n\n    `datasets`, `datasamplers`, `dataloaders` should be a dict with the same keys.\n    Their keys should be `split` (e.g. `train`, `val`, `test`).\n\n    Attributes: Progress:\n        progress (float, property): Running Progress, in `range(0, 1)`.\n\n    Attributes: Results:\n        results (NestedDict): Results include all metric information of the model.\n            Results should be in the form of `{epoch: {subset: {metric: score}}}`.\n        latest_result (NestedDict, property): Most recent result, should be in the form of `{subset: {metric: score}}`.\n        best_result (NestedDict, property): Best result, should be in the form of `{subset: {metric: score}}`.\n        scores (List[float], property): Score is the core metric that is used to evaluate the performance of the model.\n            Scores should be in the form of `{epoch: score}`.\n        latest_score (float, property): Most recent score, should be in the form of `score`.\n        best_score (float, property): Best score, should be in the form of `score`.\n        score_set (Optional[str]): The subset to calculate the score.\n            If is `None`, will use the last set of the result.\n        score_name (str): The metric name of score.\n            Defaults to `\"loss\"`.\n        is_best (bool, property): If `latest_score == best_score`.\n\n    Attributes: IO:\n        dir (str, property): Directory of the run.\n            Defaults to `os.path.join(self.project_root, f\"{self.name}-{self.id}\")`.\n        checkpoint_dir (str, property): Directory of checkpoints.\n        log_path (str, property):  Path of log file.\n        checkpoint_dir_name (str): The name of the directory under `runner.dir` to save checkpoints.\n            Defaults to `\"checkpoints\"`.\n\n    Attributes: Parallel Training:\n        world_size (int, property): Number of processes.\n        rank (int, property): Process index of all processes.\n        local_rank (int, property): Process index of local processes.\n        distributed (bool, property): If runner is running in distributed mode.\n        is_main_process (bool, property): If current process is the main process of all processes.\n        is_local_main_process (bool, property): If current process is the main process of local processes.\n\n    Attributes: logging:\n        meters (AverageMeters): Average meters.\n            Initialised to `AverageMeters` by default.\n        metrics (Metrics): Metrics for evaluating.\n        logger:\n        writer:\n\n    See Also:\n        [`RunnerState`][danling.runner.runner_state.RunnerState]: The runeer base that stores runtime information.\n        [`BaseRunner`][danling.runner.BaseRunner]: The base runner class.\n    \"\"\"\n\n    # DO NOT set default value in class, as they won't be stored in `__dict__`.\n\n    _mode: RunnerMode\n    state: RunnerState\n\n    model: Callable | None = None\n    criterion: Callable | None = None\n    optimizer: Any | None = None\n    scheduler: Any | None = None\n\n    datasets: FlatDict\n    datasamplers: FlatDict\n    dataloaders: FlatDict\n\n    meters: AverageMeters\n    metrics: Metrics | None = None\n    logger: logging.Logger | None = None\n    writer: Any | None = None\n\n    def __init__(self, config: NestedDict) -&gt; None:\n        if \"datasets\" not in self.__dict__:\n            self.datasets = FlatDict()\n        if \"datasamplers\" not in self.__dict__:\n            self.datasamplers = FlatDict()\n        if \"dataloaders\" not in self.__dict__:\n            self.dataloaders = FlatDict()\n        self._mode = RunnerMode.train\n        self.meters = AverageMeters()\n        self.metrics = None\n        # must init state at last to avoid conflict names\n        self.state = RunnerState(config)\n        self.init_distributed()\n        if self.state.seed is not None:\n            self.set_seed()\n        if self.state.deterministic:\n            self.set_deterministic()\n        if os.listdir(self.dir):\n            warn(\n                f\"Directory `{self.dir}` is not empty.\",\n                category=RuntimeWarning,\n                stacklevel=2,\n            )\n        if self.state.log:\n            self.init_logging()\n        self.init_print()\n        if self.state.tensorboard:\n            self.init_tensorboard()\n\n    def __post_init__(self, *args, **kwargs) -&gt; None:\n        pass\n\n    @property\n    def mode(self) -&gt; RunnerMode:\n        return self._mode\n\n    @mode.setter\n    def mode(self, mode: str | RunnerMode) -&gt; None:\n        if isinstance(mode, str):\n            mode = RunnerMode(mode)\n        self._mode = mode\n\n    @cached_property\n    def batch_size(self) -&gt; int:\n        r\"\"\"\n        Batch size.\n\n        Notes:\n            If `train` is in `dataloaders`, then `batch_size` is the batch size of `train`.\n            Otherwise, `batch_size` is the batch size of the first dataloader.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        batch_size = self.state.get(\"dataloader.batch_size\")\n        if batch_size:\n            return batch_size\n        if self.dataloaders:\n            loader = self.dataloaders[\"train\"] if \"train\" in self.dataloaders else next(iter(self.dataloaders.values()))\n            return loader.batch_size\n        raise AttributeError(\"batch_size could not be inferred, since no dataloader found.\")\n\n    @property\n    def batch_size_equivalent(self) -&gt; int:\n        r\"\"\"\n        Actual batch size.\n\n        Returns:\n            (int): `batch_size` * `world_size` * `accum_steps`\n        \"\"\"\n\n        return self.batch_size * self.world_size * self.accum_steps\n\n    @cached_property\n    def total_epochs(self) -&gt; int:\n        if self.state.epoch_end:\n            return self.state.epoch_end - self.state.epoch_begin\n        raise ValueError(\"epoch_end is not specified\")\n\n    @cached_property\n    def total_steps(self) -&gt; int:\n        if self.state.step_end:\n            return self.state.step_end - self.state.step_begin\n        dataset = self.datasets.get(\"train\", next(iter(self.datasets.values())))\n        return self.total_epochs * ceil(len(dataset) / self.batch_size)\n\n    @cached_property\n    def accum_steps(self) -&gt; int:\n        r\"\"\"\n        Accumulated steps.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        return self.state.get(\"accum_steps\", 1)\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Initialise distributed running environment.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @property\n    def device(self) -&gt; Any:\n        r\"\"\"\n        Device of runner.\n        \"\"\"\n\n        return \"cpu\"\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of processes.\n        \"\"\"\n\n        return 1\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index of all processes.\n        \"\"\"\n\n        return 0\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index of local processes.\n        \"\"\"\n\n        return 0\n\n    @property\n    def distributed(self) -&gt; bool:\n        r\"\"\"\n        If runner is running in distributed mode.\n        \"\"\"\n\n        return self.world_size &gt; 1\n\n    @property\n    def is_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process of all processes.\n        \"\"\"\n\n        return self.rank == 0\n\n    @property\n    def is_local_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process of local processes.\n        \"\"\"\n\n        return self.local_rank == 0\n\n    @catch\n    def save(self, obj: Any, file: PathStr, main_process_only: bool = True, *args, **kwargs) -&gt; File:\n        r\"\"\"\n        Save any file with supported extensions.\n\n        `Runner.save` internally calls `dl.save`,\n        but with additional arguments to allow it save only on the main process.\n        Moreover, any error raised by `Runner.save` will be caught and logged.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            return save(obj, file, *args, **kwargs)\n        return file\n\n    @staticmethod\n    def load(file: PathStr, *args, **kwargs) -&gt; Any:\n        r\"\"\"\n        Load any file with supported extensions.\n\n        `Runner.load` is identical to `dl.load`.\n        \"\"\"\n\n        return load(file, *args, **kwargs)\n\n    def dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Convert state to Mapping.\n\n        Args:\n            cls: Target `clc to convert to.\n        \"\"\"\n\n        return self.state.dict(cls)\n\n    @catch\n    def json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n        r\"\"\"\n        Dump Runner State to json file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            return self.state.json(file, *args, **kwargs)\n\n    @classmethod\n    def from_json(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from json file.\n\n        This function calls `self.from_jsons()` to construct object from json string.\n        You may overwrite `from_jsons` in case something is not json serializable.\n        \"\"\"\n\n        with FlatDict.open(file) as fp:\n            return cls.from_jsons(fp.read(), *args, **kwargs)\n\n    def jsons(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner State to json string.\n        \"\"\"\n\n        return self.state.jsons(*args, **kwargs)\n\n    @classmethod\n    def from_jsons(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from json string.\n        \"\"\"\n\n        return cls(Config.from_jsons(string, *args, **kwargs))\n\n    @catch\n    def yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n        r\"\"\"\n        Dump Runner State to yaml file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            return self.state.yaml(file, *args, **kwargs)\n\n    @classmethod\n    def from_yaml(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from yaml file.\n\n        This function calls `self.from_yamls()` to construct object from yaml string.\n        You may overwrite `from_yamls` in case something is not yaml serializable.\n        \"\"\"\n\n        with FlatDict.open(file) as fp:\n            return cls.from_yamls(fp.read(), *args, **kwargs)\n\n    def yamls(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner State to yaml string.\n        \"\"\"\n\n        return self.state.yamls(*args, **kwargs)\n\n    @classmethod\n    def from_yamls(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from yaml string.\n        \"\"\"\n\n        return cls(Config.from_yamls(string, *args, **kwargs))\n\n    @property\n    def progress(self) -&gt; float:\n        r\"\"\"\n        Training Progress.\n\n        Returns:\n            (float):\n\n        Raises:\n            RuntimeError: If no terminal is defined.\n        \"\"\"\n\n        return self.steps / self.total_steps\n\n    @property\n    def best_fn(self) -&gt; Callable:\n        r\"\"\"\n        Function to determine the best score from a list of scores.\n\n        By default, the `best_fn` returns `min` if `self.state.score_name` is `loss`,\n        otherwise, returns `max`.\n\n        Subclass can override this method to accommodate needs, such as `min`.\n\n        Returns:\n            (callable):\n        \"\"\"\n\n        return max if self.state.score_name != \"loss\" else min\n\n    @property\n    def best_index(self) -&gt; int:\n        r\"\"\"\n        Find the best index from all scores.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        if not self.scores:\n            return 0\n        values = list(self.scores.values())\n        return self.best_fn(range(len(values)), key=values.__getitem__)\n\n    @property\n    def latest_result(self) -&gt; NestedDict | None:\n        r\"\"\"\n        Latest result.\n        \"\"\"\n\n        if not self.state.results:\n            return None\n        latest_index = next(reversed(self.state.results if PY38_PLUS else list(self.state.results)))  # type: ignore\n        ret = self.state.results[latest_index].clone()\n        ret[\"index\"] = latest_index\n        return ret\n\n    @property\n    def best_result(self) -&gt; NestedDict | None:\n        r\"\"\"\n        Best result.\n        \"\"\"\n\n        if not self.state.results:\n            return None\n        best_index = self.best_index\n        ret = self.state.results[best_index].clone()\n        ret[\"index\"] = best_index\n        return ret\n\n    @property\n    def scores(self) -&gt; FlatDict | None:\n        r\"\"\"\n        All scores.\n\n        Scores are extracted from results by `score_set` and `runner.state.score_name`,\n        following `[r[score_set][self.state.score_name] for r in self.state.results]`.\n\n        Scores are considered as the index of the performance of the model.\n        It is useful to determine the best model and the best hyper-parameters.\n\n        `score_set` is defined in `self.state.score_set`.\n        If it is not set, `DanLing` will use `val` or `validate` if they appear in the `latest_result`.\n        If `DanLing` still could not find, it will fall back to the second key in the `latest_result`\n        if it contains more that one element, or the first key.\n\n        Note that certain keys are ignored when falling back, they are defined in {IGNORED_SET_NAMES}.\n        \"\"\"\n\n        if not self.state.results:\n            return None\n        subsets = [i for i in self.latest_result.keys() if i not in IGNORED_SET_NAMES]  # type: ignore\n        score_set = self.state.get(\"score_set\")\n        if score_set is None and \"val\" in subsets:\n            score_set = \"val\"\n        if score_set is None and \"validate\" in subsets:\n            score_set = \"validate\"\n        if score_set is None:\n            score_set = subsets[1] if len(subsets) &gt; 1 else subsets[0]\n        return FlatDict({k: v[score_set][self.state.score_name] for k, v in self.state.results.items()})\n\n    @property\n    def latest_score(self) -&gt; float | None:\n        r\"\"\"\n        Latest score.\n        \"\"\"\n\n        if not self.state.results:\n            return None\n        if not PY38_PLUS:\n            return next(reversed(list(self.scores.values())))  # type: ignore\n        return next(reversed(self.scores.values()))  # type: ignore\n\n    @property\n    def best_score(self) -&gt; float | None:\n        r\"\"\"\n        Best score.\n        \"\"\"\n\n        if not self.state.results:\n            return None\n        return self.scores[self.best_index]  # type: ignore\n\n    @property\n    def is_best(self) -&gt; bool:\n        r\"\"\"\n        If current epoch is the best epoch.\n        \"\"\"\n\n        if not self.state.results:\n            return True\n        try:\n            return abs(self.latest_score - self.best_score) &lt; 1e-7  # type: ignore\n        except TypeError:\n            return True\n\n    @cached_property\n    @ensure_dir\n    def dir(self) -&gt; str:\n        r\"\"\"\n        Directory of the run.\n        \"\"\"\n\n        if \"dir\" in self.state:\n            return self.state.dir\n        return os.path.join(self.project_root, f\"{self.name}-{self.id}\")\n\n    @cached_property\n    def log_path(self) -&gt; str:\n        r\"\"\"\n        Path of log file.\n        \"\"\"\n\n        if \"log_path\" in self.state:\n            return self.state.log_path\n        return os.path.join(self.dir, \"run.log\")\n\n    @cached_property\n    @ensure_dir\n    def checkpoint_dir(self) -&gt; str:\n        r\"\"\"\n        Directory of checkpoints.\n        \"\"\"\n\n        if \"checkpoint_dir\" in self.state:\n            return self.state.checkpoint_dir\n        return os.path.join(self.dir, self.checkpoint_dir_name)\n\n    # def __getattribute__(self, name) -&gt; Any:\n    #     if name in (\"__class__\", \"__dict__\"):\n    #         return super().__getattribute__(name)\n    #     if name in self.__dict__:\n    #         return self.__dict__[name]\n    #     if name in dir(self):\n    #         return super().__getattribute__(name)\n    #     if \"state\" in self and name in self.state:\n    #         return self.state[name]\n    #     return super().__getattribute__(name)\n\n    def __getattr__(self, name) -&gt; Any:\n        if \"state\" in self and name in self.state:\n            return self.state[name]\n        return super().__getattribute__(name)\n\n    def __setattr__(self, name, value) -&gt; None:\n        if name in self.__dict__:\n            if isinstance(self.__dict__[name], Variable):\n                self.__dict__[name].set(value)\n            else:\n                self.__dict__[name] = value\n            return\n        if name in dir(self):\n            if isinstance(super().__getattribute__(name), Variable):\n                super().__getattribute__(name).set(value)\n            else:\n                object.__setattr__(self, name, value)\n            return\n        if \"state\" in self and name in self.state:\n            if isinstance(self.state[name], Variable):\n                self.state[name].set(value)\n            else:\n                self.state[name] = value\n            return\n        object.__setattr__(self, name, value)\n\n    def __contains__(self, name) -&gt; bool:\n        return name in dir(self) or (\"state\" in self.__dict__ and name in dir(self.state))\n\n    def __repr__(self):\n        lines = []\n        for key, value in self.__dict__.items():\n            value_str = repr(value)\n            value_str = self._add_indent(value_str)\n            lines.append(\"(\" + key + \"): \" + value_str)\n\n        main_str = self.__class__.__name__ + \"(\"\n        if lines:\n            main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n        main_str += \")\"\n        return main_str\n\n    def _add_indent(self, text):\n        lines = text.split(\"\\n\")\n        # don't do anything for single-line stuff\n        if len(lines) == 1:\n            return text\n        first = lines.pop(0)\n        # add 2 spaces to each line but the first\n        lines = [(2 * \" \") + line for line in lines]\n        lines = \"\\n\".join(lines)\n        lines = first + \"\\n\" + lines\n        return lines\n\n    def init_deepspeed(  # pylint: disable=too-many-branches, too-many-statements\n        self, config: Dict = None  # type: ignore\n    ) -&gt; Dict:\n        r\"\"\"\n        Preprocess DeepSpeed config.\n        \"\"\"\n\n        if config is None:\n            config = self.state.get(\"deepspeed\")\n        if config is None:\n            return {}\n        if isinstance(config, str):\n            config = NestedDict.load(config)\n        if config.get(\"steps_per_print\", \"auto\") == \"auto\":\n            config[\"steps_per_print\"] = self.print_interval\n        if config.get(\"train_micro_batch_size_per_gpu\", \"auto\") == \"auto\":\n            config[\"train_micro_batch_size_per_gpu\"] = self.batch_size\n        if \"amp\" in config:\n            amp = config[\"amp\"]\n            if amp.get(\"enabled\", \"auto\") == \"auto\":\n                amp[\"enabled\"] = \"true\"\n            if amp.get(\"opt_level\", \"auto\") == \"auto\":\n                amp[\"opt_level\"] = \"O1\"\n        if \"zero_optimization\" in config:\n            zero = config[\"zero_optimization\"]\n            if zero.get(\"allgather_bucket_size\") == \"auto\":\n                zero[\"allgather_bucket_size\"] = 1e6\n            if zero.get(\"reduce_bucket_size\") == \"auto\":\n                zero[\"reduce_bucket_size\"] = 1e6\n            if zero.get(\"stage3_max_live_parameters\") == \"auto\":\n                zero[\"stage3_max_live_parameters\"] = 1e8\n            if zero.get(\"stage3_max_live_gradients\") == \"auto\":\n                zero[\"stage3_max_live_gradients\"] = 1e8\n            if zero.get(\"stage3_max_reuse_distance\") == \"auto\":\n                zero[\"stage3_max_reuse_distance\"] = 1e8\n            if zero.get(\"stage3_prefetch_bucket_size\") == \"auto\":\n                zero[\"stage3_prefetch_bucket_size\"] = 1e6\n            if zero.get(\"stage3_param_persistence_threshold\") == \"auto\":\n                zero[\"stage3_param_persistence_threshold\"] = 1e8\n            if \"amp\" in config:\n                if \"fp16\" not in config:\n                    config[\"fp16\"] = {}\n                if config[\"fp16\"].get(\"enabled\", \"auto\"):\n                    config[\"fp16\"][\"enabled\"] = config[\"amp\"][\"enabled\"]\n                warn(\n                    f\"AMP is not compatible with ZeRO. Automatically set 'fp16' to {config['amp']['enabled']}\",\n                    stacklevel=2,\n                )\n                del config[\"amp\"]\n        if \"optimizer\" in config:\n            if \"params\" not in config[\"optimizer\"]:\n                config[\"optimizer\"][\"params\"] = {}\n            optimizer = config[\"optimizer\"][\"params\"]\n            if optimizer.get(\"lr\", \"auto\") == \"auto\":\n                optimizer[\"lr\"] = self.state.get(\"optim.lr\", 1e-3)\n            if optimizer.get(\"weight_decay\", \"auto\") == \"auto\":\n                optimizer[\"weight_decay\"] = self.state.get(\"optim.weight_decay\", 1e-2)\n            if optimizer.get(\"betas\") == \"auto\":\n                optimizer[\"betas\"] = (0.9, 0.999)\n            if optimizer.get(\"eps\") == \"auto\":\n                optimizer[\"eps\"] = 1e-8\n        if \"scheduler\" in config:\n            if \"params\" not in config[\"scheduler\"]:\n                config[\"scheduler\"][\"params\"] = {}\n            scheduler = config[\"scheduler\"][\"params\"]\n            if scheduler.get(\"total_num_steps\", \"auto\") == \"auto\":\n                scheduler[\"total_num_steps\"] = self.total_steps\n            if scheduler.get(\"warmup_num_steps\", \"auto\") == \"auto\":\n                scheduler[\"warmup_num_steps\"] = scheduler[\"total_num_steps\"] // 20\n            if scheduler.get(\"warmup_max_lr\", \"auto\") == \"auto\":\n                if self.optimizer:\n                    scheduler[\"warmup_max_lr\"] = self.optimizer.param_groups[0][\"lr\"]\n                elif \"optimizer\" in config:\n                    scheduler[\"warmup_max_lr\"] = config[\"optimizer\"][\"params\"][\"lr\"]\n                else:\n                    raise ValueError(\"warmup_max_lr is not defined and cannot be inferred\")\n            if scheduler.get(\"warmup_min_lr\", \"auto\") == \"auto\":\n                scheduler[\"warmup_min_lr\"] = 1e-7\n        return config\n\n    @on_main_process\n    def init_logging(self) -&gt; None:\n        r\"\"\"\n        Set up logging.\n        \"\"\"\n\n        os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n        # Why is setting up proper logging so !@?#! ugly?\n        logging.config.dictConfig(\n            {\n                \"version\": 1,\n                \"disable_existing_loggers\": False,\n                \"formatters\": {\n                    \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n                },\n                \"handlers\": {\n                    \"stdout\": {\n                        \"level\": \"INFO\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.StreamHandler\",\n                        \"stream\": \"ext://sys.stdout\",\n                    },\n                    \"logfile\": {\n                        \"level\": \"DEBUG\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.FileHandler\",\n                        \"filename\": self.log_path,\n                        \"mode\": \"a\",\n                    },\n                },\n                \"loggers\": {\n                    \"\": {\n                        \"handlers\": [\"stdout\", \"logfile\"],\n                        \"level\": \"DEBUG\",\n                        \"propagate\": True,\n                    },\n                },\n            }\n        )\n        logging.captureWarnings(True)\n        self.logger = logging.getLogger(\"runner\")\n        self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n\n    def init_print(self, process: int = 0) -&gt; None:\n        r\"\"\"\n        Set up `print`.\n\n        Only print on a specific `process` or when `force = True`.\n\n        Args:\n            process: The process to `print` on.\n\n        Notes\n        -----\n        If `self.state.log = True`, the default `print` function will be override by `logging.info`.\n        \"\"\"\n\n        logger = logging.getLogger(\"print\")\n        logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n        import builtins as __builtin__  # pylint: disable=C0415\n\n        builtin_print = __builtin__.print\n\n        @catch\n        def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=redefined-builtin\n            if self.rank == process or force:\n                if self.state.log:\n                    logger.info(*args, **kwargs)\n                else:\n                    builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n        __builtin__.print = print\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        raise NotImplementedError\n\n    def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Args:\n            seed: Random seed to set.\n                Defaults to `self.state.seed` (`config.seed`).\n\n            bias: Make the seed different for each processes.\n\n                This avoids same data augmentation are applied on every processes.\n\n                Defaults to `self.rank`.\n\n                Set to `False` to disable this feature.\n        \"\"\"\n\n        seed = seed or self.state.seed\n        bias = bias or self.rank\n        if bias:\n            seed += bias\n        if np_random is not None:\n            np_random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def scale_lr(\n        self,\n        lr: float,\n        lr_scale_factor: float | None = None,\n        batch_size_base: int | None = None,\n    ) -&gt; float:\n        r\"\"\"\n        Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n        \"\"\"\n\n        if lr_scale_factor in self.state:\n            lr_scale_factor = self.state.lr_scale_factor\n\n        if lr_scale_factor is None:\n            if batch_size_base is None:\n                batch_size_base = getattr(self, \"batch_size_base\", None)\n                if batch_size_base is None:\n                    raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n            lr_scale_factor = self.batch_size_equivalent / batch_size_base\n        elif batch_size_base is not None:\n            warn(\n                \"batch_size_base will be ignored if lr_scale_factor is specified\", category=RuntimeWarning, stacklevel=2\n            )\n        lr = lr * lr_scale_factor\n        self.state.lr_scale_factor = lr_scale_factor\n        return lr\n\n    def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n        r\"\"\"\n        Backward loss and step optimizer &amp; scheduler.\n\n        This method increment `self.state.steps`.\n\n        This method also increment `self.state.iters` when `batch_size` is specified.\n\n        Args:\n            zero_grad: Whether to zero the gradients.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Return dict of all attributes for checkpoint.\n        \"\"\"\n\n        return cls(self.state)\n\n    @catch\n    @on_main_process\n    def save_checkpoint(self) -&gt; None:\n        r\"\"\"\n        Save checkpoint to `self.checkpoint_dir`.\n\n        The checkpoint will be saved to `self.checkpoint_dir/latest.pth`.\n\n        If `self.state.save_interval` is positive and `self.state.epochs + 1` is a multiple of `save_interval`,\n        the checkpoint will also be copied to `self.checkpoint_dir/epoch-{self.state.epochs}.pth`.\n\n        If `self.is_best` is `True`, the checkpoint will also be copied to `self.checkpoint_dir/best.pth`.\n        \"\"\"\n\n        latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        self.save(self.state_dict(), latest_path)\n        if (\n            hasattr(self, \"save_interval\")\n            and self.save_interval &gt; 0\n            and (self.state.epochs + 1) % self.save_interval == 0\n        ):\n            save_path = os.path.join(self.checkpoint_dir, f\"epoch-{self.state.epochs}.pth\")\n            shutil.copy(latest_path, save_path)\n        if self.is_best:\n            best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n            shutil.copy(latest_path, best_path)\n\n    def load_checkpoint(\n        self,\n        checkpoint: Mapping | bytes | str | os.PathLike | None = None,\n        auto_resume: bool | None = None,\n        override_state: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Load info from checkpoint.\n\n        Args:\n            checkpoint: Checkpoint (or its path) to load.\n                Defaults to `self.state.checkpoint`.\n            auto_resume: Automatically resume from latest checkpoint if exists.\n                Defaults to `False`.\n                If is `True` and `checkpoint` is None, will set it to `self.checkpoint_dir/latest.pth`.\n            override_state: If True, override runner state with checkpoint state.\n                Defaults to `False`.\n            *args: Additional arguments to pass to `self.load`.\n            **kwargs: Additional keyword arguments to pass to `self.load`.\n\n        Raises:\n            FileNotFoundError: If `checkpoint` does not exists.\n\n        See Also:\n            [`from_checkpoint`][danling.BaseRunner.from_checkpoint]: Build runner from checkpoint.\n            [`load_pretrained`][danling.BaseRunner.load_pretrained]: Load parameters from pretrained checkpoint.\n        \"\"\"\n\n        checkpoint = checkpoint if checkpoint is not None else self.state.get(\"checkpoint\")\n        auto_resume = auto_resume if auto_resume is not None else self.state.get(\"auto_resume\", False)\n\n        # TODO: Support loading checkpoints in other format\n        if checkpoint is not None:\n            if auto_resume:\n                warn(\n                    \"latest checkpoint is preempted by value specified in checkpoint\",\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n            if isinstance(checkpoint, (bytes, str, os.PathLike)):\n                if not os.path.exists(checkpoint):\n                    raise FileNotFoundError(f\"checkpoint is set to {checkpoint!r} but does not exist.\")\n                self.state.checkpoint = checkpoint\n                ckpt = self.load(checkpoint, *args, **kwargs)\n            elif isinstance(checkpoint, Mapping):\n                ckpt = checkpoint\n            else:\n                raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint.\")\n        elif auto_resume:\n            checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n            if os.path.exists(checkpoint):\n                self.state.checkpoint = checkpoint\n                ckpt = self.load(checkpoint, *args, **kwargs)\n            else:\n                warn(\"latest checkpoint does not exits\", category=RuntimeWarning, stacklevel=2)\n                return\n        else:\n            raise ValueError(\"checkpoint is not specified and auto_resume is not set to True\")\n\n        # TODO: Wrap state_dict in a dataclass\n        self.state.merge(ckpt[\"runner\"], overwrite=override_state)\n        if self.model is not None and \"model\" in ckpt:\n            model = self.unwrap_model(self.model)\n            model.load_state_dict(ckpt[\"model\"])\n        if self.optimizer is not None and \"optimizer\" in ckpt:\n            self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n        if self.scheduler is not None and \"scheduler\" in ckpt:\n            self.scheduler.load_state_dict(ckpt[\"scheduler\"])\n        self.state.iter_begin = self.state.iters\n        self.state.step_begin = self.state.steps\n        self.state.epoch_begin = self.state.epochs\n\n    @classmethod\n    def from_checkpoint(cls, checkpoint: Mapping | bytes | str | os.PathLike, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Build BaseRunner from checkpoint.\n\n        Args:\n            checkpoint: Checkpoint (or its path) to load.\n            *args: Additional arguments to pass to `cls.load`.\n            **kwargs: Additional keyword arguments to pass to `cls.load`.\n\n        Returns:\n            (BaseRunner):\n        \"\"\"\n\n        if isinstance(checkpoint, (bytes, str, os.PathLike)):\n            ckpt = cls.load(checkpoint, *args, **kwargs)\n        elif isinstance(checkpoint, Mapping):\n            ckpt = checkpoint\n        else:\n            raise ValueError(f\"checkpoint is set to {checkpoint} but is not a valid checkpoint.\")\n        runner = cls(**ckpt[\"runner\"])\n        runner.load_checkpoint(ckpt, override_state=False)\n        return runner\n\n    def load_pretrained(self, checkpoint: Mapping | bytes | str | os.PathLike | None = None, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Load parameters from pretrained checkpoint.\n\n        This method only loads the model weights.\n\n        Args:\n            checkpoint: Pretrained checkpoint (or its path) to load.\n                Defaults to `self.state.pretrained`.\n            *args: Additional arguments to pass to `self.load`.\n            **kwargs: Additional keyword arguments to pass to `self.load`.\n\n        Raises:\n            FileNotFoundError: If `checkpoint` does not exists.\n\n        See Also:\n            [`load_checkpoint`][danling.BaseRunner.load_checkpoint]: Load info from checkpoint.\n        \"\"\"\n\n        # TODO: Support loading checkpoints in other format\n        checkpoint = checkpoint if checkpoint is not None else self.state.get(\"pretrained\")\n        if checkpoint is None:\n            raise ValueError(\"pretrained is not specified\")\n        if isinstance(checkpoint, (bytes, str, os.PathLike)):\n            if not os.path.exists(checkpoint):\n                raise FileNotFoundError(f\"pretrained is set to {checkpoint!r} but does not exist.\")\n            ckpt = self.load(checkpoint, *args, **kwargs)\n        elif isinstance(checkpoint, Mapping):\n            ckpt = checkpoint\n        else:\n            raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint.\")\n        ckpt = ckpt.get(\"model\", ckpt)\n        ckpt = ckpt.get(\"state_dict\", ckpt)\n        model = self.unwrap_model(self.model)\n        model.load_state_dict(ckpt)\n\n    def append_result(self, result: NestedDict, index: int | None = None) -&gt; None:\n        r\"\"\"\n        Append result to `self.state.results`.\n\n        Warnings:\n            `self.state.results` is heavily relied upon for computing metrics.\n\n            Failed to use this method may lead to unexpected behavior.\n        \"\"\"\n\n        if index is None:\n            index = self.state.epochs\n            global __APPEND_RESULT_COUNTER__  # pylint: disable=global-statement\n            __APPEND_RESULT_COUNTER__ += 1\n            if index == 0 and __APPEND_RESULT_COUNTER__ &gt; 1:\n                warn(\n                    \"\"\"\n                    Automatically set index to `self.state.epochs`.\n                    Please ensure `self.state.epochs` updates before calling `append_result`\n                    \"\"\",\n                    category=RuntimeWarning,\n                    stacklevel=2,\n                )\n        if index in self.state.results:\n            self.state.results[index].merge(result)\n        else:\n            self.state.results[index] = result\n\n    def print_result(self) -&gt; None:\n        r\"\"\"\n        Print latest and best result.\n        \"\"\"\n\n        print(f\"results: {self.state.results}\")\n        print(f\"latest result: {self.latest_result}\")\n        print(f\"best result: {self.best_result}\")\n\n    def step_log(self, split: str, iteration: int, length: int | None = None):\n        if length is None:\n            length = len(self.dataloaders[split]) - 1\n        result = self.meters.val\n        if self.metrics is not None:\n            result.merge(self.metrics.val)\n        print(self.format_step_result(result, split, iteration, length))\n        if self.mode == \"train\":\n            self.write_result(result, split)\n        return result\n\n    def format_step_result(self, result: NestedDict, split: str, steps: int, length: int) -&gt; str:\n        result = NestedDict(result).clone()\n        repr_str = \"\"\n        if split is not None:\n            if self.mode == \"train\":\n                repr_str = f\"training on {split} \"\n            elif self.mode == \"eval\":\n                repr_str = f\"evaluating on {split} \"\n            else:\n                repr_str = f\"running in {self.mode} mode on {split} \"\n        repr_str += f\"[{steps}/{length}]\\t\"\n        return repr_str + self.format_result(result)\n\n    def format_epoch_result(self, result: NestedDict, epochs: int | None = None, epoch_end: int | None = None) -&gt; str:\n        result = NestedDict(result).clone()\n        epochs = epochs or self.state.epochs\n        epoch_end = epoch_end or self.state.epoch_end\n        repr_str = f\"epoch [{epochs}/{epoch_end - 1}]\\n\" if epochs is not None and epoch_end else \"\"\n        repr_str += \"\\n\".join([f\"{k}:\\t{self.format_result(v)}\" for k, v in result.items()])\n        return repr_str\n\n    def format_result(self, result):\n        return \"\\t\".join([f\"{k}: {v}\" for k, v in result.items()])\n\n    def write_result(self, result: NestedDict, split: str, steps: int | None = None):\n        if steps is None:\n            steps = self.steps\n        for name, score in result.all_items():\n            name = name.replace(\".\", \"/\")\n            if name == \"loss\" and isinstance(score, AverageMeter):\n                score = score.avg\n            if isinstance(score, Sequence):\n                for i, s in enumerate(score):\n                    self.write_score(f\"{name}/{i}\", s, split, steps)\n            elif isinstance(score, Mapping):\n                for k, s in score.items():\n                    self.write_score(f\"{name}/{k}\", s, split, steps)\n            else:\n                self.write_score(name, score, split, steps)\n\n    def write_score(self, name: str, score: float, split: str, steps: int):\n        if self.writer:\n            self.writer.add_scalar(f\"{split}/{name}\", score, steps)\n\n    @catch\n    @on_main_process\n    def save_result(self) -&gt; None:\n        r\"\"\"\n        Save result to `self.dir`.\n\n        This method will save latest and best result to\n        `self.dir/latest.json` and `self.dir/best.json` respectively.\n        \"\"\"\n\n        results_path = os.path.join(self.dir, \"results.json\")\n        self.save(\n            {\n                \"id\": self.state.id,\n                \"name\": self.state.name,\n                \"results\": self.state.results,\n            },\n            results_path,\n            indent=4,\n        )\n        ret = {\"id\": self.state.id, \"name\": self.state.name}\n        result = self.latest_result\n        if isinstance(result, FlatDict):\n            result = result.dict()\n        # This is slower but ensure id is the first key\n        if result is not None:\n            ret.update(result)\n        latest_path = os.path.join(self.dir, \"latest.json\")\n        self.save(ret, latest_path, indent=4)\n        if self.is_best:\n            best_path = os.path.join(self.dir, \"best.json\")\n            shutil.copy(latest_path, best_path)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.accum_steps","title":"<code>accum_steps: int</code>  <code>cached</code> <code>property</code>","text":"<p>Accumulated steps.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.batch_size","title":"<code>batch_size: int</code>  <code>cached</code> <code>property</code>","text":"<p>Batch size.</p> Notes <p>If <code>train</code> is in <code>dataloaders</code>, then <code>batch_size</code> is the batch size of <code>train</code>. Otherwise, <code>batch_size</code> is the batch size of the first dataloader.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.batch_size_equivalent","title":"<code>batch_size_equivalent: int</code>  <code>property</code>","text":"<p>Actual batch size.</p> <p>Returns:</p> Type Description <code>int</code> <p><code>batch_size</code> * <code>world_size</code> * <code>accum_steps</code></p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.best_fn","title":"<code>best_fn: Callable</code>  <code>property</code>","text":"<p>Function to determine the best score from a list of scores.</p> <p>By default, the <code>best_fn</code> returns <code>min</code> if <code>self.state.score_name</code> is <code>loss</code>, otherwise, returns <code>max</code>.</p> <p>Subclass can override this method to accommodate needs, such as <code>min</code>.</p> <p>Returns:</p> Type Description <code>callable</code>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.best_index","title":"<code>best_index: int</code>  <code>property</code>","text":"<p>Find the best index from all scores.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.best_result","title":"<code>best_result: NestedDict | None</code>  <code>property</code>","text":"<p>Best result.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.best_score","title":"<code>best_score: float | None</code>  <code>property</code>","text":"<p>Best score.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.checkpoint_dir","title":"<code>checkpoint_dir: str</code>  <code>cached</code> <code>property</code>","text":"<p>Directory of checkpoints.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.device","title":"<code>device: Any</code>  <code>property</code>","text":"<p>Device of runner.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.dir","title":"<code>dir: str</code>  <code>cached</code> <code>property</code>","text":"<p>Directory of the run.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.distributed","title":"<code>distributed: bool</code>  <code>property</code>","text":"<p>If runner is running in distributed mode.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.is_best","title":"<code>is_best: bool</code>  <code>property</code>","text":"<p>If current epoch is the best epoch.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.is_local_main_process","title":"<code>is_local_main_process: bool</code>  <code>property</code>","text":"<p>If current process is the main process of local processes.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.is_main_process","title":"<code>is_main_process: bool</code>  <code>property</code>","text":"<p>If current process is the main process of all processes.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.latest_result","title":"<code>latest_result: NestedDict | None</code>  <code>property</code>","text":"<p>Latest result.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.latest_score","title":"<code>latest_score: float | None</code>  <code>property</code>","text":"<p>Latest score.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.local_rank","title":"<code>local_rank: int</code>  <code>property</code>","text":"<p>Process index of local processes.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.log_path","title":"<code>log_path: str</code>  <code>cached</code> <code>property</code>","text":"<p>Path of log file.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.progress","title":"<code>progress: float</code>  <code>property</code>","text":"<p>Training Progress.</p> <p>Returns:</p> Type Description <code>float</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no terminal is defined.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.rank","title":"<code>rank: int</code>  <code>property</code>","text":"<p>Process index of all processes.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.scores","title":"<code>scores: FlatDict | None</code>  <code>property</code>","text":"<p>All scores.</p> <p>Scores are extracted from results by <code>score_set</code> and <code>runner.state.score_name</code>, following <code>[r[score_set][self.state.score_name] for r in self.state.results]</code>.</p> <p>Scores are considered as the index of the performance of the model. It is useful to determine the best model and the best hyper-parameters.</p> <p><code>score_set</code> is defined in <code>self.state.score_set</code>. If it is not set, <code>DanLing</code> will use <code>val</code> or <code>validate</code> if they appear in the <code>latest_result</code>. If <code>DanLing</code> still could not find, it will fall back to the second key in the <code>latest_result</code> if it contains more that one element, or the first key.</p> <p>Note that certain keys are ignored when falling back, they are defined in {IGNORED_SET_NAMES}.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.world_size","title":"<code>world_size: int</code>  <code>property</code>","text":"<p>Number of processes.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.append_result","title":"<code>append_result(result, index=None)</code>","text":"<p>Append result to <code>self.state.results</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def append_result(self, result: NestedDict, index: int | None = None) -&gt; None:\n    r\"\"\"\n    Append result to `self.state.results`.\n\n    Warnings:\n        `self.state.results` is heavily relied upon for computing metrics.\n\n        Failed to use this method may lead to unexpected behavior.\n    \"\"\"\n\n    if index is None:\n        index = self.state.epochs\n        global __APPEND_RESULT_COUNTER__  # pylint: disable=global-statement\n        __APPEND_RESULT_COUNTER__ += 1\n        if index == 0 and __APPEND_RESULT_COUNTER__ &gt; 1:\n            warn(\n                \"\"\"\n                Automatically set index to `self.state.epochs`.\n                Please ensure `self.state.epochs` updates before calling `append_result`\n                \"\"\",\n                category=RuntimeWarning,\n                stacklevel=2,\n            )\n    if index in self.state.results:\n        self.state.results[index].merge(result)\n    else:\n        self.state.results[index] = result\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.dict","title":"<code>dict(cls=dict)</code>","text":"<p>Convert state to Mapping.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Callable</code> <p>Target `clc to convert to.</p> <code>dict</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Convert state to Mapping.\n\n    Args:\n        cls: Target `clc to convert to.\n    \"\"\"\n\n    return self.state.dict(cls)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.from_checkpoint","title":"<code>from_checkpoint(checkpoint, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Build BaseRunner from checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Mapping | bytes | str | PathLike</code> <p>Checkpoint (or its path) to load.</p> required <code>*args</code> <p>Additional arguments to pass to <code>cls.load</code>.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>cls.load</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseRunner</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_checkpoint(cls, checkpoint: Mapping | bytes | str | os.PathLike, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Build BaseRunner from checkpoint.\n\n    Args:\n        checkpoint: Checkpoint (or its path) to load.\n        *args: Additional arguments to pass to `cls.load`.\n        **kwargs: Additional keyword arguments to pass to `cls.load`.\n\n    Returns:\n        (BaseRunner):\n    \"\"\"\n\n    if isinstance(checkpoint, (bytes, str, os.PathLike)):\n        ckpt = cls.load(checkpoint, *args, **kwargs)\n    elif isinstance(checkpoint, Mapping):\n        ckpt = checkpoint\n    else:\n        raise ValueError(f\"checkpoint is set to {checkpoint} but is not a valid checkpoint.\")\n    runner = cls(**ckpt[\"runner\"])\n    runner.load_checkpoint(ckpt, override_state=False)\n    return runner\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.from_json","title":"<code>from_json(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json file.</p> <p>This function calls <code>self.from_jsons()</code> to construct object from json string. You may overwrite <code>from_jsons</code> in case something is not json serializable.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_json(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from json file.\n\n    This function calls `self.from_jsons()` to construct object from json string.\n    You may overwrite `from_jsons` in case something is not json serializable.\n    \"\"\"\n\n    with FlatDict.open(file) as fp:\n        return cls.from_jsons(fp.read(), *args, **kwargs)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.from_jsons","title":"<code>from_jsons(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_jsons(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from json string.\n    \"\"\"\n\n    return cls(Config.from_jsons(string, *args, **kwargs))\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.from_yaml","title":"<code>from_yaml(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml file.</p> <p>This function calls <code>self.from_yamls()</code> to construct object from yaml string. You may overwrite <code>from_yamls</code> in case something is not yaml serializable.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_yaml(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from yaml file.\n\n    This function calls `self.from_yamls()` to construct object from yaml string.\n    You may overwrite `from_yamls` in case something is not yaml serializable.\n    \"\"\"\n\n    with FlatDict.open(file) as fp:\n        return cls.from_yamls(fp.read(), *args, **kwargs)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.from_yamls","title":"<code>from_yamls(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_yamls(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from yaml string.\n    \"\"\"\n\n    return cls(Config.from_yamls(string, *args, **kwargs))\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.init_deepspeed","title":"<code>init_deepspeed(config=None)</code>","text":"<p>Preprocess DeepSpeed config.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_deepspeed(  # pylint: disable=too-many-branches, too-many-statements\n    self, config: Dict = None  # type: ignore\n) -&gt; Dict:\n    r\"\"\"\n    Preprocess DeepSpeed config.\n    \"\"\"\n\n    if config is None:\n        config = self.state.get(\"deepspeed\")\n    if config is None:\n        return {}\n    if isinstance(config, str):\n        config = NestedDict.load(config)\n    if config.get(\"steps_per_print\", \"auto\") == \"auto\":\n        config[\"steps_per_print\"] = self.print_interval\n    if config.get(\"train_micro_batch_size_per_gpu\", \"auto\") == \"auto\":\n        config[\"train_micro_batch_size_per_gpu\"] = self.batch_size\n    if \"amp\" in config:\n        amp = config[\"amp\"]\n        if amp.get(\"enabled\", \"auto\") == \"auto\":\n            amp[\"enabled\"] = \"true\"\n        if amp.get(\"opt_level\", \"auto\") == \"auto\":\n            amp[\"opt_level\"] = \"O1\"\n    if \"zero_optimization\" in config:\n        zero = config[\"zero_optimization\"]\n        if zero.get(\"allgather_bucket_size\") == \"auto\":\n            zero[\"allgather_bucket_size\"] = 1e6\n        if zero.get(\"reduce_bucket_size\") == \"auto\":\n            zero[\"reduce_bucket_size\"] = 1e6\n        if zero.get(\"stage3_max_live_parameters\") == \"auto\":\n            zero[\"stage3_max_live_parameters\"] = 1e8\n        if zero.get(\"stage3_max_live_gradients\") == \"auto\":\n            zero[\"stage3_max_live_gradients\"] = 1e8\n        if zero.get(\"stage3_max_reuse_distance\") == \"auto\":\n            zero[\"stage3_max_reuse_distance\"] = 1e8\n        if zero.get(\"stage3_prefetch_bucket_size\") == \"auto\":\n            zero[\"stage3_prefetch_bucket_size\"] = 1e6\n        if zero.get(\"stage3_param_persistence_threshold\") == \"auto\":\n            zero[\"stage3_param_persistence_threshold\"] = 1e8\n        if \"amp\" in config:\n            if \"fp16\" not in config:\n                config[\"fp16\"] = {}\n            if config[\"fp16\"].get(\"enabled\", \"auto\"):\n                config[\"fp16\"][\"enabled\"] = config[\"amp\"][\"enabled\"]\n            warn(\n                f\"AMP is not compatible with ZeRO. Automatically set 'fp16' to {config['amp']['enabled']}\",\n                stacklevel=2,\n            )\n            del config[\"amp\"]\n    if \"optimizer\" in config:\n        if \"params\" not in config[\"optimizer\"]:\n            config[\"optimizer\"][\"params\"] = {}\n        optimizer = config[\"optimizer\"][\"params\"]\n        if optimizer.get(\"lr\", \"auto\") == \"auto\":\n            optimizer[\"lr\"] = self.state.get(\"optim.lr\", 1e-3)\n        if optimizer.get(\"weight_decay\", \"auto\") == \"auto\":\n            optimizer[\"weight_decay\"] = self.state.get(\"optim.weight_decay\", 1e-2)\n        if optimizer.get(\"betas\") == \"auto\":\n            optimizer[\"betas\"] = (0.9, 0.999)\n        if optimizer.get(\"eps\") == \"auto\":\n            optimizer[\"eps\"] = 1e-8\n    if \"scheduler\" in config:\n        if \"params\" not in config[\"scheduler\"]:\n            config[\"scheduler\"][\"params\"] = {}\n        scheduler = config[\"scheduler\"][\"params\"]\n        if scheduler.get(\"total_num_steps\", \"auto\") == \"auto\":\n            scheduler[\"total_num_steps\"] = self.total_steps\n        if scheduler.get(\"warmup_num_steps\", \"auto\") == \"auto\":\n            scheduler[\"warmup_num_steps\"] = scheduler[\"total_num_steps\"] // 20\n        if scheduler.get(\"warmup_max_lr\", \"auto\") == \"auto\":\n            if self.optimizer:\n                scheduler[\"warmup_max_lr\"] = self.optimizer.param_groups[0][\"lr\"]\n            elif \"optimizer\" in config:\n                scheduler[\"warmup_max_lr\"] = config[\"optimizer\"][\"params\"][\"lr\"]\n            else:\n                raise ValueError(\"warmup_max_lr is not defined and cannot be inferred\")\n        if scheduler.get(\"warmup_min_lr\", \"auto\") == \"auto\":\n            scheduler[\"warmup_min_lr\"] = 1e-7\n    return config\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.init_distributed","title":"<code>init_distributed()</code>","text":"<p>Initialise distributed running environment.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_distributed(self) -&gt; None:\n    r\"\"\"\n    Initialise distributed running environment.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.init_logging","title":"<code>init_logging()</code>","text":"<p>Set up logging.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@on_main_process\ndef init_logging(self) -&gt; None:\n    r\"\"\"\n    Set up logging.\n    \"\"\"\n\n    os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n    # Why is setting up proper logging so !@?#! ugly?\n    logging.config.dictConfig(\n        {\n            \"version\": 1,\n            \"disable_existing_loggers\": False,\n            \"formatters\": {\n                \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n            },\n            \"handlers\": {\n                \"stdout\": {\n                    \"level\": \"INFO\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.StreamHandler\",\n                    \"stream\": \"ext://sys.stdout\",\n                },\n                \"logfile\": {\n                    \"level\": \"DEBUG\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.FileHandler\",\n                    \"filename\": self.log_path,\n                    \"mode\": \"a\",\n                },\n            },\n            \"loggers\": {\n                \"\": {\n                    \"handlers\": [\"stdout\", \"logfile\"],\n                    \"level\": \"DEBUG\",\n                    \"propagate\": True,\n                },\n            },\n        }\n    )\n    logging.captureWarnings(True)\n    self.logger = logging.getLogger(\"runner\")\n    self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.init_print","title":"<code>init_print(process=0)</code>","text":"<p>Set up <code>print</code>.</p> <p>Only print on a specific <code>process</code> or when <code>force = True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>process</code> <code>int</code> <p>The process to <code>print</code> on.</p> <code>0</code>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.init_print--notes","title":"Notes","text":"<p>If <code>self.state.log = True</code>, the default <code>print</code> function will be override by <code>logging.info</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_print(self, process: int = 0) -&gt; None:\n    r\"\"\"\n    Set up `print`.\n\n    Only print on a specific `process` or when `force = True`.\n\n    Args:\n        process: The process to `print` on.\n\n    Notes\n    -----\n    If `self.state.log = True`, the default `print` function will be override by `logging.info`.\n    \"\"\"\n\n    logger = logging.getLogger(\"print\")\n    logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n    import builtins as __builtin__  # pylint: disable=C0415\n\n    builtin_print = __builtin__.print\n\n    @catch\n    def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=redefined-builtin\n        if self.rank == process or force:\n            if self.state.log:\n                logger.info(*args, **kwargs)\n            else:\n                builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n    __builtin__.print = print\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.init_tensorboard","title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.json","title":"<code>json(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner State to json file.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\ndef json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n    r\"\"\"\n    Dump Runner State to json file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        return self.state.json(file, *args, **kwargs)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.jsons","title":"<code>jsons(*args, **kwargs)</code>","text":"<p>Dump Runner State to json string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def jsons(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner State to json string.\n    \"\"\"\n\n    return self.state.jsons(*args, **kwargs)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.load","title":"<code>load(file, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load any file with supported extensions.</p> <p><code>Runner.load</code> is identical to <code>dl.load</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@staticmethod\ndef load(file: PathStr, *args, **kwargs) -&gt; Any:\n    r\"\"\"\n    Load any file with supported extensions.\n\n    `Runner.load` is identical to `dl.load`.\n    \"\"\"\n\n    return load(file, *args, **kwargs)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.load_checkpoint","title":"<code>load_checkpoint(checkpoint=None, auto_resume=None, override_state=False, *args, **kwargs)</code>","text":"<p>Load info from checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Mapping | bytes | str | PathLike | None</code> <p>Checkpoint (or its path) to load. Defaults to <code>self.state.checkpoint</code>.</p> <code>None</code> <code>auto_resume</code> <code>bool | None</code> <p>Automatically resume from latest checkpoint if exists. Defaults to <code>False</code>. If is <code>True</code> and <code>checkpoint</code> is None, will set it to <code>self.checkpoint_dir/latest.pth</code>.</p> <code>None</code> <code>override_state</code> <code>bool</code> <p>If True, override runner state with checkpoint state. Defaults to <code>False</code>.</p> <code>False</code> <code>*args</code> <p>Additional arguments to pass to <code>self.load</code>.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>self.load</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>checkpoint</code> does not exists.</p> See Also <p><code>from_checkpoint</code>: Build runner from checkpoint. <code>load_pretrained</code>: Load parameters from pretrained checkpoint.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def load_checkpoint(\n    self,\n    checkpoint: Mapping | bytes | str | os.PathLike | None = None,\n    auto_resume: bool | None = None,\n    override_state: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Load info from checkpoint.\n\n    Args:\n        checkpoint: Checkpoint (or its path) to load.\n            Defaults to `self.state.checkpoint`.\n        auto_resume: Automatically resume from latest checkpoint if exists.\n            Defaults to `False`.\n            If is `True` and `checkpoint` is None, will set it to `self.checkpoint_dir/latest.pth`.\n        override_state: If True, override runner state with checkpoint state.\n            Defaults to `False`.\n        *args: Additional arguments to pass to `self.load`.\n        **kwargs: Additional keyword arguments to pass to `self.load`.\n\n    Raises:\n        FileNotFoundError: If `checkpoint` does not exists.\n\n    See Also:\n        [`from_checkpoint`][danling.BaseRunner.from_checkpoint]: Build runner from checkpoint.\n        [`load_pretrained`][danling.BaseRunner.load_pretrained]: Load parameters from pretrained checkpoint.\n    \"\"\"\n\n    checkpoint = checkpoint if checkpoint is not None else self.state.get(\"checkpoint\")\n    auto_resume = auto_resume if auto_resume is not None else self.state.get(\"auto_resume\", False)\n\n    # TODO: Support loading checkpoints in other format\n    if checkpoint is not None:\n        if auto_resume:\n            warn(\n                \"latest checkpoint is preempted by value specified in checkpoint\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        if isinstance(checkpoint, (bytes, str, os.PathLike)):\n            if not os.path.exists(checkpoint):\n                raise FileNotFoundError(f\"checkpoint is set to {checkpoint!r} but does not exist.\")\n            self.state.checkpoint = checkpoint\n            ckpt = self.load(checkpoint, *args, **kwargs)\n        elif isinstance(checkpoint, Mapping):\n            ckpt = checkpoint\n        else:\n            raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint.\")\n    elif auto_resume:\n        checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        if os.path.exists(checkpoint):\n            self.state.checkpoint = checkpoint\n            ckpt = self.load(checkpoint, *args, **kwargs)\n        else:\n            warn(\"latest checkpoint does not exits\", category=RuntimeWarning, stacklevel=2)\n            return\n    else:\n        raise ValueError(\"checkpoint is not specified and auto_resume is not set to True\")\n\n    # TODO: Wrap state_dict in a dataclass\n    self.state.merge(ckpt[\"runner\"], overwrite=override_state)\n    if self.model is not None and \"model\" in ckpt:\n        model = self.unwrap_model(self.model)\n        model.load_state_dict(ckpt[\"model\"])\n    if self.optimizer is not None and \"optimizer\" in ckpt:\n        self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n    if self.scheduler is not None and \"scheduler\" in ckpt:\n        self.scheduler.load_state_dict(ckpt[\"scheduler\"])\n    self.state.iter_begin = self.state.iters\n    self.state.step_begin = self.state.steps\n    self.state.epoch_begin = self.state.epochs\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.load_pretrained","title":"<code>load_pretrained(checkpoint=None, *args, **kwargs)</code>","text":"<p>Load parameters from pretrained checkpoint.</p> <p>This method only loads the model weights.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Mapping | bytes | str | PathLike | None</code> <p>Pretrained checkpoint (or its path) to load. Defaults to <code>self.state.pretrained</code>.</p> <code>None</code> <code>*args</code> <p>Additional arguments to pass to <code>self.load</code>.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>self.load</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>checkpoint</code> does not exists.</p> See Also <p><code>load_checkpoint</code>: Load info from checkpoint.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def load_pretrained(self, checkpoint: Mapping | bytes | str | os.PathLike | None = None, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Load parameters from pretrained checkpoint.\n\n    This method only loads the model weights.\n\n    Args:\n        checkpoint: Pretrained checkpoint (or its path) to load.\n            Defaults to `self.state.pretrained`.\n        *args: Additional arguments to pass to `self.load`.\n        **kwargs: Additional keyword arguments to pass to `self.load`.\n\n    Raises:\n        FileNotFoundError: If `checkpoint` does not exists.\n\n    See Also:\n        [`load_checkpoint`][danling.BaseRunner.load_checkpoint]: Load info from checkpoint.\n    \"\"\"\n\n    # TODO: Support loading checkpoints in other format\n    checkpoint = checkpoint if checkpoint is not None else self.state.get(\"pretrained\")\n    if checkpoint is None:\n        raise ValueError(\"pretrained is not specified\")\n    if isinstance(checkpoint, (bytes, str, os.PathLike)):\n        if not os.path.exists(checkpoint):\n            raise FileNotFoundError(f\"pretrained is set to {checkpoint!r} but does not exist.\")\n        ckpt = self.load(checkpoint, *args, **kwargs)\n    elif isinstance(checkpoint, Mapping):\n        ckpt = checkpoint\n    else:\n        raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint.\")\n    ckpt = ckpt.get(\"model\", ckpt)\n    ckpt = ckpt.get(\"state_dict\", ckpt)\n    model = self.unwrap_model(self.model)\n    model.load_state_dict(ckpt)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.print_result","title":"<code>print_result()</code>","text":"<p>Print latest and best result.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def print_result(self) -&gt; None:\n    r\"\"\"\n    Print latest and best result.\n    \"\"\"\n\n    print(f\"results: {self.state.results}\")\n    print(f\"latest result: {self.latest_result}\")\n    print(f\"best result: {self.best_result}\")\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.save","title":"<code>save(obj, file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Save any file with supported extensions.</p> <p><code>Runner.save</code> internally calls <code>dl.save</code>, but with additional arguments to allow it save only on the main process. Moreover, any error raised by <code>Runner.save</code> will be caught and logged.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, file: PathStr, main_process_only: bool = True, *args, **kwargs) -&gt; File:\n    r\"\"\"\n    Save any file with supported extensions.\n\n    `Runner.save` internally calls `dl.save`,\n    but with additional arguments to allow it save only on the main process.\n    Moreover, any error raised by `Runner.save` will be caught and logged.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        return save(obj, file, *args, **kwargs)\n    return file\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.save_checkpoint","title":"<code>save_checkpoint()</code>","text":"<p>Save checkpoint to <code>self.checkpoint_dir</code>.</p> <p>The checkpoint will be saved to <code>self.checkpoint_dir/latest.pth</code>.</p> <p>If <code>self.state.save_interval</code> is positive and <code>self.state.epochs + 1</code> is a multiple of <code>save_interval</code>, the checkpoint will also be copied to <code>self.checkpoint_dir/epoch-{self.state.epochs}.pth</code>.</p> <p>If <code>self.is_best</code> is <code>True</code>, the checkpoint will also be copied to <code>self.checkpoint_dir/best.pth</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_checkpoint(self) -&gt; None:\n    r\"\"\"\n    Save checkpoint to `self.checkpoint_dir`.\n\n    The checkpoint will be saved to `self.checkpoint_dir/latest.pth`.\n\n    If `self.state.save_interval` is positive and `self.state.epochs + 1` is a multiple of `save_interval`,\n    the checkpoint will also be copied to `self.checkpoint_dir/epoch-{self.state.epochs}.pth`.\n\n    If `self.is_best` is `True`, the checkpoint will also be copied to `self.checkpoint_dir/best.pth`.\n    \"\"\"\n\n    latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n    self.save(self.state_dict(), latest_path)\n    if (\n        hasattr(self, \"save_interval\")\n        and self.save_interval &gt; 0\n        and (self.state.epochs + 1) % self.save_interval == 0\n    ):\n        save_path = os.path.join(self.checkpoint_dir, f\"epoch-{self.state.epochs}.pth\")\n        shutil.copy(latest_path, save_path)\n    if self.is_best:\n        best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n        shutil.copy(latest_path, best_path)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.save_result","title":"<code>save_result()</code>","text":"<p>Save result to <code>self.dir</code>.</p> <p>This method will save latest and best result to <code>self.dir/latest.json</code> and <code>self.dir/best.json</code> respectively.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_result(self) -&gt; None:\n    r\"\"\"\n    Save result to `self.dir`.\n\n    This method will save latest and best result to\n    `self.dir/latest.json` and `self.dir/best.json` respectively.\n    \"\"\"\n\n    results_path = os.path.join(self.dir, \"results.json\")\n    self.save(\n        {\n            \"id\": self.state.id,\n            \"name\": self.state.name,\n            \"results\": self.state.results,\n        },\n        results_path,\n        indent=4,\n    )\n    ret = {\"id\": self.state.id, \"name\": self.state.name}\n    result = self.latest_result\n    if isinstance(result, FlatDict):\n        result = result.dict()\n    # This is slower but ensure id is the first key\n    if result is not None:\n        ret.update(result)\n    latest_path = os.path.join(self.dir, \"latest.json\")\n    self.save(ret, latest_path, indent=4)\n    if self.is_best:\n        best_path = os.path.join(self.dir, \"best.json\")\n        shutil.copy(latest_path, best_path)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.scale_lr","title":"<code>scale_lr(lr, lr_scale_factor=None, batch_size_base=None)</code>","text":"<p>Scale learning rate according to linear scaling rule.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def scale_lr(\n    self,\n    lr: float,\n    lr_scale_factor: float | None = None,\n    batch_size_base: int | None = None,\n) -&gt; float:\n    r\"\"\"\n    Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n    \"\"\"\n\n    if lr_scale_factor in self.state:\n        lr_scale_factor = self.state.lr_scale_factor\n\n    if lr_scale_factor is None:\n        if batch_size_base is None:\n            batch_size_base = getattr(self, \"batch_size_base\", None)\n            if batch_size_base is None:\n                raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n        lr_scale_factor = self.batch_size_equivalent / batch_size_base\n    elif batch_size_base is not None:\n        warn(\n            \"batch_size_base will be ignored if lr_scale_factor is specified\", category=RuntimeWarning, stacklevel=2\n        )\n    lr = lr * lr_scale_factor\n    self.state.lr_scale_factor = lr_scale_factor\n    return lr\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.set_deterministic","title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.set_seed","title":"<code>set_seed(seed=None, bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Random seed to set. Defaults to <code>self.state.seed</code> (<code>config.seed</code>).</p> <code>None</code> <code>bias</code> <code>int | None</code> <p>Make the seed different for each processes.</p> <p>This avoids same data augmentation are applied on every processes.</p> <p>Defaults to <code>self.rank</code>.</p> <p>Set to <code>False</code> to disable this feature.</p> <code>None</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Args:\n        seed: Random seed to set.\n            Defaults to `self.state.seed` (`config.seed`).\n\n        bias: Make the seed different for each processes.\n\n            This avoids same data augmentation are applied on every processes.\n\n            Defaults to `self.rank`.\n\n            Set to `False` to disable this feature.\n    \"\"\"\n\n    seed = seed or self.state.seed\n    bias = bias or self.rank\n    if bias:\n        seed += bias\n    if np_random is not None:\n        np_random.seed(seed)\n    random.seed(seed)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.state_dict","title":"<code>state_dict(cls=dict)</code>","text":"<p>Return dict of all attributes for checkpoint.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Return dict of all attributes for checkpoint.\n    \"\"\"\n\n    return cls(self.state)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.step","title":"<code>step(loss, batch_size=None, zero_grad=True)</code>","text":"<p>Backward loss and step optimizer &amp; scheduler.</p> <p>This method increment <code>self.state.steps</code>.</p> <p>This method also increment <code>self.state.iters</code> when <code>batch_size</code> is specified.</p> <p>Parameters:</p> Name Type Description Default <code>zero_grad</code> <code>bool</code> <p>Whether to zero the gradients.</p> <code>True</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n    r\"\"\"\n    Backward loss and step optimizer &amp; scheduler.\n\n    This method increment `self.state.steps`.\n\n    This method also increment `self.state.iters` when `batch_size` is specified.\n\n    Args:\n        zero_grad: Whether to zero the gradients.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.yaml","title":"<code>yaml(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner State to yaml file.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\ndef yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n    r\"\"\"\n    Dump Runner State to yaml file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        return self.state.yaml(file, *args, **kwargs)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.yamls","title":"<code>yamls(*args, **kwargs)</code>","text":"<p>Dump Runner State to yaml string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def yamls(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner State to yaml string.\n    \"\"\"\n\n    return self.state.yamls(*args, **kwargs)\n</code></pre>"},{"location":"runner/runner_state/","title":"RunnerState","text":"<p>             Bases: <code>NestedDict</code></p> <p><code>RunnerState</code> is a <code>NestedDict</code> that contains all states of a <code>Runner</code>.</p> <p><code>RunnerState</code> is designed to store all critical information of a Run so that you can resume a run from a state and corresponding weights or even restart a run from a state.</p> <p><code>RunnerState</code> is also designed to be serialisable and hashable, so that you can save it to a file. <code>RunnerState</code> is saved in checkpoint together with weights by default.</p> <p>Since <code>RunnerState</code> is a <code>NestedDict</code>, you can access its attributes by <code>state[\"key\"]</code> or <code>state.key</code>.</p> <p>General:</p> Name Type Description <code>id</code> <code>str</code> <p><code>f\"{time_str}{self.experiment_id:.5}{self.run_id:.4}\"</code>.</p> <code>uuid</code> <code>(UUID, property)</code> <p><code>uuid5(self.run_id, self.id)</code>.</p> <code>name</code> <code>str</code> <p><code>f\"{self.experiment_name}-{self.run_name}\"</code>.</p> <code>run_id</code> <code>str</code> <p>hex of <code>self.run_uuid</code>.</p> <code>run_uuid</code> <code>(UUID, property)</code> <p><code>uuid5(self.experiment_id, config.jsons())</code>.</p> <code>run_name</code> <code>str</code> <p>Defaults to <code>\"DanLing\"</code>.</p> <code>experiment_id</code> <code>str</code> <p>git hash of the current HEAD. Defaults to <code>\"xxxxxxxxxxxxxxxx\"</code> if Runner not under a git repo or git/gitpython not installed.</p> <code>experiment_uuid</code> <code>(UUID, property)</code> <p>UUID of <code>self.experiment_id</code>. Defaults to <code>UUID('78787878-7878-7878-7878-787878787878')</code> if Runner not under a git repo or git/gitpython not installed.</p> <code>experiment_name</code> <code>str</code> <p>Defaults to <code>\"DanLing\"</code>.</p> <code>seed</code> <code>int</code> <p>Defaults to <code>randint(0, 2**32 - 1)</code>.</p> <code>deterministic</code> <code>bool</code> <p>Ensure deterministic operations. Defaults to <code>False</code>.</p> <p>Progress:</p> Name Type Description <code>iters</code> <code>int</code> <p>The number of data samples processed. equals to <code>steps</code> when <code>batch_size = 1</code>.</p> <code>steps</code> <code>int</code> <p>The number of <code>step</code> calls.</p> <code>epochs</code> <code>int</code> <p>The number of complete passes over the datasets.</p> <code>iter_end</code> <code>int</code> <p>End running iters. Note that <code>step_end</code> not initialised since this variable may not apply to some Runners.</p> <code>step_end</code> <code>int</code> <p>End running steps. Note that <code>step_end</code> not initialised since this variable may not apply to some Runners.</p> <code>epoch_end</code> <code>int</code> <p>End running epochs. Note that <code>epoch_end</code> not initialised since this variable may not apply to some Runners.</p> <p>In general you should only use one of <code>iter_end</code>, <code>step_end</code>, <code>epoch_end</code> to indicate the length of running.</p> <p>Results:</p> Name Type Description <code>results</code> <code>dict</code> <p>All results, should be in the form of <code>{step: {subset: {score_name: score}}}</code>.</p> <p><code>results</code> should be a list of <code>result</code>. <code>result</code> should be a dict with the same <code>split</code> as keys, like <code>dataloaders</code>. A typical <code>result</code> might look like this: Python<pre><code>{\n    \"train\": {\n        \"loss\": 0.1,\n        \"accuracy\": 0.9,\n    },\n    \"val\": {\n        \"loss\": 0.2,\n        \"accuracy\": 0.8,\n    },\n    \"test\": {\n        \"loss\": 0.3,\n        \"accuracy\": 0.7,\n    },\n}\n</code></pre></p> <p><code>scores</code> are dynamically extracted from <code>results</code> by <code>score_set</code> and <code>score_name</code>. They represent the core metric that is used in comparing the performance against different models and settings. For the above <code>results</code>, If <code>score_set = \"val\"</code>, <code>score_name = \"accuracy\"</code>, then <code>scores = 0.9</code>.</p> <p>IO:</p> Name Type Description <code>project_root</code> <code>str</code> <p>The root directory for all experiments. Defaults to <code>\"experiments\"</code>.</p> <p><code>project_root</code> is the root directory of all Experiments, and should be consistent across the Project.</p> <p><code>dir</code> is the directory of a certain Run.</p> <p>There is no attributes/properties for Group and Experiment.</p> <p><code>checkpoint_dir_name</code> is relative to <code>dir</code>, and is passed to generate <code>checkpoint_dir</code> (<code>checkpoint_dir = os.path.join(dir, checkpoint_dir_name)</code>). In practice, <code>checkpoint_dir_name</code> is rarely called.</p> <p>logging:</p> Name Type Description <code>log</code> <code>bool</code> <p>Whether to log the outputs. Defaults to <code>True</code>.</p> <code>tensorboard</code> <code>bool</code> <p>Whether to use <code>tensorboard</code>. Defaults to <code>False</code>.</p> <code>print_interval</code> <code>int</code> <p>Interval of printing logs. Defaults to -1.</p> <code>save_interval</code> <code>int</code> <p>Interval of saving intermediate checkpoints. Defaults to -1, never save intermediate checkpoints.</p> Notes <p><code>RunnerState</code> is a <code>NestedDict</code>, so you can access its attributes by <code>state[\"name\"]</code> or <code>state.name</code>.</p> See Also <p><code>BaseRunner</code>: The base runner class.</p> Source code in <code>danling/runner/runner_state.py</code> Python<pre><code>class RunnerState(NestedDict):  # pylint: disable=too-many-instance-attributes\n    r\"\"\"\n    `RunnerState` is a `NestedDict` that contains all states of a `Runner`.\n\n    `RunnerState` is designed to store all critical information of a Run so that you can resume a run\n    from a state and corresponding weights or even restart a run from a state.\n\n    `RunnerState` is also designed to be serialisable and hashable, so that you can save it to a file.\n    `RunnerState` is saved in checkpoint together with weights by default.\n\n    Since `RunnerState` is a `NestedDict`, you can access its attributes by `state[\"key\"]` or `state.key`.\n\n    Attributes: General:\n        id (str): `f\"{time_str}{self.experiment_id:.5}{self.run_id:.4}\"`.\n        uuid (UUID, property): `uuid5(self.run_id, self.id)`.\n        name (str): `f\"{self.experiment_name}-{self.run_name}\"`.\n        run_id (str): hex of `self.run_uuid`.\n        run_uuid (UUID, property): `uuid5(self.experiment_id, config.jsons())`.\n        run_name (str): Defaults to `\"DanLing\"`.\n        experiment_id (str): git hash of the current HEAD.\n            Defaults to `\"xxxxxxxxxxxxxxxx\"` if Runner not under a git repo or git/gitpython not installed.\n        experiment_uuid (UUID, property): UUID of `self.experiment_id`.\n            Defaults to `UUID('78787878-7878-7878-7878-787878787878')`\n            if Runner not under a git repo or git/gitpython not installed.\n        experiment_name (str): Defaults to `\"DanLing\"`.\n        seed (int): Defaults to `randint(0, 2**32 - 1)`.\n        deterministic (bool): Ensure [deterministic](https://pytorch.org/docs/stable/notes/randomness.html) operations.\n            Defaults to `False`.\n\n    Attributes: Progress:\n        iters (int): The number of data samples processed.\n            equals to `steps` when `batch_size = 1`.\n        steps (int): The number of `step` calls.\n        epochs (int): The number of complete passes over the datasets.\n        iter_end (int): End running iters.\n            Note that `step_end` not initialised since this variable may not apply to some Runners.\n        step_end (int): End running steps.\n            Note that `step_end` not initialised since this variable may not apply to some Runners.\n        epoch_end (int): End running epochs.\n            Note that `epoch_end` not initialised since this variable may not apply to some Runners.\n\n    In general you should only use one of `iter_end`, `step_end`, `epoch_end` to indicate the length of running.\n\n    Attributes: Results:\n        results (dict): All results, should be in the form of `{step: {subset: {score_name: score}}}`.\n\n    `results` should be a list of `result`.\n    `result` should be a dict with the same `split` as keys, like `dataloaders`.\n    A typical `result` might look like this:\n    ```python\n    {\n        \"train\": {\n            \"loss\": 0.1,\n            \"accuracy\": 0.9,\n        },\n        \"val\": {\n            \"loss\": 0.2,\n            \"accuracy\": 0.8,\n        },\n        \"test\": {\n            \"loss\": 0.3,\n            \"accuracy\": 0.7,\n        },\n    }\n    ```\n\n    `scores` are dynamically extracted from `results` by `score_set` and `score_name`.\n    They represent the core metric that is used in comparing the performance against different models and settings.\n    For the above `results`, If `score_set = \"val\"`, `score_name = \"accuracy\"`, then `scores = 0.9`.\n\n    Attributes: IO:\n        project_root (str): The root directory for all experiments.\n            Defaults to `\"experiments\"`.\n\n    `project_root` is the root directory of all **Experiments**, and should be consistent across the **Project**.\n\n    `dir` is the directory of a certain **Run**.\n\n    There is no attributes/properties for **Group** and **Experiment**.\n\n    `checkpoint_dir_name` is relative to `dir`, and is passed to generate `checkpoint_dir`\n    (`checkpoint_dir = os.path.join(dir, checkpoint_dir_name)`).\n    In practice, `checkpoint_dir_name` is rarely called.\n\n    Attributes: logging:\n        log (bool): Whether to log the outputs.\n            Defaults to `True`.\n        tensorboard (bool): Whether to use `tensorboard`.\n            Defaults to `False`.\n        print_interval (int): Interval of printing logs.\n            Defaults to -1.\n        save_interval (int): Interval of saving intermediate checkpoints.\n            Defaults to -1, never save intermediate checkpoints.\n\n    Notes:\n        `RunnerState` is a `NestedDict`, so you can access its attributes by `state[\"name\"]` or `state.name`.\n\n    See Also:\n        [`BaseRunner`][danling.runner.BaseRunner]: The base runner class.\n    \"\"\"\n\n    # DO NOT set default value in class, as they won't be stored in `__dict__`.\n\n    id: str\n    name: str\n    run_id: str\n    run_name: str\n    experiment_id: str\n    experiment_name: str\n\n    seed: int\n    deterministic: bool = False\n\n    iters: int = 0\n    steps: int = 0\n    epochs: int = 0\n    iter_begin: int = 0\n    step_begin: int = 0\n    epoch_begin: int = 0\n    iter_end: Optional[int] = None\n    step_end: Optional[int] = None\n    epoch_end: Optional[int] = None\n\n    results: dict\n    score_set: Optional[str] = None\n    score_name: str = \"loss\"\n\n    project_root: str = \"experiments\"\n    checkpoint_dir_name: str = \"checkpoints\"\n    log: bool = True\n    tensorboard: bool = False\n    print_interval: int = -1\n    save_interval: int = -1\n\n    distributed: Optional[bool] = None\n    dist_backend: Optional[str] = None\n    init_method: Optional[str] = None\n    master_addr: Optional[str] = None\n    master_port: Optional[int] = None\n\n    def __init__(self, *args, **kwargs):\n        for k, v in self.__class__.__dict__.items():\n            if not (k.startswith(\"__\") and k.endswith(\"__\")) and (not (isinstance(v, property) or callable(v))):\n                self.set(k, v)\n        self.run_name = defaults.DEFAULT_RUN_NAME\n        self.experiment_name = defaults.DEFAULT_EXPERIMENT_NAME\n        self.seed = randint(0, 2**32 - 1)\n        self.results = NestedDict()\n        super().__init__(*args, **kwargs)\n        self.experiment_id = self.get_experiment_id()\n        self.run_id = self.run_uuid.hex\n        self.id = f\"{self.get_time_str()}{self.experiment_id:.5}{self.run_id:.4}\"\n        self.name = f\"{self.experiment_name}-{self.run_name}\"\n        self.setattr(\"ignored_keys_in_hash\", defaults.DEFAULT_IGNORED_KEYS_IN_HASH)\n\n    # staticmethod is not recognised by `callable` in earlier python\n    def get_experiment_id(self) -&gt; str:\n        if Repo is not None:\n            try:\n                return Repo(search_parent_directories=True).head.object.hexsha\n            except ImportError:\n                pass  # handle at last\n            except (InvalidGitRepositoryError, ValueError):\n                warn(\n                    \"Unable to get git hash from CWD, fallback to top-level code environment.\",\n                    category=RuntimeWarning,\n                    stacklevel=2,\n                )\n                path = os.path.dirname(os.path.abspath(sys.argv[0]))\n                with suppress(InvalidGitRepositoryError, ValueError):\n                    return Repo(path=path, search_parent_directories=True).head.object.hexsha\n        warn(\n            \"GitPython is not installed, fallback to `DEFAULT_EXPERIMENT_ID`.\",\n            category=RuntimeWarning,\n            stacklevel=2,\n        )\n        return defaults.DEFAULT_EXPERIMENT_ID\n\n    def get_time_str(self) -&gt; str:\n        time = datetime.now()\n        time_tuple = time.isocalendar()[1:] + (\n            time.hour,\n            time.minute,\n            time.second,\n            time.microsecond,\n        )\n        return \"\".join(base62.encode(i) for i in time_tuple)\n\n    @property\n    def experiment_uuid(self) -&gt; UUID:\n        r\"\"\"\n        UUID of the experiment.\n        \"\"\"\n\n        return UUID(bytes=bytes(self.experiment_id.ljust(16, \"x\")[:16], encoding=\"ascii\"))\n\n    @property\n    def run_uuid(self) -&gt; UUID:\n        r\"\"\"\n        UUID of the run.\n        \"\"\"\n\n        return uuid5(self.experiment_uuid, str(hash(self)))\n\n    @property\n    def uuid(self) -&gt; UUID:\n        r\"\"\"\n        UUID of the state.\n        \"\"\"\n\n        return uuid5(self.run_uuid, self.id)\n\n    def __hash__(self) -&gt; int:\n        ignored_keys_in_hash = self.getattr(\"ignored_keys_in_hash\", defaults.DEFAULT_IGNORED_KEYS_IN_HASH)\n        state: NestedDict = NestedDict({k: v for k, v in self.dict().items() if k not in ignored_keys_in_hash})\n        return hash(state.yamls())\n</code></pre>"},{"location":"runner/runner_state/#danling.runner.runner_state.RunnerState.experiment_uuid","title":"<code>experiment_uuid: UUID</code>  <code>property</code>","text":"<p>UUID of the experiment.</p>"},{"location":"runner/runner_state/#danling.runner.runner_state.RunnerState.run_uuid","title":"<code>run_uuid: UUID</code>  <code>property</code>","text":"<p>UUID of the run.</p>"},{"location":"runner/runner_state/#danling.runner.runner_state.RunnerState.uuid","title":"<code>uuid: UUID</code>  <code>property</code>","text":"<p>UUID of the state.</p>"},{"location":"runner/torch_runner/","title":"TorchRunner","text":"<p>             Bases: <code>BaseRunner</code></p> <p>Set up everything for running a job.</p> <p><code>TorchRunner</code> uses <code>accelerate</code> as distributed backend to provide seamless distributed training experience.</p> <p><code>TorchRunner</code> will automatically <code>prepare</code> everything, including <code>model</code>, <code>criterion</code>, <code>optimizer</code>, <code>scheduler</code>, and <code>dataloaders</code> for distribute training, mixed precision, and deepspeed (optional).</p> <p>In fact, you don\u2019t even need to create <code>dataloaders</code>, just define <code>datasets</code> and <code>TorchRunner</code> will create <code>dataloaders</code> for you. <code>TorchRunner</code> will inspect the <code>train</code> flag in corresponding dataset to automatically set <code>shuffle</code>.</p> <p>Attributes:</p> Name Type Description <code>accelerator</code> <code>Accelerator</code> <code>accelerate</code> <code>dict</code> <p>Arguments to pass when building accelerator. Defaults to <code>{}</code>.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>class TorchRunner(BaseRunner):  # pylint: disable=too-many-public-methods\n    r\"\"\"\n    Set up everything for running a job.\n\n    `TorchRunner` uses [`accelerate`][accelerate] as distributed backend to\n    provide seamless distributed training experience.\n\n    `TorchRunner` will automatically [`prepare`][accelerate.Accelerator.prepare] everything,\n    including `model`, `criterion`, `optimizer`, `scheduler`, and `dataloaders` for distribute training,\n    mixed precision, and deepspeed (optional).\n\n    In fact, you don't even need to create `dataloaders`, just define\n    `datasets` and `TorchRunner` will create `dataloaders` for you.\n    `TorchRunner` will inspect the `train` flag in corresponding dataset to\n    automatically set `shuffle`.\n\n    Attributes:\n        accelerator (Accelerator):\n        accelerate: Arguments to pass when building accelerator. Defaults to `{}`.\n    \"\"\"\n\n    accelerator: Accelerator\n    accelerate: dict\n\n    model: nn.Module\n    criterion: nn.Module\n    optimizer: optim.Optimizer\n    scheduler: optim.lr_scheduler._LRScheduler\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        if len(args) != 1 or kwargs:\n            message = (\n                \"Passing multiple args &amp; kwargs to build Runner is deprecated and will be removed in DanLing v0.3.\\n\"\n                \"Please only pass a config dict instead.\"\n            )\n            warn(message, DeprecationWarning, stacklevel=2)\n            config = NestedDict(*args, **kwargs)\n        else:\n            config = args[0]\n        if \"accelerate\" not in self:  # class attributes\n            self.accelerate = {}\n        self.accelerate.update(config.get(\"accelerate\", {}))\n        super().__init__(config)\n\n    def __post_init__(self, *args, **kwargs) -&gt; None:\n        self._prepare()\n\n    def _prepare(self):\n        if self.datasets:\n            datasets = {k: d for k, d in self.datasets.items() if k not in self.dataloaders}\n            dataloader_kwargs = self.state.get(\"dataloader\", {})\n            for k, d in datasets.items():\n                shuffle = dataloader_kwargs.shuffle if \"shuffle\" in dataloader_kwargs else getattr(d, \"train\", True)\n                self.dataloaders[k] = utils.data.DataLoader(d, shuffle=shuffle, **dataloader_kwargs)\n        objects = [self.model, self.criterion, self.optimizer, self.scheduler]\n        num_objects = len(objects)\n        dataloader_names = []\n        for name, dataloader in self.dataloaders.items():\n            dataloader_names.append(name)\n            objects.append(dataloader)\n        objects = self.prepare(*objects)\n        self.model, self.criterion, self.optimizer, self.scheduler = objects[:num_objects]\n        if len(objects) != len(dataloader_names) + num_objects:\n            raise ValueError(\"Number of dataloaders does not match.\")\n        for name, dataloader in zip(dataloader_names, objects[num_objects:]):\n            self.dataloaders[name] = dataloader\n\n    @property\n    def deepspeed(self) -&gt; dict | None:\n        if \"accelerator\" not in self:\n            raise ValueError(\"accelerator is not used\")\n        if self.accelerator.state.deepspeed_plugin is not None:\n            return self.accelerator.state.deepspeed_plugin.deepspeed_config\n        return None\n\n    def train(self, train_splits: list[str] | None = None, eval_splits: list[str] | None = None) -&gt; NestedDict:\n        r\"\"\"\n        Perform training on `split`.\n\n        Args:\n            train_splits (list[str]): list of split to run train.\n                Defaults to `[\"train\"]`.\n            eval_splits (list[str]): list of split to run evaluate.\n                Defaults to `self.dataloaders` except for those in `train_splits`.\n\n        Return:\n            NestedDict: train results\n        \"\"\"\n\n        early_stop_counter = 0\n        if train_splits is None:\n            train_splits = [\"train\"]\n        if eval_splits is None:\n            eval_splits = [s for s in self.dataloaders if s not in train_splits]\n        self.state.epoch_begin = self.state.epochs\n        print(f\"Begin training from {self.state.epoch_begin} to {self.state.epoch_end}\")\n        print(f\"Training splits: {train_splits}\")\n        print(f\"Evaluation splits: {eval_splits}\")\n        patience = self.state.get(\"patience\", float(\"inf\"))\n        for epochs in range(self.state.epoch_begin, self.state.epoch_end):  # type: ignore\n            self.state.epochs = epochs\n            result = NestedDict()\n            result.setattr(\"convert_mapping\", True)\n            for split in train_splits:\n                result[split] = self.train_epoch(split)\n            for split in eval_splits:\n                result[split] = self.evaluate_epoch(split)\n            self.append_result(result)\n            print(self.format_epoch_result(result))\n            self.save_result()\n            self.save_checkpoint()\n            \"\"\"@nni.report_intermediate_result(self.latest_score)\"\"\"\n            early_stop_counter = 0 if self.is_best else early_stop_counter + 1\n            if early_stop_counter &gt; patience:\n                print(\"early stop\")\n                break\n        \"\"\"@nni.report_final_result(self.latest_score)\"\"\"\n        return self.results\n\n    def train_epoch(self, split: str = \"train\") -&gt; NestedDict:\n        r\"\"\"\n        Train one epoch on `split`.\n\n        Args:\n            split (str): split to run train\n\n        Return:\n            NestedDict: train result\n        \"\"\"\n\n        self.mode = \"train\"  # type: ignore\n        loader = self.dataloaders[split]\n        length = len(loader) - 1\n        last_print_iteration = -1\n        self.meters.reset()\n        if self.metrics is not None:\n            self.metrics.reset()\n        batch_time = time()\n        if hasattr(loader.batch_sampler, \"set_epoch\"):\n            loader.batch_sampler.set_epoch(self.epochs)\n        if hasattr(loader.sampler, \"set_epoch\"):\n            loader.sampler.set_epoch(self.epochs)\n\n        for iteration, data in enumerate(loader):\n            with self.autocast(), self.accumulate():\n                input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n                target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n                pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n                loss = self.criterion(pred, target)\n                if self.metrics is not None:\n                    self.metrics.update(pred, target)\n                self.step(loss)\n\n            if self.print_interval &gt; 0 and (\n                iteration &gt; 0 and iteration % self.print_interval == 0 or iteration == length\n            ):\n                interval = iteration - last_print_iteration\n                if self.device == torch.device(\"cuda\"):\n                    torch.cuda.synchronize()\n                self.meters.time.update((time() - batch_time) / interval)\n                batch_time = time()\n                reduced_loss = self.reduce(loss).item()\n                self.meters.loss.update(reduced_loss)\n                self.step_log(split, iteration, length)\n                last_print_iteration = iteration\n\n        result = self.meters.avg\n        if self.metrics is not None:\n            result.merge(self.metrics.avg)\n        return result\n\n    def evaluate(self, eval_splits: list[str] | None = None) -&gt; NestedDict:\n        r\"\"\"\n        Perform evaluation on `eval_splits`.\n\n        Args:\n            eval_splits (list[str]): list of split to run evaluate.\n                Defaults to `[\"eval\"]`.\n\n        Return:\n            NestedDict: evaluation result\n        \"\"\"\n\n        if eval_splits is None:\n            eval_splits = [\"eval\"]\n\n        print(\"Begin evaluation\")\n        print(f\"Evaluation splits: {eval_splits}\")\n        result = NestedDict()\n        result.setattr(\"convert_mapping\", True)\n        for split in eval_splits:\n            result[split] = self.evaluate_epoch(split=split)\n        print(self.format_epoch_result(result))\n        return result\n\n    @torch.inference_mode()\n    def evaluate_epoch(self, split: str = \"val\") -&gt; NestedDict:\n        r\"\"\"\n        Evaluate one epoch on `split`.\n\n        Args:\n            split (str): split to run evaluate\n\n        Return:\n            NestedDict: evaluation result\n        \"\"\"\n\n        self.mode = \"eval\"  # type: ignore\n        loader = self.dataloaders[split]\n        length = len(loader) - 1\n        last_print_iteration = -1\n        self.meters.reset()\n        if self.metrics is not None:\n            self.metrics.reset()\n        batch_time = time()\n\n        for iteration, data in enumerate(loader):\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            loss = self.criterion(pred, target)\n            if self.metrics is not None:\n                self.metrics.update(pred, target)\n\n            if self.print_interval &gt; 0 and (\n                iteration &gt; 0 and iteration % self.print_interval == 0 or iteration == length\n            ):\n                interval = iteration - last_print_iteration\n                if self.device == torch.device(\"cuda\"):\n                    torch.cuda.synchronize()\n                self.meters.time.update((time() - batch_time) / interval)\n                batch_time = time()\n                reduced_loss = self.reduce(loss).item()\n                self.meters.loss.update(reduced_loss)\n                self.step_log(split, iteration, length)\n                last_print_iteration = iteration\n\n        result = self.meters.avg\n        if self.metrics is not None:\n            result.merge(self.metrics.avg)\n        self.write_result(result, split, self.state.epochs)\n        return result\n\n    @torch.inference_mode()\n    def inference(self, split: str = \"inf\") -&gt; list:\n        r\"\"\"\n        Perform inference on `split`.\n\n        Args:\n            split (str): split to run inference\n\n        Return:\n            Tensor: inference outputs\n        \"\"\"\n\n        self.mode = \"inf\"  # type: ignore\n        loader = self.dataloaders[split]\n        self.meters.reset()\n        output = []\n        for _, data in tqdm(enumerate(loader), total=len(loader)):\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            output.extend(pred.tolist())\n\n        if self.distributed:\n            torch.cuda.synchronize()\n            output = self.gather_for_metrics(output)\n        return output\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Set up distributed training.\n\n        Initialise process group and set up DDP variables.\n        \"\"\"\n\n        if os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\").lower() == \"true\":\n            deepspeed_config = self.state.get(\"deepspeed\", os.environ.get(\"ACCELERATE_DEEPSPEED_CONFIG_FILE\"))\n            self.accelerate[\"deepspeed_plugin\"] = DeepSpeedPlugin(hf_ds_config=self.init_deepspeed(deepspeed_config))\n        self.accelerator = Accelerator(**self.accelerate)\n        if self.distributed:\n            object_list = [self.state.id]\n            dist.broadcast_object_list(object_list)\n            self.state.id = object_list[0]\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n        if \"log_dir\" not in kwargs:\n            kwargs[\"log_dir\"] = self.dir\n\n        self.writer = SummaryWriter(*args, **kwargs)\n        self.writer.add_scalar = catch(OSError, verbose=False)(self.writer.add_scalar)\n\n    def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Args:\n            seed: Random seed to set.\n                Defaults to `self.state.seed` (`config.seed`).\n\n            bias: Make the seed different for each processes.\n                This is used to ensure the data augmentation are applied differently on every processes.\n                Defaults to `self.rank`.\n                Set to `False` to disable this feature.\n        \"\"\"\n\n        seed = seed or self.state.seed\n        if self.distributed:\n            object_list = [seed]\n            dist.broadcast_object_list(object_list)\n            seed = object_list[0]\n        bias = bias or self.rank\n        if bias:\n            seed += bias\n        self.state.seed = seed\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        if np_random is not None:\n            np_random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n        if torch.__version__ &gt;= \"1.8.0\":\n            torch.use_deterministic_algorithms(True)\n\n    def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n        r\"\"\"\n        Backward loss and step optimizer &amp; scheduler.\n\n        This method increment `self.state.steps`.\n\n        This method also increment `self.state.iters` when `batch_size` is specified.\n\n        Args:\n            zero_grad: Whether to zero the gradients.\n        \"\"\"\n\n        self.accelerator.backward(loss)\n        if self.sync_gradients:\n            if self.state.get(\"max_grad_value\") is not None:\n                self.clip_grad_value_(self.model.parameters(), self.state.get(\"max_grad_value\"))\n            if self.state.get(\"max_grad_norm\") is not None:\n                self.clip_grad_norm_(self.model.parameters(), self.state.get(\"max_grad_norm\"))\n        if self.optimizer is not None:\n            self.optimizer.step()\n            if zero_grad:\n                self.optimizer.zero_grad()\n        if self.scheduler is not None:\n            self.scheduler.step()\n        self.state.steps += 1\n        if batch_size is None:\n            batch_size = self.batch_size_equivalent\n        self.state.iters += batch_size\n        # TODO: Support `drop_last = False`\n        # self.state.iters += self.batch_size_equivalent\n\n    def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Return dict of all attributes for checkpoint.\n        \"\"\"\n\n        if self.model is None:\n            raise ValueError(\"Model must be defined when calling state_dict\")\n        model = self.accelerator.unwrap_model(self.model)\n        return cls(\n            runner=self.state.dict(),\n            model=model.state_dict(),\n            optimizer=self.optimizer.state_dict() if self.optimizer else None,\n            scheduler=self.scheduler.state_dict() if self.scheduler else None,\n        )\n\n    def prepare(self, *args, device_placement: list[bool] | None = None) -&gt; None:\n        r\"\"\"\n        Prepare all objects passed in `args` for distributed training and mixed precision,\n        then return them in the same order.\n        \"\"\"\n\n        return self.accelerator.prepare(*args, device_placement=device_placement)\n\n    def accumulate(self, model: nn.Module | None = None):\n        r\"\"\"\n        Context manager that enables gradient accumulate.\n        \"\"\"\n\n        model = model or self.model\n        return self.accelerator.accumulate(model)\n\n    def autocast(self):\n        r\"\"\"\n        Context manager that enables auto-casting for the forward pass (and maybe backward pass).\n        \"\"\"\n\n        return self.accelerator.autocast()\n\n    def backward(self, loss) -&gt; None:\n        r\"\"\"\n        Backward loss to compute gradients.\n        \"\"\"\n\n        return self.accelerator.backward(loss)\n\n    def unwrap_model(self, model: nn.Module | None = None) -&gt; nn.Module:\n        r\"\"\"\n        Unwrap DDP model.\n\n        Args:\n            model (Optional[nn.Module]):\n                Defaults to `self.model`.\n        \"\"\"\n\n        if model is not None:\n            model = self.model\n        if self.accelerator is not None:\n            return self.accelerator.unwrap_model(model)\n        if self.distributed:\n            return model.module\n        return model\n\n    @property\n    def mode(self) -&gt; RunnerMode:\n        return self._mode\n\n    @mode.setter\n    def mode(self, mode: str | RunnerMode) -&gt; None:\n        if isinstance(mode, str):\n            mode = RunnerMode(mode)\n        self._mode = mode\n        if self.model is not None:\n            self.model.train(mode == RunnerMode.train)\n\n    @property\n    def batch_size(self) -&gt; int:\n        r\"\"\"\n        Batch size.\n\n        Notes:\n            If `train` is in `dataloaders`, then `batch_size` is the batch size of `train`.\n            Otherwise, `batch_size` is the batch size of the first dataloader.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        batch_size = self.state.get(\"dataloader.batch_size\")\n        if batch_size:\n            return batch_size\n        if self.dataloaders:\n            loader = self.dataloaders.get(\"train\", next(iter(self.dataloaders.values())))\n            if loader.batch_size:\n                return loader.batch_size\n            batch_sampler = loader.batch_sampler if loader.batch_sampler is not None else loader.sampler\n            return batch_sampler.batch_size\n        raise AttributeError(\"batch_size could not be inferred, since no dataloader found.\")\n\n    @property\n    def accum_steps(self) -&gt; int:\n        r\"\"\"\n        Gradient accumulation steps.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        return self.accelerator.gradient_accumulation_steps\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of runner.\n        \"\"\"\n\n        return self.accelerator.device\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n        \"\"\"\n\n        return self.accelerator.num_processes\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n        \"\"\"\n\n        return self.accelerator.process_index\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n        \"\"\"\n\n        return self.accelerator.local_process_index\n\n    def gather(self, tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Gather tensor.\n        \"\"\"\n\n        return self.accelerator.gather(tensor)\n\n    def reduce(self, tensor, reduction: str = \"sum\") -&gt; torch.Tensor:\n        r\"\"\"\n        Reduce tensor.\n        \"\"\"\n\n        return self.accelerator.reduce(tensor, reduction=reduction)\n\n    def __getattr__(self, name: str) -&gt; Any:\n        with suppress(AttributeError):\n            return super().__getattr__(name)\n        if \"accelerator\" in self.__dict__ and hasattr(self.accelerator, name):\n            return getattr(self.accelerator, name)\n        raise super().__getattribute__(name)\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.accum_steps","title":"<code>accum_steps: int</code>  <code>property</code>","text":"<p>Gradient accumulation steps.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.batch_size","title":"<code>batch_size: int</code>  <code>property</code>","text":"<p>Batch size.</p> Notes <p>If <code>train</code> is in <code>dataloaders</code>, then <code>batch_size</code> is the batch size of <code>train</code>. Otherwise, <code>batch_size</code> is the batch size of the first dataloader.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Device of runner.</p>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.local_rank","title":"<code>local_rank: int</code>  <code>property</code>","text":"<p>Process index in local processes.</p>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.rank","title":"<code>rank: int</code>  <code>property</code>","text":"<p>Process index in all processes.</p>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.world_size","title":"<code>world_size: int</code>  <code>property</code>","text":"<p>Number of Processes.</p>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.accumulate","title":"<code>accumulate(model=None)</code>","text":"<p>Context manager that enables gradient accumulate.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def accumulate(self, model: nn.Module | None = None):\n    r\"\"\"\n    Context manager that enables gradient accumulate.\n    \"\"\"\n\n    model = model or self.model\n    return self.accelerator.accumulate(model)\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.autocast","title":"<code>autocast()</code>","text":"<p>Context manager that enables auto-casting for the forward pass (and maybe backward pass).</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def autocast(self):\n    r\"\"\"\n    Context manager that enables auto-casting for the forward pass (and maybe backward pass).\n    \"\"\"\n\n    return self.accelerator.autocast()\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.backward","title":"<code>backward(loss)</code>","text":"<p>Backward loss to compute gradients.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def backward(self, loss) -&gt; None:\n    r\"\"\"\n    Backward loss to compute gradients.\n    \"\"\"\n\n    return self.accelerator.backward(loss)\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.evaluate","title":"<code>evaluate(eval_splits=None)</code>","text":"<p>Perform evaluation on <code>eval_splits</code>.</p> <p>Parameters:</p> Name Type Description Default <code>eval_splits</code> <code>list[str]</code> <p>list of split to run evaluate. Defaults to <code>[\"eval\"]</code>.</p> <code>None</code> Return Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def evaluate(self, eval_splits: list[str] | None = None) -&gt; NestedDict:\n    r\"\"\"\n    Perform evaluation on `eval_splits`.\n\n    Args:\n        eval_splits (list[str]): list of split to run evaluate.\n            Defaults to `[\"eval\"]`.\n\n    Return:\n        NestedDict: evaluation result\n    \"\"\"\n\n    if eval_splits is None:\n        eval_splits = [\"eval\"]\n\n    print(\"Begin evaluation\")\n    print(f\"Evaluation splits: {eval_splits}\")\n    result = NestedDict()\n    result.setattr(\"convert_mapping\", True)\n    for split in eval_splits:\n        result[split] = self.evaluate_epoch(split=split)\n    print(self.format_epoch_result(result))\n    return result\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.evaluate_epoch","title":"<code>evaluate_epoch(split='val')</code>","text":"<p>Evaluate one epoch on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run evaluate</p> <code>'val'</code> Return Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@torch.inference_mode()\ndef evaluate_epoch(self, split: str = \"val\") -&gt; NestedDict:\n    r\"\"\"\n    Evaluate one epoch on `split`.\n\n    Args:\n        split (str): split to run evaluate\n\n    Return:\n        NestedDict: evaluation result\n    \"\"\"\n\n    self.mode = \"eval\"  # type: ignore\n    loader = self.dataloaders[split]\n    length = len(loader) - 1\n    last_print_iteration = -1\n    self.meters.reset()\n    if self.metrics is not None:\n        self.metrics.reset()\n    batch_time = time()\n\n    for iteration, data in enumerate(loader):\n        input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n        target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n        pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n        loss = self.criterion(pred, target)\n        if self.metrics is not None:\n            self.metrics.update(pred, target)\n\n        if self.print_interval &gt; 0 and (\n            iteration &gt; 0 and iteration % self.print_interval == 0 or iteration == length\n        ):\n            interval = iteration - last_print_iteration\n            if self.device == torch.device(\"cuda\"):\n                torch.cuda.synchronize()\n            self.meters.time.update((time() - batch_time) / interval)\n            batch_time = time()\n            reduced_loss = self.reduce(loss).item()\n            self.meters.loss.update(reduced_loss)\n            self.step_log(split, iteration, length)\n            last_print_iteration = iteration\n\n    result = self.meters.avg\n    if self.metrics is not None:\n        result.merge(self.metrics.avg)\n    self.write_result(result, split, self.state.epochs)\n    return result\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.gather","title":"<code>gather(tensor)</code>","text":"<p>Gather tensor.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def gather(self, tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Gather tensor.\n    \"\"\"\n\n    return self.accelerator.gather(tensor)\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.inference","title":"<code>inference(split='inf')</code>","text":"<p>Perform inference on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run inference</p> <code>'inf'</code> Return Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@torch.inference_mode()\ndef inference(self, split: str = \"inf\") -&gt; list:\n    r\"\"\"\n    Perform inference on `split`.\n\n    Args:\n        split (str): split to run inference\n\n    Return:\n        Tensor: inference outputs\n    \"\"\"\n\n    self.mode = \"inf\"  # type: ignore\n    loader = self.dataloaders[split]\n    self.meters.reset()\n    output = []\n    for _, data in tqdm(enumerate(loader), total=len(loader)):\n        input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n        pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n        output.extend(pred.tolist())\n\n    if self.distributed:\n        torch.cuda.synchronize()\n        output = self.gather_for_metrics(output)\n    return output\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.init_distributed","title":"<code>init_distributed()</code>","text":"<p>Set up distributed training.</p> <p>Initialise process group and set up DDP variables.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def init_distributed(self) -&gt; None:\n    r\"\"\"\n    Set up distributed training.\n\n    Initialise process group and set up DDP variables.\n    \"\"\"\n\n    if os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\").lower() == \"true\":\n        deepspeed_config = self.state.get(\"deepspeed\", os.environ.get(\"ACCELERATE_DEEPSPEED_CONFIG_FILE\"))\n        self.accelerate[\"deepspeed_plugin\"] = DeepSpeedPlugin(hf_ds_config=self.init_deepspeed(deepspeed_config))\n    self.accelerator = Accelerator(**self.accelerate)\n    if self.distributed:\n        object_list = [self.state.id]\n        dist.broadcast_object_list(object_list)\n        self.state.id = object_list[0]\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.init_tensorboard","title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n    self.writer.add_scalar = catch(OSError, verbose=False)(self.writer.add_scalar)\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.prepare","title":"<code>prepare(*args, device_placement=None)</code>","text":"<p>Prepare all objects passed in <code>args</code> for distributed training and mixed precision, then return them in the same order.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def prepare(self, *args, device_placement: list[bool] | None = None) -&gt; None:\n    r\"\"\"\n    Prepare all objects passed in `args` for distributed training and mixed precision,\n    then return them in the same order.\n    \"\"\"\n\n    return self.accelerator.prepare(*args, device_placement=device_placement)\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.reduce","title":"<code>reduce(tensor, reduction='sum')</code>","text":"<p>Reduce tensor.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def reduce(self, tensor, reduction: str = \"sum\") -&gt; torch.Tensor:\n    r\"\"\"\n    Reduce tensor.\n    \"\"\"\n\n    return self.accelerator.reduce(tensor, reduction=reduction)\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.set_deterministic","title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    if torch.__version__ &gt;= \"1.8.0\":\n        torch.use_deterministic_algorithms(True)\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.set_seed","title":"<code>set_seed(seed=None, bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Random seed to set. Defaults to <code>self.state.seed</code> (<code>config.seed</code>).</p> <code>None</code> <code>bias</code> <code>int | None</code> <p>Make the seed different for each processes. This is used to ensure the data augmentation are applied differently on every processes. Defaults to <code>self.rank</code>. Set to <code>False</code> to disable this feature.</p> <code>None</code> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Args:\n        seed: Random seed to set.\n            Defaults to `self.state.seed` (`config.seed`).\n\n        bias: Make the seed different for each processes.\n            This is used to ensure the data augmentation are applied differently on every processes.\n            Defaults to `self.rank`.\n            Set to `False` to disable this feature.\n    \"\"\"\n\n    seed = seed or self.state.seed\n    if self.distributed:\n        object_list = [seed]\n        dist.broadcast_object_list(object_list)\n        seed = object_list[0]\n    bias = bias or self.rank\n    if bias:\n        seed += bias\n    self.state.seed = seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    if np_random is not None:\n        np_random.seed(seed)\n    random.seed(seed)\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.state_dict","title":"<code>state_dict(cls=dict)</code>","text":"<p>Return dict of all attributes for checkpoint.</p> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Return dict of all attributes for checkpoint.\n    \"\"\"\n\n    if self.model is None:\n        raise ValueError(\"Model must be defined when calling state_dict\")\n    model = self.accelerator.unwrap_model(self.model)\n    return cls(\n        runner=self.state.dict(),\n        model=model.state_dict(),\n        optimizer=self.optimizer.state_dict() if self.optimizer else None,\n        scheduler=self.scheduler.state_dict() if self.scheduler else None,\n    )\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.step","title":"<code>step(loss, batch_size=None, zero_grad=True)</code>","text":"<p>Backward loss and step optimizer &amp; scheduler.</p> <p>This method increment <code>self.state.steps</code>.</p> <p>This method also increment <code>self.state.iters</code> when <code>batch_size</code> is specified.</p> <p>Parameters:</p> Name Type Description Default <code>zero_grad</code> <code>bool</code> <p>Whether to zero the gradients.</p> <code>True</code> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n    r\"\"\"\n    Backward loss and step optimizer &amp; scheduler.\n\n    This method increment `self.state.steps`.\n\n    This method also increment `self.state.iters` when `batch_size` is specified.\n\n    Args:\n        zero_grad: Whether to zero the gradients.\n    \"\"\"\n\n    self.accelerator.backward(loss)\n    if self.sync_gradients:\n        if self.state.get(\"max_grad_value\") is not None:\n            self.clip_grad_value_(self.model.parameters(), self.state.get(\"max_grad_value\"))\n        if self.state.get(\"max_grad_norm\") is not None:\n            self.clip_grad_norm_(self.model.parameters(), self.state.get(\"max_grad_norm\"))\n    if self.optimizer is not None:\n        self.optimizer.step()\n        if zero_grad:\n            self.optimizer.zero_grad()\n    if self.scheduler is not None:\n        self.scheduler.step()\n    self.state.steps += 1\n    if batch_size is None:\n        batch_size = self.batch_size_equivalent\n    self.state.iters += batch_size\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.train","title":"<code>train(train_splits=None, eval_splits=None)</code>","text":"<p>Perform training on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>train_splits</code> <code>list[str]</code> <p>list of split to run train. Defaults to <code>[\"train\"]</code>.</p> <code>None</code> <code>eval_splits</code> <code>list[str]</code> <p>list of split to run evaluate. Defaults to <code>self.dataloaders</code> except for those in <code>train_splits</code>.</p> <code>None</code> Return Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def train(self, train_splits: list[str] | None = None, eval_splits: list[str] | None = None) -&gt; NestedDict:\n    r\"\"\"\n    Perform training on `split`.\n\n    Args:\n        train_splits (list[str]): list of split to run train.\n            Defaults to `[\"train\"]`.\n        eval_splits (list[str]): list of split to run evaluate.\n            Defaults to `self.dataloaders` except for those in `train_splits`.\n\n    Return:\n        NestedDict: train results\n    \"\"\"\n\n    early_stop_counter = 0\n    if train_splits is None:\n        train_splits = [\"train\"]\n    if eval_splits is None:\n        eval_splits = [s for s in self.dataloaders if s not in train_splits]\n    self.state.epoch_begin = self.state.epochs\n    print(f\"Begin training from {self.state.epoch_begin} to {self.state.epoch_end}\")\n    print(f\"Training splits: {train_splits}\")\n    print(f\"Evaluation splits: {eval_splits}\")\n    patience = self.state.get(\"patience\", float(\"inf\"))\n    for epochs in range(self.state.epoch_begin, self.state.epoch_end):  # type: ignore\n        self.state.epochs = epochs\n        result = NestedDict()\n        result.setattr(\"convert_mapping\", True)\n        for split in train_splits:\n            result[split] = self.train_epoch(split)\n        for split in eval_splits:\n            result[split] = self.evaluate_epoch(split)\n        self.append_result(result)\n        print(self.format_epoch_result(result))\n        self.save_result()\n        self.save_checkpoint()\n        \"\"\"@nni.report_intermediate_result(self.latest_score)\"\"\"\n        early_stop_counter = 0 if self.is_best else early_stop_counter + 1\n        if early_stop_counter &gt; patience:\n            print(\"early stop\")\n            break\n    \"\"\"@nni.report_final_result(self.latest_score)\"\"\"\n    return self.results\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.train_epoch","title":"<code>train_epoch(split='train')</code>","text":"<p>Train one epoch on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run train</p> <code>'train'</code> Return Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def train_epoch(self, split: str = \"train\") -&gt; NestedDict:\n    r\"\"\"\n    Train one epoch on `split`.\n\n    Args:\n        split (str): split to run train\n\n    Return:\n        NestedDict: train result\n    \"\"\"\n\n    self.mode = \"train\"  # type: ignore\n    loader = self.dataloaders[split]\n    length = len(loader) - 1\n    last_print_iteration = -1\n    self.meters.reset()\n    if self.metrics is not None:\n        self.metrics.reset()\n    batch_time = time()\n    if hasattr(loader.batch_sampler, \"set_epoch\"):\n        loader.batch_sampler.set_epoch(self.epochs)\n    if hasattr(loader.sampler, \"set_epoch\"):\n        loader.sampler.set_epoch(self.epochs)\n\n    for iteration, data in enumerate(loader):\n        with self.autocast(), self.accumulate():\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            loss = self.criterion(pred, target)\n            if self.metrics is not None:\n                self.metrics.update(pred, target)\n            self.step(loss)\n\n        if self.print_interval &gt; 0 and (\n            iteration &gt; 0 and iteration % self.print_interval == 0 or iteration == length\n        ):\n            interval = iteration - last_print_iteration\n            if self.device == torch.device(\"cuda\"):\n                torch.cuda.synchronize()\n            self.meters.time.update((time() - batch_time) / interval)\n            batch_time = time()\n            reduced_loss = self.reduce(loss).item()\n            self.meters.loss.update(reduced_loss)\n            self.step_log(split, iteration, length)\n            last_print_iteration = iteration\n\n    result = self.meters.avg\n    if self.metrics is not None:\n        result.merge(self.metrics.avg)\n    return result\n</code></pre>"},{"location":"runner/torch_runner/#danling.runner.TorchRunner.unwrap_model","title":"<code>unwrap_model(model=None)</code>","text":"<p>Unwrap DDP model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[Module]</code> <p>Defaults to <code>self.model</code>.</p> <code>None</code> Source code in <code>danling/runner/torch_runner.py</code> Python<pre><code>def unwrap_model(self, model: nn.Module | None = None) -&gt; nn.Module:\n    r\"\"\"\n    Unwrap DDP model.\n\n    Args:\n        model (Optional[nn.Module]):\n            Defaults to `self.model`.\n    \"\"\"\n\n    if model is not None:\n        model = self.model\n    if self.accelerator is not None:\n        return self.accelerator.unwrap_model(model)\n    if self.distributed:\n        return model.module\n    return model\n</code></pre>"},{"location":"runner/utils/","title":"Utilities","text":""},{"location":"runner/utils/#danling.runner.utils.RunnerMode","title":"<code>RunnerMode</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p><code>RunnerMode</code> is an enumeration of running modes.</p> <p>Attributes:</p> Name Type Description <code>train</code> <p>Training mode.</p> <code>eval</code> <p>Evaluation mode.</p> <code>inf</code> <p>Inference mode.</p> Source code in <code>danling/runner/utils.py</code> Python<pre><code>class RunnerMode(StrEnum):  # pylint: disable=too-few-public-methods\n    r\"\"\"\n    `RunnerMode` is an enumeration of running modes.\n\n    Attributes:\n        train: Training mode.\n        eval: Evaluation mode.\n        inf: Inference mode.\n    \"\"\"\n\n    train = auto()\n    eval = auto()\n    inf = auto()\n</code></pre>"},{"location":"runner/utils/#danling.runner.utils.on_local_main_process","title":"<code>on_local_main_process(func)</code>","text":"<p>Decorator to run func only on local main process.</p> Source code in <code>danling/runner/utils.py</code> Python<pre><code>def on_local_main_process(func):\n    \"\"\"\n    Decorator to run func only on local main process.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(self, *args, **kwargs) -&gt; Any | None:\n        if self.is_local_main_process or not self.distributed:\n            return func(self, *args, **kwargs)\n        return None\n\n    return wrapper\n</code></pre>"},{"location":"runner/utils/#danling.runner.utils.on_main_process","title":"<code>on_main_process(func)</code>","text":"<p>Decorator to run func only on main process.</p> Source code in <code>danling/runner/utils.py</code> Python<pre><code>def on_main_process(func):\n    \"\"\"\n    Decorator to run func only on main process.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(self, *args, **kwargs) -&gt; Any | None:\n        if self.is_main_process or not self.distributed:\n            return func(self, *args, **kwargs)\n        return None\n\n    return wrapper\n</code></pre>"},{"location":"tensors/nested_tensor/","title":"NestedTensor","text":"<p>Wrap a sequence of tensors into a single tensor with a mask.</p> <p>In sequence to sequence tasks, elements of a batch are usually not of the same length. This made it tricky to use a single tensor to represent a batch of sequences.</p> <p><code>NestedTensor</code> allows to store a sequence of tensors of different lengths in a single object. It also provides a mask that can be used to retrieve the original sequence of tensors.</p> <p>When calling <code>__getitem__(arg)</code> on a <code>NestedTensor</code>, it has two return type: 1. if arg is <code>int</code> or <code>slice</code>, returns a tuple of two <code>tensor</code>s, representing data and padding mask. 2. if arg is a <code>tuple</code>, return a new <code>NestedTensor</code> with specified shape.</p> <p>Attributes:</p> Text Only<pre><code>_storage: The sequence of tensors.\nbatch_first:  Whether the first dimension of the tensors is the batch dimension.\n\n    If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.\n\n    If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`\npadding_value: The value used to pad the tensors.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Iterable[Tensor]</code> required <code>batch_first</code> <code>bool</code> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensors</code> is not a sequence.</p> <code>ValueError</code> <p>If <code>tensors</code> is empty.</p> Notes <p>We have rewritten the <code>__getattr__</code> function to support as much native tensor operations as possible. However, not all operations are tested.</p> <p>Please file an issue if you find any bugs.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n&gt;&gt;&gt; nested_tensor.dtype\ntorch.int64\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n&gt;&gt;&gt; nested_tensor.to(torch.float).tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]])\n&gt;&gt;&gt; nested_tensor.half().tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]], dtype=torch.float16)\n&gt;&gt;&gt; nested_tensor[:]\n(tensor([[1, 2, 3],\n        [4, 5, 0]]), tensor([[ True,  True,  True],\n        [ True,  True, False]]))\n&gt;&gt;&gt; nested_tensor[1]\n(tensor([4, 5]), tensor([True, True]))\n&gt;&gt;&gt; nested_tensor[:, 1:]\nNestedTensor([[2, 3],\n        [5, 0]])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class NestedTensor:\n    r\"\"\"\n    Wrap a sequence of tensors into a single tensor with a mask.\n\n    In sequence to sequence tasks, elements of a batch are usually not of the same length.\n    This made it tricky to use a single tensor to represent a batch of sequences.\n\n    `NestedTensor` allows to store a sequence of tensors of different lengths in a single object.\n    It also provides a mask that can be used to retrieve the original sequence of tensors.\n\n    When calling `__getitem__(arg)` on a `NestedTensor`, it has two return type:\n    1. if arg is `int` or `slice`, returns a tuple of two `tensor`s, representing data and padding mask.\n    2. if arg is a `tuple`, return a new `NestedTensor` with specified shape.\n\n    Attributes:\n\n        _storage: The sequence of tensors.\n        batch_first:  Whether the first dimension of the tensors is the batch dimension.\n\n            If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.\n\n            If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`\n        padding_value: The value used to pad the tensors.\n\n    Args:\n        tensors:\n        batch_first:\n\n    Raises:\n        ValueError: If `tensors` is not a sequence.\n        ValueError: If `tensors` is empty.\n\n    Notes:\n        We have rewritten the `__getattr__` function to support as much native tensor operations as possible.\n        However, not all operations are tested.\n\n        Please file an issue if you find any bugs.\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.device\n        device(type='cpu')\n        &gt;&gt;&gt; nested_tensor.dtype\n        torch.int64\n        &gt;&gt;&gt; nested_tensor.tensor\n        tensor([[1, 2, 3],\n                [4, 5, 0]])\n        &gt;&gt;&gt; nested_tensor.mask\n        tensor([[ True,  True,  True],\n                [ True,  True, False]])\n        &gt;&gt;&gt; nested_tensor.to(torch.float).tensor\n        tensor([[1., 2., 3.],\n                [4., 5., 0.]])\n        &gt;&gt;&gt; nested_tensor.half().tensor\n        tensor([[1., 2., 3.],\n                [4., 5., 0.]], dtype=torch.float16)\n        &gt;&gt;&gt; nested_tensor[:]\n        (tensor([[1, 2, 3],\n                [4, 5, 0]]), tensor([[ True,  True,  True],\n                [ True,  True, False]]))\n        &gt;&gt;&gt; nested_tensor[1]\n        (tensor([4, 5]), tensor([True, True]))\n        &gt;&gt;&gt; nested_tensor[:, 1:]\n        NestedTensor([[2, 3],\n                [5, 0]])\n    \"\"\"\n\n    _storage: Sequence[Tensor]\n    batch_first: bool = True\n    padding_value: SupportsFloat = 0.0\n    mask_value: bool = False\n\n    def __init__(\n        self,\n        tensors: Iterable[Tensor],\n        batch_first: bool = True,\n        padding_value: SupportsFloat = 0.0,\n        mask_value: bool = False,\n    ) -&gt; None:\n        if not isinstance(tensors, Iterable):\n            raise ValueError(f\"NestedTensor must be initialised with an Iterable, bug got {type(tensors)}.\")\n        tensors = list(tensors)\n        if len(tensors) == 0:\n            raise ValueError(\"NestedTensor must be initialised with a non-empty Iterable.\")\n        if not isinstance(tensors[0], Tensor):\n            tensors = [torch.tensor(tensor) for tensor in tensors]\n        self._storage = tensors\n        self.batch_first = batch_first\n        self.padding_value = padding_value\n        self.mask_value = mask_value\n\n    def storage(self):\n        return self._storage\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Return a single tensor by padding all the tensors.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.tensor\n            tensor([[1, 2, 3],\n                    [4, 5, 0]])\n        \"\"\"\n\n        return self._tensor(tuple(self._storage), self.batch_first, float(self.padding_value))\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Padding mask of `tensor`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.mask\n            tensor([[ True,  True,  True],\n                    [ True,  True, False]])\n        \"\"\"\n\n        return self._mask(tuple(self._storage), self.batch_first, self.mask_value)\n\n    @classmethod\n    def from_tensor_mask(cls, tensor: Tensor, mask: Tensor):\n        r\"\"\"\n        Build a `NestedTensor` object from a padded `Tensor` and corresponding mask `Tensor`.\n\n        Args:\n            tensor: Padded Tensor.\n            mask: Tensor Mask.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; padded_tensor = torch.tensor([[1, 2, 3, 0, 0],\n            ...                                [4, 5, 0, 0, 0],\n            ...                                [6, 7, 8, 9, 0]])\n            &gt;&gt;&gt; mask_tensor = torch.tensor([[1, 1, 1, 0, 0],\n            ...                             [1, 1, 0, 0, 0],\n            ...                             [1, 1, 1, 1, 0]])\n            &gt;&gt;&gt; nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)\n            &gt;&gt;&gt; nested_tensor.tensor\n            tensor([[1, 2, 3, 0],\n                    [4, 5, 0, 0],\n                    [6, 7, 8, 9]])\n        \"\"\"\n\n        if mask.ndim == 2:\n            return cls(t[slice(0, m.sum())] for t, m in zip(tensor, mask))\n        return cls(\n            t[[slice(0, (m.sum(dim=dim) &gt; 0).sum().item()) for dim in reversed(range(m.dim()))]]\n            for t, m in zip(tensor, mask)\n        )\n\n    def nested_like(self, other: Tensor, unsafe: bool = False) -&gt; NestedTensor:\n        r\"\"\"\n        Create a new `NestedTensor` from a `Tensor`.\n        The newly created `NestedTensor` will have the same shape as current `NestedTensor`.\n\n        Args:\n            other: The `Tensor` to be nested.\n            unsafe: Whether to check the shape of `other` and current `NestedTensor`.\n\n        Returns:\n            (NestedTensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; tensor = nested_tensor.tensor\n            &gt;&gt;&gt; new_tensor = nested_tensor.nested_like(tensor)\n            &gt;&gt;&gt; all([(x == y).all() for x, y in zip(nested_tensor.storage(), new_tensor.storage())])\n            True\n            &gt;&gt;&gt; f = nested_tensor.nested_like(torch.randn(2, 2))\n            Traceback (most recent call last):\n            ValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])\n            &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(2, 2), True)\n            &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(3, 3), True)\n            Traceback (most recent call last):\n            ValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\n        \"\"\"  # noqa: E501\n\n        if not unsafe and self.shape != other.shape:\n            raise ValueError(\n                f\"The shape of NestedTensor and input tensor does not match, {self.shape} != {other.shape}\"\n            )\n        if self.size(0) != other.size(0):\n            raise ValueError(\n                f\"The batch size of NestedTensor and input tensor does not match, {self.size(0)} != {other.size(0)}\"\n            )\n        return NestedTensor([o[tuple(slice(0, dim) for dim in t.shape)] for t, o in zip(self._storage, other)])\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of the NestedTensor.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.device\n            device(type='cpu')\n        \"\"\"\n\n        return self._device(tuple(self._storage))\n\n    @property\n    def shape(self) -&gt; torch.Size | int:\n        r\"\"\"\n        Alias for `size()`.\n        \"\"\"\n\n        return self.size()\n\n    @property\n    def ndim(self) -&gt; int:\n        r\"\"\"\n        Alias for `dim()`.\n        \"\"\"\n\n        return self.dim()\n\n    def size(self, dim: int | None = None) -&gt; torch.Size | int:\n        r\"\"\"\n        Returns the size of the self `NestedTensor`.\n\n        Args:\n            dim: If not specified, the returned value is a `torch.Size`, a subclass of `tuple`.\n                If specified, returns an `int` holding the size of that dimension.\n                Defaults to `None`.\n\n        Returns:\n            (torch.Size | int):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.size()\n            torch.Size([2, 3])\n            &gt;&gt;&gt; nested_tensor.size(0)\n            2\n            &gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n            &gt;&gt;&gt; nested_tensor.shape\n            torch.Size([2, 4])\n            &gt;&gt;&gt; nested_tensor.size(1)\n            4\n        \"\"\"\n\n        return self._size(tuple(self._storage), dim, self.batch_first)\n\n    def dim(self) -&gt; int:\n        r\"\"\"\n        Number of dimension of the NestedTensor.\n\n        Returns:\n            (int):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.dim()\n            2\n            &gt;&gt;&gt; nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))\n            &gt;&gt;&gt; nested_tensor.ndim\n            2\n        \"\"\"\n\n        return self._dim(tuple(self._storage))\n\n    def tolist(self) -&gt; list:\n        return [t.tolist() for t in self._storage]\n\n    def where(self, condition, other) -&gt; NestedTensor:\n        r\"\"\"\n        Return a NestedTensor of elements selected from either self or other, depending on condition.\n\n        Returns:\n            (NestedTensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.size()\n            torch.Size([2, 3])\n            &gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n            &gt;&gt;&gt; nested_tensor.size()\n            torch.Size([2, 4])\n        \"\"\"\n\n        if isinstance(condition, NestedTensor) and isinstance(other, NestedTensor):\n            return NestedTensor(\n                [x.where(c, y) for x, c, y in zip(self._storage, condition._storage, other._storage)], **self._state\n            )\n        if isinstance(condition, NestedTensor):\n            return NestedTensor([x.where(c, other) for x, c in zip(self._storage, condition._storage)], **self._state)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x.where(condition, y) for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor(x.where(condition, other) for x in self._storage)\n\n    def __abs__(self):\n        return NestedTensor([abs(value) for value in self._storage], **self._state)\n\n    def __add__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x + y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value + other for value in self._storage], **self._state)\n\n    def __radd__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y + x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other + value for value in self._storage], **self._state)\n\n    def __iadd__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x += y\n        else:\n            for value in self._storage:\n                value += other\n        return self\n\n    def __and__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x &amp; y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value &amp; other for value in self._storage], **self._state)\n\n    def __rand__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y &amp; x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other &amp; value for value in self._storage], **self._state)\n\n    def __iand__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x &amp;= y\n        else:\n            for value in self._storage:\n                value &amp;= other\n        return self\n\n    def __floordiv__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x // y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value // other for value in self._storage], **self._state)\n\n    def __rfloordiv__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y // x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other // value for value in self._storage], **self._state)\n\n    def __ifloordiv__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x //= y\n        else:\n            for value in self._storage:\n                value //= other\n        return self\n\n    def __mod__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x % y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value % other for value in self._storage], **self._state)\n\n    def __rmod__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y % x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other % value for value in self._storage], **self._state)\n\n    def __imod__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x %= y\n        else:\n            for value in self._storage:\n                value %= other\n        return self\n\n    def __mul__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x * y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value * other for value in self._storage], **self._state)\n\n    def __rmul__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y * x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other * value for value in self._storage], **self._state)\n\n    def __imul__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x *= y\n        else:\n            for value in self._storage:\n                value *= other\n        return self\n\n    def __matmul__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x @ y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value @ other for value in self._storage], **self._state)\n\n    def __rmatmul__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y @ x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other @ value for value in self._storage], **self._state)\n\n    def __imatmul__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x @= y\n        else:\n            for value in self._storage:\n                value @= other\n        return self\n\n    def __pow__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x**y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value**other for value in self._storage], **self._state)\n\n    def __rpow__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y**x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other**value for value in self._storage], **self._state)\n\n    def __ipow__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x *= y\n        else:\n            for value in self._storage:\n                value *= other\n        return self\n\n    def __truediv__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x / y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value / other for value in self._storage], **self._state)\n\n    def __rtruediv__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y / x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other / value for value in self._storage], **self._state)\n\n    def __itruediv__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x /= y\n        else:\n            for value in self._storage:\n                value /= other\n        return self\n\n    def __sub__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x - y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value - other for value in self._storage], **self._state)\n\n    def __rsub__(self, other):\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y - x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other - value for value in self._storage], **self._state)\n\n    def __isub__(self, other):\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x -= y\n        else:\n            for value in self._storage:\n                value -= other\n        return self\n\n    def __getitem__(self, index: int | slice | tuple) -&gt; tuple[Tensor, Tensor] | NestedTensor:\n        if isinstance(index, tuple):\n            return NestedTensor([t[index[0]][index[1:]] for t in self._storage])\n        if isinstance(index, (int, slice)):\n            ret = self._storage[index]\n            if isinstance(ret, Tensor):\n                return ret, torch.ones_like(ret, dtype=torch.bool)\n            return self.tensor, self.mask\n        raise ValueError(f\"Unsupported index type {type(index)}\")\n\n    def __getattr__(self, name) -&gt; Any:\n        if not self._storage:\n            raise ValueError(f\"Unable to get {name} from an empty {self.__class__.__name__}\")\n        ret = [getattr(i, name) for i in self._storage]\n        elem = ret[0]\n        if isinstance(elem, Tensor):\n            return NestedTensor(ret, **self._state)\n        if callable(elem):\n            return NestedTensorFuncWrapper(ret, state=self._state)\n        if elem.__hash__ is not None and len(set(ret)) == 1:\n            return elem\n        return ret\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        if func not in NestedTensorFunc or not all(issubclass(t, (torch.Tensor, NestedTensor)) for t in types):\n            args = [a.tensor if hasattr(a, \"tensor\") else a for a in args]\n            return func(*args, **kwargs)\n        return NestedTensorFunc[func](*args, **kwargs)\n\n    def __len__(self) -&gt; int:\n        return len(self._storage)\n\n    def __eq__(  # type: ignore[override]\n        self, other: Tensor | NestedTensor | SupportsFloat\n    ) -&gt; bool | Tensor | NestedTensor:\n        if isinstance(other, NestedTensor):\n            return NestedTensor(i == j for i, j in zip(self._storage, other._storage))\n        if isinstance(other, Tensor):\n            return self.tensor == other\n        if isinstance(other, SupportsFloat):\n            return NestedTensor([x == other for x in self._storage], **self._state)\n        return False\n\n    @property\n    def _state(self) -&gt; Mapping:\n        return {k: v for k, v in self.__dict__.items() if k != \"_storage\"}\n\n    def __state__(self) -&gt; Mapping:\n        return self.__dict__\n\n    def __setstate__(self, state: Mapping) -&gt; None:\n        self.__dict__.update(state)\n\n    def __repr__(self):\n        return self.__class__.__name__ + repr(self.tensor)[len(self.tensor.__class__.__name__) :]  # noqa: E203\n\n    @staticmethod\n    @lru_cache(maxsize=1)\n    def _tensor(storage, batch_first, padding_value: float = 0) -&gt; Tensor:\n        if storage[0].dim() == 0:\n            return torch.stack(storage, dim=0)\n        return pad_tensor(storage, batch_first=batch_first, padding_value=padding_value)\n\n    @staticmethod\n    @lru_cache(maxsize=1)\n    def _mask(storage, batch_first, mask_value: bool = False) -&gt; Tensor:\n        if storage[0].dim() == 0:\n            return torch.full(len(storage), fill_value=not mask_value, dtype=torch.bool, device=storage[0].device)\n        return mask_tensor(storage, batch_first=batch_first, mask_value=mask_value)\n\n    @staticmethod\n    @lru_cache(maxsize=1)\n    def _device(storage) -&gt; torch.device:\n        return storage[0].device\n\n    @staticmethod\n    @lru_cache(maxsize=1)\n    def _size(storage, dim: int | None = None, batch_first: bool = True) -&gt; torch.Size | int:\n        if dim is not None:\n            if dim == 0:\n                return len(storage)\n            return max(t.size(dim - 1) for t in storage)\n        if max(t.dim() for t in storage) == 0:\n            return torch.Size((len(storage),))\n        ndim = max(t.dim() for t in storage)\n        size = [max(t.shape[i] if i &lt; len(t.shape) else 0 for t in storage) for i in range(ndim)]\n        size.insert(0 if batch_first else 1, len(storage))\n        return torch.Size(size)\n\n    @staticmethod\n    @lru_cache(maxsize=1)\n    def _dim(storage) -&gt; int:\n        return max(t.dim() for t in storage) + 1\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Device of the NestedTensor.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.mask","title":"<code>mask: Tensor</code>  <code>property</code>","text":"<p>Padding mask of <code>tensor</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.ndim","title":"<code>ndim: int</code>  <code>property</code>","text":"<p>Alias for <code>dim()</code>.</p>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.shape","title":"<code>shape: torch.Size | int</code>  <code>property</code>","text":"<p>Alias for <code>size()</code>.</p>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.tensor","title":"<code>tensor: Tensor</code>  <code>property</code>","text":"<p>Return a single tensor by padding all the tensors.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.dim","title":"<code>dim()</code>","text":"<p>Number of dimension of the NestedTensor.</p> <p>Returns:</p> Type Description <code>int</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.dim()\n2\n&gt;&gt;&gt; nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))\n&gt;&gt;&gt; nested_tensor.ndim\n2\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def dim(self) -&gt; int:\n    r\"\"\"\n    Number of dimension of the NestedTensor.\n\n    Returns:\n        (int):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.dim()\n        2\n        &gt;&gt;&gt; nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))\n        &gt;&gt;&gt; nested_tensor.ndim\n        2\n    \"\"\"\n\n    return self._dim(tuple(self._storage))\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.from_tensor_mask","title":"<code>from_tensor_mask(tensor, mask)</code>  <code>classmethod</code>","text":"<p>Build a <code>NestedTensor</code> object from a padded <code>Tensor</code> and corresponding mask <code>Tensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Padded Tensor.</p> required <code>mask</code> <code>Tensor</code> <p>Tensor Mask.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; padded_tensor = torch.tensor([[1, 2, 3, 0, 0],\n...                                [4, 5, 0, 0, 0],\n...                                [6, 7, 8, 9, 0]])\n&gt;&gt;&gt; mask_tensor = torch.tensor([[1, 1, 1, 0, 0],\n...                             [1, 1, 0, 0, 0],\n...                             [1, 1, 1, 1, 0]])\n&gt;&gt;&gt; nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3, 0],\n        [4, 5, 0, 0],\n        [6, 7, 8, 9]])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@classmethod\ndef from_tensor_mask(cls, tensor: Tensor, mask: Tensor):\n    r\"\"\"\n    Build a `NestedTensor` object from a padded `Tensor` and corresponding mask `Tensor`.\n\n    Args:\n        tensor: Padded Tensor.\n        mask: Tensor Mask.\n\n    Returns:\n        (torch.Tensor):\n\n    Examples:\n        &gt;&gt;&gt; padded_tensor = torch.tensor([[1, 2, 3, 0, 0],\n        ...                                [4, 5, 0, 0, 0],\n        ...                                [6, 7, 8, 9, 0]])\n        &gt;&gt;&gt; mask_tensor = torch.tensor([[1, 1, 1, 0, 0],\n        ...                             [1, 1, 0, 0, 0],\n        ...                             [1, 1, 1, 1, 0]])\n        &gt;&gt;&gt; nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)\n        &gt;&gt;&gt; nested_tensor.tensor\n        tensor([[1, 2, 3, 0],\n                [4, 5, 0, 0],\n                [6, 7, 8, 9]])\n    \"\"\"\n\n    if mask.ndim == 2:\n        return cls(t[slice(0, m.sum())] for t, m in zip(tensor, mask))\n    return cls(\n        t[[slice(0, (m.sum(dim=dim) &gt; 0).sum().item()) for dim in reversed(range(m.dim()))]]\n        for t, m in zip(tensor, mask)\n    )\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.nested_like","title":"<code>nested_like(other, unsafe=False)</code>","text":"<p>Create a new <code>NestedTensor</code> from a <code>Tensor</code>. The newly created <code>NestedTensor</code> will have the same shape as current <code>NestedTensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Tensor</code> <p>The <code>Tensor</code> to be nested.</p> required <code>unsafe</code> <code>bool</code> <p>Whether to check the shape of <code>other</code> and current <code>NestedTensor</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>NestedTensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; tensor = nested_tensor.tensor\n&gt;&gt;&gt; new_tensor = nested_tensor.nested_like(tensor)\n&gt;&gt;&gt; all([(x == y).all() for x, y in zip(nested_tensor.storage(), new_tensor.storage())])\nTrue\n&gt;&gt;&gt; f = nested_tensor.nested_like(torch.randn(2, 2))\nTraceback (most recent call last):\nValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])\n&gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(2, 2), True)\n&gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(3, 3), True)\nTraceback (most recent call last):\nValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def nested_like(self, other: Tensor, unsafe: bool = False) -&gt; NestedTensor:\n    r\"\"\"\n    Create a new `NestedTensor` from a `Tensor`.\n    The newly created `NestedTensor` will have the same shape as current `NestedTensor`.\n\n    Args:\n        other: The `Tensor` to be nested.\n        unsafe: Whether to check the shape of `other` and current `NestedTensor`.\n\n    Returns:\n        (NestedTensor):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; tensor = nested_tensor.tensor\n        &gt;&gt;&gt; new_tensor = nested_tensor.nested_like(tensor)\n        &gt;&gt;&gt; all([(x == y).all() for x, y in zip(nested_tensor.storage(), new_tensor.storage())])\n        True\n        &gt;&gt;&gt; f = nested_tensor.nested_like(torch.randn(2, 2))\n        Traceback (most recent call last):\n        ValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])\n        &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(2, 2), True)\n        &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(3, 3), True)\n        Traceback (most recent call last):\n        ValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\n    \"\"\"  # noqa: E501\n\n    if not unsafe and self.shape != other.shape:\n        raise ValueError(\n            f\"The shape of NestedTensor and input tensor does not match, {self.shape} != {other.shape}\"\n        )\n    if self.size(0) != other.size(0):\n        raise ValueError(\n            f\"The batch size of NestedTensor and input tensor does not match, {self.size(0)} != {other.size(0)}\"\n        )\n    return NestedTensor([o[tuple(slice(0, dim) for dim in t.shape)] for t, o in zip(self._storage, other)])\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.size","title":"<code>size(dim=None)</code>","text":"<p>Returns the size of the self <code>NestedTensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int | None</code> <p>If not specified, the returned value is a <code>torch.Size</code>, a subclass of <code>tuple</code>. If specified, returns an <code>int</code> holding the size of that dimension. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Size | int</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.size(0)\n2\n&gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 4])\n&gt;&gt;&gt; nested_tensor.size(1)\n4\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def size(self, dim: int | None = None) -&gt; torch.Size | int:\n    r\"\"\"\n    Returns the size of the self `NestedTensor`.\n\n    Args:\n        dim: If not specified, the returned value is a `torch.Size`, a subclass of `tuple`.\n            If specified, returns an `int` holding the size of that dimension.\n            Defaults to `None`.\n\n    Returns:\n        (torch.Size | int):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.size(0)\n        2\n        &gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 4])\n        &gt;&gt;&gt; nested_tensor.size(1)\n        4\n    \"\"\"\n\n    return self._size(tuple(self._storage), dim, self.batch_first)\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.where","title":"<code>where(condition, other)</code>","text":"<p>Return a NestedTensor of elements selected from either self or other, depending on condition.</p> <p>Returns:</p> Type Description <code>NestedTensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 4])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def where(self, condition, other) -&gt; NestedTensor:\n    r\"\"\"\n    Return a NestedTensor of elements selected from either self or other, depending on condition.\n\n    Returns:\n        (NestedTensor):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 4])\n    \"\"\"\n\n    if isinstance(condition, NestedTensor) and isinstance(other, NestedTensor):\n        return NestedTensor(\n            [x.where(c, y) for x, c, y in zip(self._storage, condition._storage, other._storage)], **self._state\n        )\n    if isinstance(condition, NestedTensor):\n        return NestedTensor([x.where(c, other) for x, c in zip(self._storage, condition._storage)], **self._state)\n    if isinstance(other, NestedTensor):\n        return NestedTensor([x.where(condition, y) for x, y in zip(self._storage, other._storage)], **self._state)\n    return NestedTensor(x.where(condition, other) for x in self._storage)\n</code></pre>"},{"location":"tensors/pn_tensor/","title":"PNTensor","text":"<p>             Bases: <code>Tensor</code></p> <p>Wrapper for tensors to be converted to <code>NestedTensor</code>.</p> <p><code>PNTensor</code> is a subclass of <code>torch.Tensor</code>. It implements two additional methods as <code>NestedTensor</code>: <code>tensor</code> and <code>mask</code>.</p> <p>Although it is possible to construct <code>NestedTensor</code> in dataset, the best practice is to do so in <code>collate_fn</code>. However, it is hard to tell if a batch of <code>Tensor</code> should be stacked or converted to <code>NestedTensor</code>.</p> <p><code>PNTensor</code> is introduced overcome this limitation.</p> <p>Convert tensors that will be converted to <code>NestedTensor</code> to a <code>PNTensor</code>, and all you need to do is to convert <code>PNTensor</code> to <code>NestedTensor</code> in <code>collate_fn</code>.</p> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class PNTensor(Tensor):\n    r\"\"\"\n    Wrapper for tensors to be converted to `NestedTensor`.\n\n    `PNTensor` is a subclass of `torch.Tensor`.\n    It implements two additional methods as `NestedTensor`: `tensor` and `mask`.\n\n    Although it is possible to construct `NestedTensor` in dataset,\n    the best practice is to do so in `collate_fn`.\n    However, it is hard to tell if a batch of `Tensor` should be stacked or converted to `NestedTensor`.\n\n    `PNTensor` is introduced overcome this limitation.\n\n    Convert tensors that will be converted to `NestedTensor` to a `PNTensor`,\n    and all you need to do is to convert `PNTensor` to `NestedTensor` in `collate_fn`.\n    \"\"\"\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Identical to `self`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n            &gt;&gt;&gt; pn_tensor = PNTensor(tensor)\n            &gt;&gt;&gt; (tensor == pn_tensor).all()\n            PNTensor(True)\n            &gt;&gt;&gt; (tensor == pn_tensor.tensor).all()\n            PNTensor(True)\n        \"\"\"\n\n        return self\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Identical to `torch.ones_like(self)`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n            &gt;&gt;&gt; pn_tensor = PNTensor(tensor)\n            &gt;&gt;&gt; (pn_tensor.mask == torch.ones_like(pn_tensor)).all()\n            PNTensor(True)\n        \"\"\"\n\n        return torch.ones_like(self)\n\n    def new_empty(self, *args, **kwargs):\n        return PNTensor(super().new_empty(*args, **kwargs))\n</code></pre>"},{"location":"tensors/pn_tensor/#danling.tensors.PNTensor.mask","title":"<code>mask: Tensor</code>  <code>property</code>","text":"<p>Identical to <code>torch.ones_like(self)</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; pn_tensor = PNTensor(tensor)\n&gt;&gt;&gt; (pn_tensor.mask == torch.ones_like(pn_tensor)).all()\nPNTensor(True)\n</code></pre>"},{"location":"tensors/pn_tensor/#danling.tensors.PNTensor.tensor","title":"<code>tensor: Tensor</code>  <code>property</code>","text":"<p>Identical to <code>self</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; pn_tensor = PNTensor(tensor)\n&gt;&gt;&gt; (tensor == pn_tensor).all()\nPNTensor(True)\n&gt;&gt;&gt; (tensor == pn_tensor.tensor).all()\nPNTensor(True)\n</code></pre>"},{"location":"tensors/torch_func_registry/","title":"TorchFuncRegistry","text":"<p>             Bases: <code>Registry</code></p> <p><code>TorchFuncRegistry</code> for extending PyTorch Tensor.</p> Source code in <code>danling/tensors/utils.py</code> Python<pre><code>class TorchFuncRegistry(Registry):  # pylint: disable=too-few-public-methods\n    \"\"\"\n    `TorchFuncRegistry` for extending PyTorch Tensor.\n    \"\"\"\n\n    def implement(self, torch_function: Callable) -&gt; Callable:\n        r\"\"\"\n        Implement an implementation for a torch function.\n\n        Args:\n            function: The torch function to implement.\n\n        Returns:\n            function: The registered function.\n\n        Raises:\n            ValueError: If the function with the same name already registered and `TorchFuncRegistry.override=False`.\n\n        Examples:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; registry = TorchFuncRegistry(\"test\")\n            &gt;&gt;&gt; @registry.implement(torch.mean)\n            ... def mean(input):\n            ...     raise input.mean()\n            &gt;&gt;&gt; registry  # doctest: +ELLIPSIS\n            TorchFuncRegistry(\n              (&lt;built-in method mean of type object at ...&gt;): &lt;function mean at ...&gt;\n            )\n        \"\"\"\n\n        if torch_function in self and not self.override:\n            raise ValueError(f\"Torch function {torch_function.__name__} already registered.\")\n\n        @wraps(self.register)\n        def register(function):\n            self.set(torch_function, function)\n            return function\n\n        return register\n</code></pre>"},{"location":"tensors/torch_func_registry/#danling.tensors.TorchFuncRegistry.implement","title":"<code>implement(torch_function)</code>","text":"<p>Implement an implementation for a torch function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <p>The torch function to implement.</p> required <p>Returns:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The registered function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the function with the same name already registered and <code>TorchFuncRegistry.override=False</code>.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; registry = TorchFuncRegistry(\"test\")\n&gt;&gt;&gt; @registry.implement(torch.mean)\n... def mean(input):\n...     raise input.mean()\n&gt;&gt;&gt; registry\nTorchFuncRegistry(\n  (&lt;built-in method mean of type object at ...&gt;): &lt;function mean at ...&gt;\n)\n</code></pre> Source code in <code>danling/tensors/utils.py</code> Python<pre><code>def implement(self, torch_function: Callable) -&gt; Callable:\n    r\"\"\"\n    Implement an implementation for a torch function.\n\n    Args:\n        function: The torch function to implement.\n\n    Returns:\n        function: The registered function.\n\n    Raises:\n        ValueError: If the function with the same name already registered and `TorchFuncRegistry.override=False`.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; registry = TorchFuncRegistry(\"test\")\n        &gt;&gt;&gt; @registry.implement(torch.mean)\n        ... def mean(input):\n        ...     raise input.mean()\n        &gt;&gt;&gt; registry  # doctest: +ELLIPSIS\n        TorchFuncRegistry(\n          (&lt;built-in method mean of type object at ...&gt;): &lt;function mean at ...&gt;\n        )\n    \"\"\"\n\n    if torch_function in self and not self.override:\n        raise ValueError(f\"Torch function {torch_function.__name__} already registered.\")\n\n    @wraps(self.register)\n    def register(function):\n        self.set(torch_function, function)\n        return function\n\n    return register\n</code></pre>"},{"location":"utils/basex/","title":"basex","text":""},{"location":"utils/contextmanagers/","title":"Context Manager","text":""},{"location":"utils/contextmanagers/#danling.utils.contextmanagers.debug","title":"<code>debug(enable=True, error=Exception, exclude=None)</code>","text":"<p>Contextmanager to enter debug mode on <code>error</code> except for <code>exclude</code>.</p> <p><code>debug</code> is intended to be used to catch the error and enter debug mode. Since it is mainly for development purposed, we include an <code>enable</code> args so that it can be deactivated.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Whether to enable the contextmanager. Defaults to <code>True</code>.</p> <code>True</code> <code>error</code> <code>Exceptions</code> <p>The error to catch. Defaults to <code>Exception</code>.</p> <code>Exception</code> <code>exclude</code> <code>Optional[Exceptions]</code> <p>The error to exclude. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>danling/utils/contextmanagers.py</code> Python<pre><code>@contextmanager\ndef debug(\n    enable: bool = True,\n    error: Exceptions = Exception,\n    exclude: Optional[Exceptions] = None,\n):\n    \"\"\"\n    Contextmanager to enter debug mode on `error` except for `exclude`.\n\n    `debug` is intended to be used to catch the error and enter debug mode.\n    Since it is mainly for development purposed, we include an `enable` args so that it can be deactivated.\n\n    Args:\n        enable: Whether to enable the contextmanager.\n            Defaults to `True`.\n        error: The error to catch.\n            Defaults to `Exception`.\n        exclude: The error to exclude.\n            Defaults to `None`.\n    \"\"\"\n\n    if not enable:\n        yield\n        return\n    try:\n        yield\n    except error as exc:  # pylint: disable=broad-exception-caught\n        if exclude is not None and isinstance(exc, exclude):\n            raise exc\n        _, m, tb = sys.exc_info()\n        print(repr(m), file=sys.stderr)\n        pdb.post_mortem(tb)\n    finally:\n        pass\n</code></pre>"},{"location":"utils/decorators/","title":"Decorator","text":""},{"location":"utils/decorators/#danling.utils.decorators.catch","title":"<code>catch(error=Exception, exclude=None, callback=print_exc, *callback_args, **callback_kwargs)</code>","text":"<p>Decorator to catch <code>error</code> except for <code>exclude</code>. Detailed traceback will be printed to <code>stderr</code>.</p> <p><code>catch</code> is extremely useful for unfatal errors. For example, <code>Runner</code> saves checkpoint regularly, however, this might break running if the space is full. Decorating <code>save</code> method with <code>catch</code> will allow you to catch these errors and continue your running.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exceptions</code> <p>Exceptions to be caught.</p> <code>Exception</code> <code>exclude</code> <code>Exceptions | None</code> <p>Exceptions to be excluded.</p> <code>None</code> <code>callback</code> <code>Callable</code> <p>Callback to be called when an error occurs. The first four arguments to <code>callback</code> are <code>exc</code>, <code>func</code>, <code>args</code>, <code>kwargs</code>. Additional arguments should be passed with <code>*callback_args</code> and <code>**callback_kwargs</code>.</p> <code>print_exc</code> <code>callback_args</code> <p>Arguments to be passed to <code>callback</code>.</p> <code>()</code> <code>callback_kwargs</code> <p>Keyword arguments to be passed to <code>callback</code>.</p> <code>{}</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; def file_not_found(*args, **kwargs):\n...     return exclude\n...     raise FileNotFoundError\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; file_not_found()\n&gt;&gt;&gt; raise ValueError\n&gt;&gt;&gt; assert 1 == 2\n</code></pre> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>@flexible_decorator\ndef catch(  # pylint: disable=keyword-arg-before-vararg\n    error: Exceptions = Exception,\n    exclude: Exceptions | None = None,\n    callback: Callable = print_exc,\n    *callback_args,\n    **callback_kwargs,\n):\n    r\"\"\"\n    Decorator to catch `error` except for `exclude`.\n    Detailed traceback will be printed to `stderr`.\n\n    `catch` is extremely useful for unfatal errors.\n    For example, `Runner` saves checkpoint regularly, however, this might break running if the space is full.\n    Decorating `save` method with `catch` will allow you to catch these errors and continue your running.\n\n    Args:\n        error: Exceptions to be caught.\n        exclude: Exceptions to be excluded.\n        callback: Callback to be called when an error occurs.\n            The first four arguments to `callback` are `exc`, `func`, `args`, `kwargs`.\n            Additional arguments should be passed with `*callback_args` and `**callback_kwargs`.\n        callback_args: Arguments to be passed to `callback`.\n        callback_kwargs: Keyword arguments to be passed to `callback`.\n\n    Examples:\n        &gt;&gt;&gt; def file_not_found(*args, **kwargs):\n        ...     return exclude\n        ...     raise FileNotFoundError\n\n        &gt;&gt;&gt; file_not_found()\n        &gt;&gt;&gt; raise ValueError\n        &gt;&gt;&gt; assert 1 == 2\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):  # pylint: disable=inconsistent-return-statements\n            try:\n                return func(*args, **kwargs)\n            except error as exc:  # pylint: disable=broad-exception-caught\n                if exclude is not None and isinstance(exc, exclude):\n                    raise exc\n                callback(exc, func, args, kwargs, *callback_args, **callback_kwargs)\n\n        return wrapper\n\n    decorator.__doc__ = catch.__doc__\n\n    return decorator\n</code></pre>"},{"location":"utils/decorators/#danling.utils.decorators.ensure_dir","title":"<code>ensure_dir(func)</code>","text":"<p>Decorator to ensure a directory property exists.</p> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>def ensure_dir(func):\n    r\"\"\"\n    Decorator to ensure a directory property exists.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        path = abspath(func(*args, **kwargs))\n        makedirs(path, exist_ok=True)\n        return path\n\n    return wrapper\n</code></pre>"},{"location":"utils/decorators/#danling.utils.decorators.flexible_decorator","title":"<code>flexible_decorator(maybe_decorator=None)</code>","text":"<p>Decorator to allow bracket-less when no arguments are passed.</p> <p>Examples:</p> <p>For decorator defined as follows:</p> Python Console Session<pre><code>&gt;&gt;&gt; @flexible_decorator\n... def decorator(*args, **kwargs):\n...     def wrapper(func, *args, **kwargs):\n...         pass\n...     return wrapper\n</code></pre> <p>The following two are equivalent:</p> Python Console Session<pre><code>&gt;&gt;&gt; @decorator\n... def func(*args, **kwargs):\n...     pass\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; @decorator()\n... def func(*args, **kwargs):\n...     pass\n</code></pre> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>def flexible_decorator(maybe_decorator: Optional[Callable] = None):\n    r\"\"\"\n    Decorator to allow bracket-less when no arguments are passed.\n\n    Examples:\n        For decorator defined as follows:\n\n        &gt;&gt;&gt; @flexible_decorator\n        ... def decorator(*args, **kwargs):\n        ...     def wrapper(func, *args, **kwargs):\n        ...         pass\n        ...     return wrapper\n\n        The following two are equivalent:\n\n        &gt;&gt;&gt; @decorator\n        ... def func(*args, **kwargs):\n        ...     pass\n\n        &gt;&gt;&gt; @decorator()\n        ... def func(*args, **kwargs):\n        ...     pass\n    \"\"\"\n\n    def decorator(func: Callable):\n        @wraps(decorator)\n        def wrapper(*args, **kwargs):\n            if len(args) == 1 and isfunction(args[0]):\n                return func(**kwargs)(args[0])\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    if maybe_decorator is None:\n        return decorator\n    return decorator(maybe_decorator)\n</code></pre>"},{"location":"utils/decorators/#danling.utils.decorators.method_cache","title":"<code>method_cache(*cache_args, **lru_kwargs)</code>","text":"<p>Decorator to cache the result of an instance method.</p> <p><code>functools.lru_cache</code> uses a strong reference to the instance, which will make the instance immortal and break the garbage collection.</p> <p><code>method_cache</code> uses a weak reference to the instance and works fine.</p> <p>https://rednafi.github.io/reflections/dont-wrap-instance-methods-with-functoolslru_cache-decorator-in-python.html</p> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>def method_cache(*cache_args, **lru_kwargs):\n    r\"\"\"\n    Decorator to cache the result of an instance method.\n\n    `functools.lru_cache` uses a strong reference to the instance,\n    which will make the instance immortal and break the garbage collection.\n\n    `method_cache` uses a weak reference to the instance and works fine.\n\n    https://rednafi.github.io/reflections/dont-wrap-instance-methods-with-functoolslru_cache-decorator-in-python.html\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            self_ref = ref(self)\n\n            @wraps(func)\n            @lru_cache(*cache_args, **lru_kwargs)\n            def cached_method(*args, **kwargs):\n                return func(self_ref(), *args, **kwargs)\n\n            setattr(self, func.__name__, cached_method)\n            return cached_method(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"utils/decorators/#danling.utils.decorators.print_exc","title":"<code>print_exc(exc, func, args, kwargs, verbosity=40)</code>","text":"<p>Print exception raised by <code>func</code> with <code>args</code> and <code>kwargs</code> to <code>stderr</code>. This function serves as the default callback for catch.</p> <p>Parameters:</p> Name Type Description Default <code>verbosity</code> <code>int</code> <p>What level of traceback to print. 0-: No traceback. 0-10: Full information of arguments and key word arguments. 10-20: Stack trace to function calls. 40+: Function name and error messages.</p> <code>40</code> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>def print_exc(exc, func, args, kwargs, verbosity: int = 40):  # pylint: disable=W0613\n    r\"\"\"\n    Print exception raised by `func` with `args` and `kwargs` to `stderr`.\n    This function serves as the default callback for catch.\n\n    Args:\n        verbosity: What level of traceback to print.\n            0-: No traceback.\n            0-10: Full information of arguments and key word arguments.\n            10-20: Stack trace to function calls.\n            40+: Function name and error messages.\n    \"\"\"\n\n    if verbosity &gt;= 0:\n        message = traceback.format_exc()\n        message += f\"\\nencoutered when calling {func}\"\n        if verbosity &lt;= 20:\n            message += \"\\n\\nstack:\\n\" + \"\\n\".join(traceback.format_stack()[:-2])\n        if verbosity &lt;= 10:\n            message += \"\\n\" + f\"args: {args}\\nkwargs: {kwargs}\"\n        try:\n            print(message, file=stderr, force=True)  # type: ignore\n        except TypeError:\n            print(message, file=stderr)\n</code></pre>"},{"location":"utils/io/","title":"IO","text":""},{"location":"utils/io/#danling.utils.io.is_json_serializable","title":"<code>is_json_serializable(obj)</code>","text":"<p>Check if <code>obj</code> is JSON serializable.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def is_json_serializable(obj: Any) -&gt; bool:\n    r\"\"\"\n    Check if `obj` is JSON serializable.\n    \"\"\"\n    try:\n        json.dumps(obj)\n        return True\n    except (TypeError, OverflowError):\n        return False\n</code></pre>"},{"location":"utils/io/#danling.utils.io.load","title":"<code>load(file, *args, **kwargs)</code>","text":"<p>Load any file with supported extensions.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def load(file: PathStr, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n    r\"\"\"\n    Load any file with supported extensions.\n    \"\"\"\n    if not os.path.isfile(file):\n        raise ValueError(f\"Trying to load {file!r} but it is not a file.\")\n    extension = os.path.splitext(file)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if not TORCH_AVAILABLE:\n            raise ImportError(f\"Trying to load {file!r} but torch is not installed.\")\n        return torch.load(file, *args, **kwargs)\n    if extension in NUMPY:\n        if not NUMPY_AVAILABLE:\n            raise ImportError(f\"Trying to load {file!r} but numpy is not installed.\")\n        return numpy.load(file, *args, **kwargs)\n    if extension in CSV:\n        if not PANDAS_AVAILABLE:\n            raise ImportError(f\"Trying to load {file!r} but pandas is not installed.\")\n        return pandas.read_csv(file, *args, **kwargs)\n    if extension in JSON:\n        with open(file) as fp:\n            return json.load(fp, *args, **kwargs)  # type: ignore\n    if extension in YAML:\n        with open(file) as fp:\n            return yaml.load(fp, *args, **kwargs)  # type: ignore\n    if extension in PICKLE:\n        with open(file, \"rb\") as fp:\n            return pickle.load(fp, *args, **kwargs)  # type: ignore\n    raise ValueError(f\"Tying to load {file!r} with unsupported extension={extension!r}\")\n</code></pre>"},{"location":"utils/io/#danling.utils.io.save","title":"<code>save(obj, file, *args, **kwargs)</code>","text":"<p>Save any file with supported extensions.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def save(obj: Any, file: PathStr, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; File:\n    r\"\"\"\n    Save any file with supported extensions.\n    \"\"\"\n    extension = os.path.splitext(file)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if not TORCH_AVAILABLE:\n            raise ImportError(f\"Trying to save {obj} to {file!r} but torch is not installed.\")\n        torch.save(obj, file, *args, **kwargs)\n    elif extension in NUMPY:\n        if not NUMPY_AVAILABLE:\n            raise ImportError(f\"Trying to save {obj} to {file!r} but numpy is not installed.\")\n        numpy.save(file, obj, *args, **kwargs)\n    elif extension in CSV:\n        if isinstance(obj, pandas.DataFrame):\n            obj.to_csv(file, *args, **kwargs)\n        else:\n            raise NotImplementedError(f\"Trying to save {obj} to {file!r} but is not supported\")\n    elif extension in JSON:\n        if isinstance(obj, FlatDict):\n            obj.json(file)\n        else:\n            with open(file, \"w\") as fp:\n                json.dump(obj, fp, *args, **kwargs)  # type: ignore\n    elif extension in YAML:\n        if isinstance(obj, FlatDict):\n            obj.yaml(file)\n        else:\n            with open(file, \"w\") as fp:\n                yaml.dump(obj, fp, *args, **kwargs)  # type: ignore\n    elif extension in PICKLE:\n        with open(file, \"wb\") as fp:\n            pickle.dump(obj, fp, *args, **kwargs)  # type: ignore\n    else:\n        raise ValueError(f\"Tying to save {obj} to {file!r} with unsupported extension={extension!r}\")\n    return file\n</code></pre>"}]}