{"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\-]","pipeline":["stemmer"]},"docs":[{"location":"","title":"DanLing","text":""},{"location":"#introduction","title":"Introduction","text":"<p>DanLing (\u200b\u4e39\u7075\u200b) is a high-level library to help with running neural networks flexibly and transparently.</p> <p>DanLing is meant to be a scaffold for experienced researchers and engineers who know how to define a training loop, but are bored of writing the same boilerplate code, such as DDP, logging, checkpointing, etc., over and over again.</p> <p>Therefore, DanLing does not feature complex Runner designs with many pre-defined methods and complicated hooks. Instead, the Runner of DanLing just initialise the essential parts for you, and you can do whatever you want, however you want.</p> <p>Although many attributes and properties are pre-defined and are expected to be used in DanLing, you have full control over your code.</p> <p>DanLing also provides some utilities, such as <code>NestedTensor</code>, <code>LRScheduler</code>, <code>catch</code>, etc.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the most recent stable version on pypi:</p> Bash<pre><code>pip install danling\n</code></pre> <p>Install the latest version from source:</p> Bash<pre><code>pip install git+https://github.com/ZhiyuanChen/DanLing\n</code></pre> <p>It works the way it should have worked.</p>"},{"location":"#license","title":"License","text":"<p>DanLing is multi-licensed under the following licenses:</p> <ul> <li>The Unlicense</li> <li>GNU Affero General Public License v3.0 or later</li> <li>GNU General Public License v2.0 or later</li> <li>BSD 4-Clause \u201cOriginal\u201d or \u201cOld\u201d License</li> <li>MIT License</li> <li>Apache License 2.0</li> </ul> <p>You can choose any (one or more) of these licenses if you use this work.</p> <p><code>SPDX-License-Identifier: Unlicense OR AGPL-3.0-or-later OR GPL-2.0-or-later OR BSD-4-Clause OR MIT OR Apache-2.0</code></p>"},{"location":"package/","title":"DanLing","text":""},{"location":"package/#danling.AccelerateRunner","title":"<code>AccelerateRunner</code>","text":"<p>               Bases: <code>BaseRunner</code></p> <p>Set up everything for running a job.</p> <p><code>AccelerateRunner</code> uses <code>accelerate</code> as distributed backend to provide seamless distributed training experience.</p> <p><code>AccelerateRunner</code> will automatically <code>prepare</code> everything, including <code>model</code>, <code>criterion</code>, <code>optimizer</code>, <code>scheduler</code>, and <code>dataloaders</code> for distribute training, mixed precision, and deepspeed (optional).</p> <p>In fact, you don\u2019t even need to create <code>dataloaders</code>, just define <code>datasets</code> and <code>AccelerateRunner</code> will create <code>dataloaders</code> for you. <code>AccelerateRunner</code> will inspect the <code>train</code> flag in corresponding dataset to automatically set <code>shuffle</code>.</p> <p>Attributes:</p> Name Type Description <code>accelerator</code> <code>Accelerator</code> <code>accelerate</code> <code>dict</code> <p>Arguments to pass when building accelerator. Defaults to <code>{}</code>.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>class AccelerateRunner(BaseRunner):  # pylint: disable=too-many-public-methods\n    r\"\"\"\n    Set up everything for running a job.\n\n    `AccelerateRunner` uses [`accelerate`][accelerate] as distributed backend to\n    provide seamless distributed training experience.\n\n    `AccelerateRunner` will automatically [`prepare`][accelerate.Accelerator.prepare] everything,\n    including `model`, `criterion`, `optimizer`, `scheduler`, and `dataloaders` for distribute training,\n    mixed precision, and deepspeed (optional).\n\n    In fact, you don't even need to create `dataloaders`, just define\n    `datasets` and `AccelerateRunner` will create `dataloaders` for you.\n    `AccelerateRunner` will inspect the `train` flag in corresponding dataset to\n    automatically set `shuffle`.\n\n    Attributes:\n        accelerator (Accelerator):\n        accelerate: Arguments to pass when building accelerator. Defaults to `{}`.\n    \"\"\"\n\n    accelerator: Accelerator\n    accelerate: dict\n\n    model: nn.Module\n    criterion: nn.Module\n    optimizer: optim.Optimizer\n    scheduler: optim.lr_scheduler._LRScheduler\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        if len(args) != 1 or kwargs:\n            message = (\n                \"Passing multiple args &amp; kwargs to build Runner is deprecated and will be removed in DanLing v0.3.\\n\"\n                \"Please only pass a config dict instead.\"\n            )\n            warn(message, DeprecationWarning, stacklevel=2)\n            config = NestedDict(*args, **kwargs)\n        else:\n            config = args[0]\n        if \"accelerate\" not in self:  # class attributes\n            self.accelerate = {}\n        self.accelerate.update(config.get(\"accelerate\", {}))\n        super().__init__(config)\n\n    def __post_init__(self) -&gt; None:\n        self.model, self.criterion, self.optimizer = self.prepare(self.model, self.criterion, self.optimizer)\n        self.scheduler = self.prepare(self.scheduler)\n        if self.datasets:\n            datasets = {k: d for k, d in self.datasets.items() if k not in self.dataloaders}\n            default_kwargs = self.state.setdefault(\"dataloader\", NestedDict())\n            dataloader_kwargs = NestedDict({k: default_kwargs.pop(k) for k in self.datasets if k in default_kwargs})\n            for k, d in datasets.items():\n                dataloader_kwargs.setdefault(k, NestedDict())\n                dataloader_kwargs[k].merge(default_kwargs, overwrite=False)\n                dataloader_kwargs[k].setdefault(\"shuffle\", getattr(d, \"train\", True))\n                dataloader_kwargs[k].setdefault(\"drop_last\", not getattr(d, \"train\", True))\n                self.dataloaders[k] = utils.data.DataLoader(d, **dataloader_kwargs[k])\n            default_kwargs.update(dataloader_kwargs)\n        for k, d in self.dataloaders.items():\n            self.dataloaders[k] = self.prepare(d)\n        if self.state.get(\"log_interval\") is None:\n            self.state.log_interval = max(len(d) for d in self.dataloaders.values()) // 10\n\n    @property\n    def deepspeed(self) -&gt; dict | None:\n        if \"accelerator\" not in self:\n            raise ValueError(\"accelerator is not used\")\n        if self.accelerator.state.deepspeed_plugin is not None:\n            return self.accelerator.state.deepspeed_plugin.deepspeed_config\n        return None\n\n    def train(self, train_splits: list[str] | None = None, eval_splits: list[str] | None = None) -&gt; NestedDict:\n        r\"\"\"\n        Perform training on `split`.\n\n        Args:\n            train_splits (list[str]): list of split to run train.\n                Defaults to `[\"train\"]`.\n            eval_splits (list[str]): list of split to run evaluate.\n                Defaults to `self.dataloaders` except for those in `train_splits`.\n\n        Return:\n            NestedDict: train results\n        \"\"\"\n\n        early_stop_counter = 0\n        if train_splits is None:\n            train_splits = [\"train\"]\n        if eval_splits is None:\n            eval_splits = [s for s in self.dataloaders if s not in train_splits]\n        self.state.epoch_begin = self.state.epochs\n        print(f\"Begin training from {self.state.epoch_begin} to {self.state.epoch_end}\")\n        print(f\"Training splits: {train_splits}\")\n        print(f\"Evaluation splits: {eval_splits}\")\n        patience = self.state.get(\"patience\", float(\"inf\"))\n        for epochs in range(self.state.epoch_begin, self.state.epoch_end):  # type: ignore\n            self.state.epochs = epochs\n            result = NestedDict()\n            result.setattr(\"convert_mapping\", True)\n            for split in train_splits:\n                result[split] = self.train_epoch(split)\n            for split in eval_splits:\n                result[split] = self.evaluate_epoch(split)\n            self.append_result(result)\n            print(self.format_epoch_result(result))\n            self.save_result()\n            if self.state.save_interval is not None:\n                self.save_checkpoint(epochs)\n            \"\"\"@nni.report_intermediate_result(self.latest_score)\"\"\"\n            early_stop_counter = 0 if self.is_best else early_stop_counter + 1\n            if early_stop_counter &gt; patience:\n                print(\"early stop\")\n                break\n        \"\"\"@nni.report_final_result(self.latest_score)\"\"\"\n        return self.results\n\n    def train_epoch(self, split: str = \"train\") -&gt; NestedDict:\n        r\"\"\"\n        Train one epoch on `split`.\n\n        Args:\n            split (str): split to run train\n\n        Return:\n            NestedDict: train result\n        \"\"\"\n\n        self.mode = \"train\"  # type: ignore\n        self.split = split\n        loader = self.dataloaders[split]\n        length = len(loader) - 1\n        last_print_iteration = -1\n        log_interval = self.state.get(\"log_interval\", -1)\n        self.meters.reset()\n        if self.metrics is not None:\n            self.metrics.reset()\n        batch_time = time()\n        if hasattr(loader.batch_sampler, \"set_epoch\"):\n            loader.batch_sampler.set_epoch(self.epochs)\n        if hasattr(loader.sampler, \"set_epoch\"):\n            loader.sampler.set_epoch(self.epochs)\n\n        for iteration, data in enumerate(loader):\n            with self.autocast(), self.accumulate():\n                input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n                target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n                pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n                loss = self.criterion(pred, target)\n                if self.metrics is not None:\n                    self.metrics.update(pred.squeeze(-1), target)\n                self.step(loss)\n\n            if log_interval &gt; 0 and (iteration &gt; 0 and iteration % log_interval == 0 or iteration == length):\n                interval = iteration - last_print_iteration\n                if self.device == torch.device(\"cuda\"):\n                    torch.cuda.synchronize()\n                self.meters.time.update((time() - batch_time) / interval)\n                batch_time = time()\n                reduced_loss = self.reduce(loss).item()\n                self.meters.loss.update(reduced_loss)\n                self.step_log(split, iteration, length)\n                last_print_iteration = iteration\n\n        result = self.meters.average()\n        if self.metrics is not None:\n            result.merge(self.metrics.average())\n        return result\n\n    def evaluate(self, eval_splits: list[str] | None = None) -&gt; NestedDict:\n        r\"\"\"\n        Perform evaluation on `eval_splits`.\n\n        Args:\n            eval_splits (list[str]): list of split to run evaluate.\n                Defaults to `[\"eval\"]`.\n\n        Return:\n            NestedDict: evaluation result\n        \"\"\"\n\n        if eval_splits is None:\n            eval_splits = [\"eval\"]\n\n        print(\"Begin evaluation\")\n        print(f\"Evaluation splits: {eval_splits}\")\n        result = NestedDict()\n        result.setattr(\"convert_mapping\", True)\n        for split in eval_splits:\n            result[split] = self.evaluate_epoch(split=split)\n        print(self.format_epoch_result(result))\n        return result\n\n    @torch.inference_mode()\n    def evaluate_epoch(self, split: str = \"val\") -&gt; NestedDict:\n        r\"\"\"\n        Evaluate one epoch on `split`.\n\n        Args:\n            split (str): split to run evaluate\n\n        Return:\n            NestedDict: evaluation result\n        \"\"\"\n\n        self.mode = \"eval\"  # type: ignore\n        self.split = split\n        loader = self.dataloaders[split]\n        length = len(loader) - 1\n        last_print_iteration = -1\n        log_interval = self.state.get(\"log_interval\", -1)\n        self.meters.reset()\n        if self.metrics is not None:\n            self.metrics.reset()\n        batch_time = time()\n\n        for iteration, data in enumerate(loader):\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            loss = self.criterion(pred, target)\n            if self.metrics is not None:\n                self.metrics.update(pred.squeeze(-1), target)\n\n            if log_interval &gt; 0 and (iteration &gt; 0 and iteration % log_interval == 0 or iteration == length):\n                interval = iteration - last_print_iteration\n                if self.device == torch.device(\"cuda\"):\n                    torch.cuda.synchronize()\n                self.meters.time.update((time() - batch_time) / interval)\n                batch_time = time()\n                reduced_loss = self.reduce(loss).item()\n                self.meters.loss.update(reduced_loss)\n                self.step_log(split, iteration, length)\n                last_print_iteration = iteration\n\n        result = self.meters.average()\n        if self.metrics is not None:\n            result.merge(self.metrics.average())\n        self.write_result(result, split, self.state.epochs)\n        return result\n\n    @torch.inference_mode()\n    def inference(self, split: str = \"inf\") -&gt; list:\n        r\"\"\"\n        Perform inference on `split`.\n\n        Args:\n            split (str): split to run inference\n\n        Return:\n            Tensor: inference outputs\n        \"\"\"\n\n        # pylint: disable=E1102, W0622\n        self.mode = \"inf\"  # type: ignore\n        loader = self.dataloaders[split]\n        self.meters.reset()\n        output = []\n        for _, data in tqdm(enumerate(loader), total=len(loader)):\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            output.extend(pred.squeeze(-1).tolist())\n\n        if self.distributed:\n            torch.cuda.synchronize()\n            output = self.gather_for_metrics(output)\n        return output\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Set up distributed training.\n\n        Initialise process group and set up DDP variables.\n        \"\"\"\n\n        if os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\").lower() == \"true\":\n            deepspeed_config = self.state.get(\"deepspeed\", os.environ.get(\"ACCELERATE_DEEPSPEED_CONFIG_FILE\"))\n            self.accelerate[\"deepspeed_plugin\"] = DeepSpeedPlugin(hf_ds_config=self.init_deepspeed(deepspeed_config))\n        self.accelerator = Accelerator(**self.accelerate)\n        if self.distributed:\n            object_list = [self.id, self.timestamp]\n            dist.broadcast_object_list(object_list)\n            self.id, self.timestamp = object_list\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n        if \"log_dir\" not in kwargs:\n            kwargs[\"log_dir\"] = self.dir\n\n        self.writer = SummaryWriter(*args, **kwargs)\n        self.writer.add_scalar = catch(OSError, verbose=False)(self.writer.add_scalar)\n\n    def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Args:\n            seed: Random seed to set.\n                Defaults to `self.state.seed` (`config.seed`).\n\n            bias: Make the seed different for each processes.\n                This is used to ensure the data augmentation are applied differently on every processes.\n                Defaults to `self.rank`.\n                Set to `False` to disable this feature.\n        \"\"\"\n\n        seed = seed or self.state.seed\n        if self.distributed:\n            object_list = [seed]\n            dist.broadcast_object_list(object_list)\n            seed = object_list[0]\n        bias = bias or self.rank\n        if bias:\n            seed += bias\n        self.state.seed = seed\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        if np_random is not None:\n            np_random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n        if torch.__version__ &gt;= \"1.8.0\":\n            torch.use_deterministic_algorithms(True)\n\n    def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n        r\"\"\"\n        Backward loss and step optimizer &amp; scheduler.\n\n        This method increment `self.state.steps`.\n\n        This method also increment `self.state.iters` when `batch_size` is specified.\n\n        Args:\n            zero_grad: Whether to zero the gradients.\n        \"\"\"\n\n        self.accelerator.backward(loss)\n        if self.sync_gradients:\n            if self.state.get(\"max_grad_value\") is not None:\n                self.clip_grad_value_(self.model.parameters(), self.state.get(\"max_grad_value\"))\n            if self.state.get(\"max_grad_norm\") is not None:\n                self.clip_grad_norm_(self.model.parameters(), self.state.get(\"max_grad_norm\"))\n        if self.optimizer is not None:\n            self.optimizer.step()\n            if zero_grad:\n                self.optimizer.zero_grad()\n        if self.scheduler is not None:\n            self.scheduler.step()\n        self.state.steps += 1\n        if batch_size is None:\n            batch_size = self.batch_size_equivalent\n        self.state.iters += batch_size\n        # TODO: Support `drop_last = False`\n        # self.state.iters += self.batch_size_equivalent\n\n    def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Return dict of all attributes for checkpoint.\n        \"\"\"\n\n        if self.model is None:\n            raise ValueError(\"Model must be defined when calling state_dict\")\n        model = self.accelerator.unwrap_model(self.model)\n        return cls(\n            runner=self.state.dict(),\n            model=model.state_dict(),\n            optimizer=self.optimizer.state_dict() if self.optimizer else None,\n            scheduler=self.scheduler.state_dict() if self.scheduler else None,\n        )\n\n    def prepare(self, *args: list[Any], device_placement: list[bool] | None = None) -&gt; list[Any]:\n        r\"\"\"\n        Prepare all objects passed in `args` for distributed training and mixed precision,\n        then return them in the same order.\n        \"\"\"\n\n        return self.accelerator.prepare(*args, device_placement=device_placement)\n\n    def accumulate(self, model: nn.Module | None = None):\n        r\"\"\"\n        Context manager that enables gradient accumulate.\n        \"\"\"\n\n        model = model or self.model\n        return self.accelerator.accumulate(model)\n\n    def autocast(self):\n        r\"\"\"\n        Context manager that enables auto-casting for the forward pass (and maybe backward pass).\n        \"\"\"\n\n        return self.accelerator.autocast()\n\n    def backward(self, loss) -&gt; None:\n        r\"\"\"\n        Backward loss to compute gradients.\n        \"\"\"\n\n        return self.accelerator.backward(loss)\n\n    def unwrap_model(self, model: nn.Module | None = None) -&gt; nn.Module:\n        r\"\"\"\n        Unwrap DDP model.\n\n        Args:\n            model (Optional[nn.Module]):\n                Defaults to `self.model`.\n        \"\"\"\n\n        if model is not None:\n            model = self.model\n        if self.accelerator is not None:\n            return self.accelerator.unwrap_model(model)\n        if self.distributed:\n            return model.module\n        return model\n\n    @property\n    def mode(self) -&gt; RunnerMode:\n        return self._mode\n\n    @mode.setter\n    def mode(self, mode: str | RunnerMode) -&gt; None:\n        if isinstance(mode, str):\n            mode = RunnerMode(mode)\n        self._mode = mode\n        if self.model is not None:\n            self.model.train(mode == RunnerMode.train)\n\n    @property\n    def batch_size(self) -&gt; int:\n        r\"\"\"\n        Batch size.\n\n        Notes:\n            If `train` is in `dataloaders`, then `batch_size` is the batch size of `train`.\n            Otherwise, `batch_size` is the batch size of the first dataloader.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        batch_size = self.state.get(\"dataloader.batch_size\")\n        if batch_size:\n            return batch_size\n        if self.dataloaders:\n            loader = self.dataloaders.get(\"train\", next(iter(self.dataloaders.values())))\n            if loader.batch_size:\n                return loader.batch_size\n            batch_sampler = loader.batch_sampler if loader.batch_sampler is not None else loader.sampler\n            return batch_sampler.batch_size\n        raise AttributeError(\"batch_size could not be inferred, since no dataloader found.\")\n\n    @property\n    def accum_steps(self) -&gt; int:\n        r\"\"\"\n        Gradient accumulation steps.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        return self.accelerator.gradient_accumulation_steps\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of runner.\n        \"\"\"\n\n        return self.accelerator.device\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n        \"\"\"\n\n        return self.accelerator.num_processes\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n        \"\"\"\n\n        return self.accelerator.process_index\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n        \"\"\"\n\n        return self.accelerator.local_process_index\n\n    def gather(self, tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Gather tensor.\n        \"\"\"\n\n        return self.accelerator.gather(tensor)\n\n    def reduce(self, tensor, reduction: str = \"sum\") -&gt; torch.Tensor:\n        r\"\"\"\n        Reduce tensor.\n        \"\"\"\n\n        return self.accelerator.reduce(tensor, reduction=reduction)\n\n    def __getattr__(self, name: str) -&gt; Any:\n        with suppress(AttributeError):\n            return super().__getattr__(name)\n        if \"accelerator\" in self.__dict__ and hasattr(self.accelerator, name):\n            return getattr(self.accelerator, name)\n        raise super().__getattribute__(name)\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.accum_steps","title":"<code>accum_steps: int</code>  <code>property</code>","text":"<p>Gradient accumulation steps.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"package/#danling.AccelerateRunner.batch_size","title":"<code>batch_size: int</code>  <code>property</code>","text":"<p>Batch size.</p> Notes <p>If <code>train</code> is in <code>dataloaders</code>, then <code>batch_size</code> is the batch size of <code>train</code>. Otherwise, <code>batch_size</code> is the batch size of the first dataloader.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"package/#danling.AccelerateRunner.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Device of runner.</p>"},{"location":"package/#danling.AccelerateRunner.local_rank","title":"<code>local_rank: int</code>  <code>property</code>","text":"<p>Process index in local processes.</p>"},{"location":"package/#danling.AccelerateRunner.rank","title":"<code>rank: int</code>  <code>property</code>","text":"<p>Process index in all processes.</p>"},{"location":"package/#danling.AccelerateRunner.world_size","title":"<code>world_size: int</code>  <code>property</code>","text":"<p>Number of Processes.</p>"},{"location":"package/#danling.AccelerateRunner.accumulate","title":"<code>accumulate(model=None)</code>","text":"<p>Context manager that enables gradient accumulate.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def accumulate(self, model: nn.Module | None = None):\n    r\"\"\"\n    Context manager that enables gradient accumulate.\n    \"\"\"\n\n    model = model or self.model\n    return self.accelerator.accumulate(model)\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.autocast","title":"<code>autocast()</code>","text":"<p>Context manager that enables auto-casting for the forward pass (and maybe backward pass).</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def autocast(self):\n    r\"\"\"\n    Context manager that enables auto-casting for the forward pass (and maybe backward pass).\n    \"\"\"\n\n    return self.accelerator.autocast()\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.backward","title":"<code>backward(loss)</code>","text":"<p>Backward loss to compute gradients.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def backward(self, loss) -&gt; None:\n    r\"\"\"\n    Backward loss to compute gradients.\n    \"\"\"\n\n    return self.accelerator.backward(loss)\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.evaluate","title":"<code>evaluate(eval_splits=None)</code>","text":"<p>Perform evaluation on <code>eval_splits</code>.</p> <p>Parameters:</p> Name Type Description Default <code>eval_splits</code> <code>list[str]</code> <p>list of split to run evaluate. Defaults to <code>[\"eval\"]</code>.</p> <code>None</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def evaluate(self, eval_splits: list[str] | None = None) -&gt; NestedDict:\n    r\"\"\"\n    Perform evaluation on `eval_splits`.\n\n    Args:\n        eval_splits (list[str]): list of split to run evaluate.\n            Defaults to `[\"eval\"]`.\n\n    Return:\n        NestedDict: evaluation result\n    \"\"\"\n\n    if eval_splits is None:\n        eval_splits = [\"eval\"]\n\n    print(\"Begin evaluation\")\n    print(f\"Evaluation splits: {eval_splits}\")\n    result = NestedDict()\n    result.setattr(\"convert_mapping\", True)\n    for split in eval_splits:\n        result[split] = self.evaluate_epoch(split=split)\n    print(self.format_epoch_result(result))\n    return result\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.evaluate_epoch","title":"<code>evaluate_epoch(split='val')</code>","text":"<p>Evaluate one epoch on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run evaluate</p> <code>'val'</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>@torch.inference_mode()\ndef evaluate_epoch(self, split: str = \"val\") -&gt; NestedDict:\n    r\"\"\"\n    Evaluate one epoch on `split`.\n\n    Args:\n        split (str): split to run evaluate\n\n    Return:\n        NestedDict: evaluation result\n    \"\"\"\n\n    self.mode = \"eval\"  # type: ignore\n    self.split = split\n    loader = self.dataloaders[split]\n    length = len(loader) - 1\n    last_print_iteration = -1\n    log_interval = self.state.get(\"log_interval\", -1)\n    self.meters.reset()\n    if self.metrics is not None:\n        self.metrics.reset()\n    batch_time = time()\n\n    for iteration, data in enumerate(loader):\n        input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n        target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n        pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n        loss = self.criterion(pred, target)\n        if self.metrics is not None:\n            self.metrics.update(pred.squeeze(-1), target)\n\n        if log_interval &gt; 0 and (iteration &gt; 0 and iteration % log_interval == 0 or iteration == length):\n            interval = iteration - last_print_iteration\n            if self.device == torch.device(\"cuda\"):\n                torch.cuda.synchronize()\n            self.meters.time.update((time() - batch_time) / interval)\n            batch_time = time()\n            reduced_loss = self.reduce(loss).item()\n            self.meters.loss.update(reduced_loss)\n            self.step_log(split, iteration, length)\n            last_print_iteration = iteration\n\n    result = self.meters.average()\n    if self.metrics is not None:\n        result.merge(self.metrics.average())\n    self.write_result(result, split, self.state.epochs)\n    return result\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.gather","title":"<code>gather(tensor)</code>","text":"<p>Gather tensor.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def gather(self, tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Gather tensor.\n    \"\"\"\n\n    return self.accelerator.gather(tensor)\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.inference","title":"<code>inference(split='inf')</code>","text":"<p>Perform inference on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run inference</p> <code>'inf'</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>@torch.inference_mode()\ndef inference(self, split: str = \"inf\") -&gt; list:\n    r\"\"\"\n    Perform inference on `split`.\n\n    Args:\n        split (str): split to run inference\n\n    Return:\n        Tensor: inference outputs\n    \"\"\"\n\n    # pylint: disable=E1102, W0622\n    self.mode = \"inf\"  # type: ignore\n    loader = self.dataloaders[split]\n    self.meters.reset()\n    output = []\n    for _, data in tqdm(enumerate(loader), total=len(loader)):\n        input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n        pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n        output.extend(pred.squeeze(-1).tolist())\n\n    if self.distributed:\n        torch.cuda.synchronize()\n        output = self.gather_for_metrics(output)\n    return output\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.init_distributed","title":"<code>init_distributed()</code>","text":"<p>Set up distributed training.</p> <p>Initialise process group and set up DDP variables.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def init_distributed(self) -&gt; None:\n    r\"\"\"\n    Set up distributed training.\n\n    Initialise process group and set up DDP variables.\n    \"\"\"\n\n    if os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\").lower() == \"true\":\n        deepspeed_config = self.state.get(\"deepspeed\", os.environ.get(\"ACCELERATE_DEEPSPEED_CONFIG_FILE\"))\n        self.accelerate[\"deepspeed_plugin\"] = DeepSpeedPlugin(hf_ds_config=self.init_deepspeed(deepspeed_config))\n    self.accelerator = Accelerator(**self.accelerate)\n    if self.distributed:\n        object_list = [self.id, self.timestamp]\n        dist.broadcast_object_list(object_list)\n        self.id, self.timestamp = object_list\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.init_tensorboard","title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n    self.writer.add_scalar = catch(OSError, verbose=False)(self.writer.add_scalar)\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.prepare","title":"<code>prepare(*args, device_placement=None)</code>","text":"<p>Prepare all objects passed in <code>args</code> for distributed training and mixed precision, then return them in the same order.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def prepare(self, *args: list[Any], device_placement: list[bool] | None = None) -&gt; list[Any]:\n    r\"\"\"\n    Prepare all objects passed in `args` for distributed training and mixed precision,\n    then return them in the same order.\n    \"\"\"\n\n    return self.accelerator.prepare(*args, device_placement=device_placement)\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.reduce","title":"<code>reduce(tensor, reduction='sum')</code>","text":"<p>Reduce tensor.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def reduce(self, tensor, reduction: str = \"sum\") -&gt; torch.Tensor:\n    r\"\"\"\n    Reduce tensor.\n    \"\"\"\n\n    return self.accelerator.reduce(tensor, reduction=reduction)\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.set_deterministic","title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    if torch.__version__ &gt;= \"1.8.0\":\n        torch.use_deterministic_algorithms(True)\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.set_seed","title":"<code>set_seed(seed=None, bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Random seed to set. Defaults to <code>self.state.seed</code> (<code>config.seed</code>).</p> <code>None</code> <code>bias</code> <code>int | None</code> <p>Make the seed different for each processes. This is used to ensure the data augmentation are applied differently on every processes. Defaults to <code>self.rank</code>. Set to <code>False</code> to disable this feature.</p> <code>None</code> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Args:\n        seed: Random seed to set.\n            Defaults to `self.state.seed` (`config.seed`).\n\n        bias: Make the seed different for each processes.\n            This is used to ensure the data augmentation are applied differently on every processes.\n            Defaults to `self.rank`.\n            Set to `False` to disable this feature.\n    \"\"\"\n\n    seed = seed or self.state.seed\n    if self.distributed:\n        object_list = [seed]\n        dist.broadcast_object_list(object_list)\n        seed = object_list[0]\n    bias = bias or self.rank\n    if bias:\n        seed += bias\n    self.state.seed = seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    if np_random is not None:\n        np_random.seed(seed)\n    random.seed(seed)\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.state_dict","title":"<code>state_dict(cls=dict)</code>","text":"<p>Return dict of all attributes for checkpoint.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Return dict of all attributes for checkpoint.\n    \"\"\"\n\n    if self.model is None:\n        raise ValueError(\"Model must be defined when calling state_dict\")\n    model = self.accelerator.unwrap_model(self.model)\n    return cls(\n        runner=self.state.dict(),\n        model=model.state_dict(),\n        optimizer=self.optimizer.state_dict() if self.optimizer else None,\n        scheduler=self.scheduler.state_dict() if self.scheduler else None,\n    )\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.step","title":"<code>step(loss, batch_size=None, zero_grad=True)</code>","text":"<p>Backward loss and step optimizer &amp; scheduler.</p> <p>This method increment <code>self.state.steps</code>.</p> <p>This method also increment <code>self.state.iters</code> when <code>batch_size</code> is specified.</p> <p>Parameters:</p> Name Type Description Default <code>zero_grad</code> <code>bool</code> <p>Whether to zero the gradients.</p> <code>True</code> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n    r\"\"\"\n    Backward loss and step optimizer &amp; scheduler.\n\n    This method increment `self.state.steps`.\n\n    This method also increment `self.state.iters` when `batch_size` is specified.\n\n    Args:\n        zero_grad: Whether to zero the gradients.\n    \"\"\"\n\n    self.accelerator.backward(loss)\n    if self.sync_gradients:\n        if self.state.get(\"max_grad_value\") is not None:\n            self.clip_grad_value_(self.model.parameters(), self.state.get(\"max_grad_value\"))\n        if self.state.get(\"max_grad_norm\") is not None:\n            self.clip_grad_norm_(self.model.parameters(), self.state.get(\"max_grad_norm\"))\n    if self.optimizer is not None:\n        self.optimizer.step()\n        if zero_grad:\n            self.optimizer.zero_grad()\n    if self.scheduler is not None:\n        self.scheduler.step()\n    self.state.steps += 1\n    if batch_size is None:\n        batch_size = self.batch_size_equivalent\n    self.state.iters += batch_size\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.train","title":"<code>train(train_splits=None, eval_splits=None)</code>","text":"<p>Perform training on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>train_splits</code> <code>list[str]</code> <p>list of split to run train. Defaults to <code>[\"train\"]</code>.</p> <code>None</code> <code>eval_splits</code> <code>list[str]</code> <p>list of split to run evaluate. Defaults to <code>self.dataloaders</code> except for those in <code>train_splits</code>.</p> <code>None</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def train(self, train_splits: list[str] | None = None, eval_splits: list[str] | None = None) -&gt; NestedDict:\n    r\"\"\"\n    Perform training on `split`.\n\n    Args:\n        train_splits (list[str]): list of split to run train.\n            Defaults to `[\"train\"]`.\n        eval_splits (list[str]): list of split to run evaluate.\n            Defaults to `self.dataloaders` except for those in `train_splits`.\n\n    Return:\n        NestedDict: train results\n    \"\"\"\n\n    early_stop_counter = 0\n    if train_splits is None:\n        train_splits = [\"train\"]\n    if eval_splits is None:\n        eval_splits = [s for s in self.dataloaders if s not in train_splits]\n    self.state.epoch_begin = self.state.epochs\n    print(f\"Begin training from {self.state.epoch_begin} to {self.state.epoch_end}\")\n    print(f\"Training splits: {train_splits}\")\n    print(f\"Evaluation splits: {eval_splits}\")\n    patience = self.state.get(\"patience\", float(\"inf\"))\n    for epochs in range(self.state.epoch_begin, self.state.epoch_end):  # type: ignore\n        self.state.epochs = epochs\n        result = NestedDict()\n        result.setattr(\"convert_mapping\", True)\n        for split in train_splits:\n            result[split] = self.train_epoch(split)\n        for split in eval_splits:\n            result[split] = self.evaluate_epoch(split)\n        self.append_result(result)\n        print(self.format_epoch_result(result))\n        self.save_result()\n        if self.state.save_interval is not None:\n            self.save_checkpoint(epochs)\n        \"\"\"@nni.report_intermediate_result(self.latest_score)\"\"\"\n        early_stop_counter = 0 if self.is_best else early_stop_counter + 1\n        if early_stop_counter &gt; patience:\n            print(\"early stop\")\n            break\n    \"\"\"@nni.report_final_result(self.latest_score)\"\"\"\n    return self.results\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.train_epoch","title":"<code>train_epoch(split='train')</code>","text":"<p>Train one epoch on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run train</p> <code>'train'</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def train_epoch(self, split: str = \"train\") -&gt; NestedDict:\n    r\"\"\"\n    Train one epoch on `split`.\n\n    Args:\n        split (str): split to run train\n\n    Return:\n        NestedDict: train result\n    \"\"\"\n\n    self.mode = \"train\"  # type: ignore\n    self.split = split\n    loader = self.dataloaders[split]\n    length = len(loader) - 1\n    last_print_iteration = -1\n    log_interval = self.state.get(\"log_interval\", -1)\n    self.meters.reset()\n    if self.metrics is not None:\n        self.metrics.reset()\n    batch_time = time()\n    if hasattr(loader.batch_sampler, \"set_epoch\"):\n        loader.batch_sampler.set_epoch(self.epochs)\n    if hasattr(loader.sampler, \"set_epoch\"):\n        loader.sampler.set_epoch(self.epochs)\n\n    for iteration, data in enumerate(loader):\n        with self.autocast(), self.accumulate():\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            loss = self.criterion(pred, target)\n            if self.metrics is not None:\n                self.metrics.update(pred.squeeze(-1), target)\n            self.step(loss)\n\n        if log_interval &gt; 0 and (iteration &gt; 0 and iteration % log_interval == 0 or iteration == length):\n            interval = iteration - last_print_iteration\n            if self.device == torch.device(\"cuda\"):\n                torch.cuda.synchronize()\n            self.meters.time.update((time() - batch_time) / interval)\n            batch_time = time()\n            reduced_loss = self.reduce(loss).item()\n            self.meters.loss.update(reduced_loss)\n            self.step_log(split, iteration, length)\n            last_print_iteration = iteration\n\n    result = self.meters.average()\n    if self.metrics is not None:\n        result.merge(self.metrics.average())\n    return result\n</code></pre>"},{"location":"package/#danling.AccelerateRunner.unwrap_model","title":"<code>unwrap_model(model=None)</code>","text":"<p>Unwrap DDP model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[Module]</code> <p>Defaults to <code>self.model</code>.</p> <code>None</code> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def unwrap_model(self, model: nn.Module | None = None) -&gt; nn.Module:\n    r\"\"\"\n    Unwrap DDP model.\n\n    Args:\n        model (Optional[nn.Module]):\n            Defaults to `self.model`.\n    \"\"\"\n\n    if model is not None:\n        model = self.model\n    if self.accelerator is not None:\n        return self.accelerator.unwrap_model(model)\n    if self.distributed:\n        return model.module\n    return model\n</code></pre>"},{"location":"package/#danling.AverageMeter","title":"<code>AverageMeter</code>","text":"<p>Computes and stores the average and current value.</p> <p>Attributes:</p> Name Type Description <code>val</code> <code>float</code> <p>Results of current batch on current device.</p> <code>bat</code> <p>Results of current batch on all devices.</p> <code>avg</code> <p>Results of all results on all devices.</p> <code>sum</code> <code>float</code> <p>Sum of values.</p> <code>count</code> <code>float</code> <p>Number of values.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\nnan\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>class AverageMeter:\n    r\"\"\"\n    Computes and stores the average and current value.\n\n    Attributes:\n        val: Results of current batch on current device.\n        bat: Results of current batch on all devices.\n        avg: Results of all results on all devices.\n        sum: Sum of values.\n        count: Number of values.\n\n    Examples:\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        nan\n    \"\"\"\n\n    val: float = 0\n    n: float = 1\n    sum: float = 0\n    count: float = 0\n\n    def __init__(self) -&gt; None:\n        self.reset()\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets the meter.\n\n        Examples:\n            &gt;&gt;&gt; meter = AverageMeter()\n            &gt;&gt;&gt; meter.update(0.7)\n            &gt;&gt;&gt; meter.val\n            0.7\n            &gt;&gt;&gt; meter.avg\n            0.7\n            &gt;&gt;&gt; meter.reset()\n            &gt;&gt;&gt; meter.val\n            0\n            &gt;&gt;&gt; meter.avg\n            nan\n        \"\"\"\n\n        self.val = 0\n        self.n = 1\n        self.sum = 0\n        self.count = 0\n\n    def update(self, value, n: float = 1) -&gt; None:\n        r\"\"\"\n        Updates the average and current value in the meter.\n\n        Args:\n            value: Value to be added to the average.\n            n: Number of values to be added.\n\n        Examples:\n            &gt;&gt;&gt; meter = AverageMeter()\n            &gt;&gt;&gt; meter.update(0.7)\n            &gt;&gt;&gt; meter.val\n            0.7\n            &gt;&gt;&gt; meter.avg\n            0.7\n            &gt;&gt;&gt; meter.update(0.9)\n            &gt;&gt;&gt; meter.val\n            0.9\n            &gt;&gt;&gt; meter.avg\n            0.8\n            &gt;&gt;&gt; meter.sum\n            1.6\n            &gt;&gt;&gt; meter.count\n            2\n        \"\"\"\n\n        self.val = value\n        self.n = n\n        self.sum += value * n\n        self.count += n\n\n    def value(self):\n        return self.val\n\n    def batch(self):\n        world_size = get_world_size()\n        if world_size == 1:\n            return self.val / self.n if self.n != 0 else float(\"nan\")\n        synced_tuple = [None for _ in range(world_size)]\n        dist.all_gather_object(synced_tuple, (self.val * self.n, self.n))\n        val, n = zip(*synced_tuple)\n        count = sum(n)\n        if count == 0:\n            return float(\"nan\")\n        return sum(val) / count\n\n    def average(self):\n        world_size = get_world_size()\n        if world_size == 1:\n            return self.sum / self.count if self.count != 0 else float(\"nan\")\n        synced_tuple = [None for _ in range(world_size)]\n        dist.all_gather_object(synced_tuple, (self.sum, self.count))\n        val, n = zip(*synced_tuple)\n        count = sum(n)\n        if count == 0:\n            return float(\"nan\")\n        return sum(val) / count\n\n    @property\n    def bat(self):\n        return self.batch()\n\n    @property\n    def avg(self):\n        return self.average()\n\n    def __format__(self, format_spec) -&gt; str:\n        return f\"{self.val.__format__(format_spec)} ({self.avg.__format__(format_spec)})\"\n</code></pre>"},{"location":"package/#danling.AverageMeter.reset","title":"<code>reset()</code>","text":"<p>Resets the meter.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\nnan\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets the meter.\n\n    Examples:\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        nan\n    \"\"\"\n\n    self.val = 0\n    self.n = 1\n    self.sum = 0\n    self.count = 0\n</code></pre>"},{"location":"package/#danling.AverageMeter.update","title":"<code>update(value, n=1)</code>","text":"<p>Updates the average and current value in the meter.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>Value to be added to the average.</p> required <code>n</code> <code>float</code> <p>Number of values to be added.</p> <code>1</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def update(self, value, n: float = 1) -&gt; None:\n    r\"\"\"\n    Updates the average and current value in the meter.\n\n    Args:\n        value: Value to be added to the average.\n        n: Number of values to be added.\n\n    Examples:\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n    \"\"\"\n\n    self.val = value\n    self.n = n\n    self.sum += value * n\n    self.count += n\n</code></pre>"},{"location":"package/#danling.AverageMeters","title":"<code>AverageMeters</code>","text":"<p>               Bases: <code>MetricsDict</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.update({\"loss\": 0.6, \"auroc\": 0.7, \"r2\": 0.8})\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.6000 (0.6000)\nauroc: 0.7000 (0.7000)\nr2: 0.8000 (0.8000)\n&gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.9000 (0.7500)\nauroc: 0.7000 (0.7000)\nr2: 0.8000 (0.8000)\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 1.5, 'auroc': 0.7, 'r2': 0.8}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 2, 'auroc': 1, 'r2': 1}\n&gt;&gt;&gt; meters.reset()\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.0000 (nan)\nauroc: 0.0000 (nan)\nr2: 0.0000 (nan)\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>class AverageMeters(MetricsDict):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; meters = AverageMeters()\n        &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"auroc\": 0.7, \"r2\": 0.8})\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.6000 (0.6000)\n        auroc: 0.7000 (0.7000)\n        r2: 0.8000 (0.8000)\n        &gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.9000 (0.7500)\n        auroc: 0.7000 (0.7000)\n        r2: 0.8000 (0.8000)\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 1.5, 'auroc': 0.7, 'r2': 0.8}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 2, 'auroc': 1, 'r2': 1}\n        &gt;&gt;&gt; meters.reset()\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.0000 (nan)\n        auroc: 0.0000 (nan)\n        r2: 0.0000 (nan)\n    \"\"\"\n\n    def __init__(self, *args, default_factory=AverageMeter, **kwargs) -&gt; None:\n        super().__init__(*args, default_factory=default_factory, **kwargs)\n\n    @property\n    def sum(self) -&gt; FlatDict[str, float]:\n        return FlatDict({key: meter.sum for key, meter in self.all_items()})\n\n    @property\n    def count(self) -&gt; FlatDict[str, int]:\n        return FlatDict({key: meter.count for key, meter in self.all_items()})\n\n    def update(self, values: Dict, *, n: int = 1) -&gt; None:  # pylint: disable=W0237\n        r\"\"\"\n        Updates the average and current value in all meters.\n\n        Args:\n            values: Dict of values to be added to the average.\n            n: Number of values to be added.\n\n        Raises:\n            ValueError: If the value is not an instance of (int, float, Mapping).\n\n        Examples:\n            &gt;&gt;&gt; meters = AverageMeters()\n            &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"auroc\": 0.7, \"r2\": 0.8})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 0.6, 'auroc': 0.7, 'r2': 0.8}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 1, 'auroc': 1, 'r2': 1}\n            &gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 1.5, 'auroc': 0.7, 'r2': 0.8}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 2, 'auroc': 1, 'r2': 1}\n            &gt;&gt;&gt; meters.update({\"loss\": 0.8, \"auroc\": 0.9, \"r2\": 0.8})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 2.3, 'auroc': 1.6, 'r2': 1.6}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 3, 'auroc': 2, 'r2': 2}\n            &gt;&gt;&gt; meters.update({\"auroc\": 0.7, \"r2\": 0.7})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 2.3, 'auroc': 2.3, 'r2': 2.3}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 3, 'auroc': 3, 'r2': 3}\n            &gt;&gt;&gt; meters.update({\"dataset1\": {\"cls.auroc\": 0.9}, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n            Traceback (most recent call last):\n            ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'dict'&gt;\n            This is likely due to nested dictionary in the values.\n            Nested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\n            &gt;&gt;&gt; meters.update(dict(loss=\"\"))\n            Traceback (most recent call last):\n            ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'str'&gt;\n        \"\"\"  # noqa: E501\n\n        for meter, value in values.items():\n            if isinstance(value, (int, float)):\n                self[meter].update(value, n)\n            elif isinstance(value, Dict):\n                value.setdefault(\"n\", n)\n                try:\n                    self[meter].update(**value)\n                except TypeError:\n                    raise ValueError(\n                        f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\\n\"\n                        \"This is likely due to nested dictionary in the values.\\n\"\n                        \"Nested dictionaries cannot be processed due to the method's design, which uses Mapping \"\n                        \"to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\"\n                    ) from None\n            else:\n                raise ValueError(f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\")\n</code></pre>"},{"location":"package/#danling.AverageMeters.update","title":"<code>update(values, *, n=1)</code>","text":"<p>Updates the average and current value in all meters.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict</code> <p>Dict of values to be added to the average.</p> required <code>n</code> <code>int</code> <p>Number of values to be added.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is not an instance of (int, float, Mapping).</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.update({\"loss\": 0.6, \"auroc\": 0.7, \"r2\": 0.8})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 0.6, 'auroc': 0.7, 'r2': 0.8}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 1, 'auroc': 1, 'r2': 1}\n&gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 1.5, 'auroc': 0.7, 'r2': 0.8}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 2, 'auroc': 1, 'r2': 1}\n&gt;&gt;&gt; meters.update({\"loss\": 0.8, \"auroc\": 0.9, \"r2\": 0.8})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 2.3, 'auroc': 1.6, 'r2': 1.6}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 3, 'auroc': 2, 'r2': 2}\n&gt;&gt;&gt; meters.update({\"auroc\": 0.7, \"r2\": 0.7})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 2.3, 'auroc': 2.3, 'r2': 2.3}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 3, 'auroc': 3, 'r2': 3}\n&gt;&gt;&gt; meters.update({\"dataset1\": {\"cls.auroc\": 0.9}, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\nTraceback (most recent call last):\nValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'dict'&gt;\nThis is likely due to nested dictionary in the values.\nNested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\n&gt;&gt;&gt; meters.update(dict(loss=\"\"))\nTraceback (most recent call last):\nValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'str'&gt;\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def update(self, values: Dict, *, n: int = 1) -&gt; None:  # pylint: disable=W0237\n    r\"\"\"\n    Updates the average and current value in all meters.\n\n    Args:\n        values: Dict of values to be added to the average.\n        n: Number of values to be added.\n\n    Raises:\n        ValueError: If the value is not an instance of (int, float, Mapping).\n\n    Examples:\n        &gt;&gt;&gt; meters = AverageMeters()\n        &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"auroc\": 0.7, \"r2\": 0.8})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 0.6, 'auroc': 0.7, 'r2': 0.8}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 1, 'auroc': 1, 'r2': 1}\n        &gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 1.5, 'auroc': 0.7, 'r2': 0.8}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 2, 'auroc': 1, 'r2': 1}\n        &gt;&gt;&gt; meters.update({\"loss\": 0.8, \"auroc\": 0.9, \"r2\": 0.8})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 2.3, 'auroc': 1.6, 'r2': 1.6}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 3, 'auroc': 2, 'r2': 2}\n        &gt;&gt;&gt; meters.update({\"auroc\": 0.7, \"r2\": 0.7})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 2.3, 'auroc': 2.3, 'r2': 2.3}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 3, 'auroc': 3, 'r2': 3}\n        &gt;&gt;&gt; meters.update({\"dataset1\": {\"cls.auroc\": 0.9}, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n        Traceback (most recent call last):\n        ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'dict'&gt;\n        This is likely due to nested dictionary in the values.\n        Nested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\n        &gt;&gt;&gt; meters.update(dict(loss=\"\"))\n        Traceback (most recent call last):\n        ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'str'&gt;\n    \"\"\"  # noqa: E501\n\n    for meter, value in values.items():\n        if isinstance(value, (int, float)):\n            self[meter].update(value, n)\n        elif isinstance(value, Dict):\n            value.setdefault(\"n\", n)\n            try:\n                self[meter].update(**value)\n            except TypeError:\n                raise ValueError(\n                    f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\\n\"\n                    \"This is likely due to nested dictionary in the values.\\n\"\n                    \"Nested dictionaries cannot be processed due to the method's design, which uses Mapping \"\n                    \"to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\"\n                ) from None\n        else:\n            raise ValueError(f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\")\n</code></pre>"},{"location":"package/#danling.BaseRunner","title":"<code>BaseRunner</code>","text":"<p>Base class for all runners.</p> <p><code>BaseRunner</code> sets up basic running environment, including <code>seed</code>, <code>deterministic</code>, and <code>logging</code>.</p> <p><code>BaseRunner</code> also provides some basic methods, such as, <code>step</code>, <code>state_dict</code>, <code>save_checkpoint</code>, <code>load_checkpoint</code>.</p> <p><code>BaseRunner</code> defines all basic attributes and relevant properties such as <code>scores</code>, <code>progress</code>, etc.</p> <p>ID:</p> Name Type Description <code>timestamp</code> <code>str</code> <p>A time string representing the creation time of run.</p> <code>name</code> <code>str</code> <p><code>f\"{self.state.experiment_name}-{self.state.run_name}\"</code>.</p> <code>id</code> <code>str</code> <p><code>f\"{self.state.experiment_id:.8}{self.state.run_id:.8}\"</code>.</p> <code>uuid</code> <code>(UUID, property)</code> <p><code>uuid5(self.state.run_id, self.id)</code>.</p> <p>Core:</p> Name Type Description <code>mode</code> <code>(RunnerMode, property)</code> <p>Running mode.</p> <code>state</code> <code>RunnerState</code> <p>Running state. See <code>RunnerState</code> for details.</p> <p>Model:</p> Name Type Description <code>model</code> <code>Callable</code> <code>criterion</code> <code>Callable</code> <code>optimizer</code> <code>Any | None</code> <code>scheduler</code> <code>Any | None</code> <p>Data:</p> Name Type Description <code>datasets</code> <code>FlatDict</code> <p>All datasets, should be in the form of <code>{subset: dataset}</code>. Initialised to <code>FlatDict</code> by default.</p> <code>datasamplers</code> <code>FlatDict</code> <p>All datasamplers, should be in the form of <code>{subset: datasampler}</code>. Initialised to <code>FlatDict</code> by default.</p> <code>dataloaders</code> <code>FlatDict</code> <p>All dataloaders, should be in the form of <code>{subset: dataloader}</code>. Initialised to <code>FlatDict</code> by default.</p> <code>split</code> <code>str</code> <p>Current running split.</p> <code>batch_size</code> <code>(int, property)</code> <p>Number of samples per batch in current running split.</p> <code>batch_size_equivalent</code> <code>(int, property)</code> <p>Total batch_size (<code>batch_size * world_size * accum_steps</code>).</p> <p><code>datasets</code>, <code>datasamplers</code>, <code>dataloaders</code> should be a dict with the same keys. Their keys should be <code>split</code> (e.g. <code>train</code>, <code>val</code>, <code>test</code>).</p> <p>Progress:</p> Name Type Description <code>progress</code> <code>(float, property)</code> <p>Running Progress, in <code>range(0, 1)</code>.</p> <p>Results:</p> Name Type Description <code>results</code> <code>NestedDict</code> <p>Results include all metric information of the model. Results should be in the form of <code>{epoch: {subset: {metric: score}}}</code>.</p> <code>latest_result</code> <code>(NestedDict, property)</code> <p>Most recent result, should be in the form of <code>{subset: {metric: score}}</code>.</p> <code>best_result</code> <code>(NestedDict, property)</code> <p>Best result, should be in the form of <code>{subset: {metric: score}}</code>.</p> <code>scores</code> <code>(List[float], property)</code> <p>Score is the core metric that is used to evaluate the performance of the model. Scores should be in the form of <code>{epoch: score}</code>.</p> <code>latest_score</code> <code>(float, property)</code> <p>Most recent score, should be in the form of <code>score</code>.</p> <code>best_score</code> <code>(float, property)</code> <p>Best score, should be in the form of <code>score</code>.</p> <code>score_split</code> <code>Optional[str]</code> <p>The subset to calculate the score. If is <code>None</code>, will use the last set of the result.</p> <code>score_name</code> <code>str</code> <p>The metric name of score. Defaults to <code>\"loss\"</code>.</p> <code>is_best</code> <code>(bool, property)</code> <p>If <code>latest_score == best_score</code>.</p> <p>A <code>result</code> is a dict with the same <code>split</code> as keys, like <code>dataloaders</code>. A typical <code>result</code> shall look like this: Python<pre><code>{\n    \"train\": {\n        \"loss\": 0.1,\n        \"accuracy\": 0.9,\n    },\n    \"val\": {\n        \"loss\": 0.2,\n        \"accuracy\": 0.8,\n    },\n    \"test\": {\n        \"loss\": 0.3,\n        \"accuracy\": 0.7,\n    },\n}\n</code></pre></p> <p><code>scores</code> are dynamically extracted from <code>results</code> by <code>score_split</code> and <code>score_name</code>. They represent the core metric that is used in comparing the performance against different models and settings. For the above <code>results</code>, If <code>score_split = \"val\"</code>, <code>score_name = \"accuracy\"</code>, then <code>scores = 0.9</code>.</p> <p>IO:</p> Name Type Description <code>dir</code> <code>(str, property)</code> <p>Directory of the run. Defaults to <code>${self.project_root}/${self.name}-${self.id}/${self.timestamp})</code>.</p> <code>checkpoint_dir</code> <code>(str, property)</code> <p>Directory of checkpoints.</p> <code>log_path</code> <code>(str, property)</code> <p>Path of log file.</p> <code>checkpoint_dir_name</code> <code>str</code> <p>The name of the directory under <code>runner.dir</code> to save checkpoints. Defaults to <code>\"checkpoints\"</code>.</p> <p>Parallel Training:</p> Name Type Description <code>world_size</code> <code>(int, property)</code> <p>Number of processes.</p> <code>rank</code> <code>(int, property)</code> <p>Process index of all processes.</p> <code>local_rank</code> <code>(int, property)</code> <p>Process index of local processes.</p> <code>distributed</code> <code>(bool, property)</code> <p>If runner is running in distributed mode.</p> <code>is_main_process</code> <code>(bool, property)</code> <p>If current process is the main process of all processes.</p> <code>is_local_main_process</code> <code>(bool, property)</code> <p>If current process is the main process of local processes.</p> <p>logging:</p> Name Type Description <code>meters</code> <code>AverageMeters | MultiTaskAverageMeters</code> <p>Average meters. Initialised to <code>AverageMeters</code> by default.</p> <code>metrics</code> <code>Metrics | MultiTaskMetrics | None</code> <p>Metrics for evaluating.</p> <code>logger</code> <code>Logger | None</code> <code>writer</code> <code>Any | None</code> See Also <p><code>RunnerState</code>: The runeer base that stores runtime information. <code>BaseRunner</code>: The base runner class.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>class BaseRunner(metaclass=RunnerMeta):  # pylint: disable=too-many-public-methods\n    r\"\"\"\n    Base class for all runners.\n\n    `BaseRunner` sets up basic running environment, including `seed`, `deterministic`, and `logging`.\n\n    `BaseRunner` also provides some basic methods, such as, `step`, `state_dict`, `save_checkpoint`, `load_checkpoint`.\n\n    `BaseRunner` defines all basic attributes and relevant properties such as `scores`, `progress`, etc.\n\n    Attributes: ID:\n        timestamp (str): A time string representing the creation time of run.\n        name (str): `f\"{self.state.experiment_name}-{self.state.run_name}\"`.\n        id (str): `f\"{self.state.experiment_id:.8}{self.state.run_id:.8}\"`.\n        uuid (UUID, property): `uuid5(self.state.run_id, self.id)`.\n\n    Attributes: Core:\n        mode (RunnerMode, property): Running mode.\n        state (RunnerState): Running state. See `RunnerState` for details.\n\n    Attributes: Model:\n        model (Callable):\n        criterion (Callable):\n        optimizer:\n        scheduler:\n\n    Attributes: Data:\n        datasets (FlatDict): All datasets, should be in the form of ``{subset: dataset}``.\n            Initialised to `FlatDict` by default.\n        datasamplers (FlatDict): All datasamplers, should be in the form of ``{subset: datasampler}``.\n            Initialised to `FlatDict` by default.\n        dataloaders (FlatDict): All dataloaders, should be in the form of ``{subset: dataloader}``.\n            Initialised to `FlatDict` by default.\n        split (str): Current running split.\n        batch_size (int, property): Number of samples per batch in current running split.\n        batch_size_equivalent (int, property): Total batch_size (`batch_size * world_size * accum_steps`).\n\n    `datasets`, `datasamplers`, `dataloaders` should be a dict with the same keys.\n    Their keys should be `split` (e.g. `train`, `val`, `test`).\n\n    Attributes: Progress:\n        progress (float, property): Running Progress, in `range(0, 1)`.\n\n    Attributes: Results:\n        results (NestedDict): Results include all metric information of the model.\n            Results should be in the form of `{epoch: {subset: {metric: score}}}`.\n        latest_result (NestedDict, property): Most recent result, should be in the form of `{subset: {metric: score}}`.\n        best_result (NestedDict, property): Best result, should be in the form of `{subset: {metric: score}}`.\n        scores (List[float], property): Score is the core metric that is used to evaluate the performance of the model.\n            Scores should be in the form of `{epoch: score}`.\n        latest_score (float, property): Most recent score, should be in the form of `score`.\n        best_score (float, property): Best score, should be in the form of `score`.\n        score_split (Optional[str]): The subset to calculate the score.\n            If is `None`, will use the last set of the result.\n        score_name (str): The metric name of score.\n            Defaults to `\"loss\"`.\n        is_best (bool, property): If `latest_score == best_score`.\n\n    A `result` is a dict with the same `split` as keys, like `dataloaders`.\n    A typical `result` shall look like this:\n    ```python\n    {\n        \"train\": {\n            \"loss\": 0.1,\n            \"accuracy\": 0.9,\n        },\n        \"val\": {\n            \"loss\": 0.2,\n            \"accuracy\": 0.8,\n        },\n        \"test\": {\n            \"loss\": 0.3,\n            \"accuracy\": 0.7,\n        },\n    }\n    ```\n\n    `scores` are dynamically extracted from `results` by `score_split` and `score_name`.\n    They represent the core metric that is used in comparing the performance against different models and settings.\n    For the above `results`, If `score_split = \"val\"`, `score_name = \"accuracy\"`, then `scores = 0.9`.\n\n    Attributes: IO:\n        dir (str, property): Directory of the run.\n            Defaults to `${self.project_root}/${self.name}-${self.id}/${self.timestamp})`.\n        checkpoint_dir (str, property): Directory of checkpoints.\n        log_path (str, property):  Path of log file.\n        checkpoint_dir_name (str): The name of the directory under `runner.dir` to save checkpoints.\n            Defaults to `\"checkpoints\"`.\n\n    Attributes: Parallel Training:\n        world_size (int, property): Number of processes.\n        rank (int, property): Process index of all processes.\n        local_rank (int, property): Process index of local processes.\n        distributed (bool, property): If runner is running in distributed mode.\n        is_main_process (bool, property): If current process is the main process of all processes.\n        is_local_main_process (bool, property): If current process is the main process of local processes.\n\n    Attributes: logging:\n        meters (AverageMeters | MultiTaskAverageMeters): Average meters.\n            Initialised to `AverageMeters` by default.\n        metrics (Metrics | MultiTaskMetrics | None): Metrics for evaluating.\n        logger:\n        writer:\n\n    See Also:\n        [`RunnerState`][danling.runner.runner_state.RunnerState]: The runeer base that stores runtime information.\n        [`BaseRunner`][danling.runner.BaseRunner]: The base runner class.\n    \"\"\"\n\n    # DO NOT set default value in class, as they won't be stored in `__dict__`.\n\n    timestamp: str\n\n    _mode: RunnerMode\n    _state: RunnerState\n    inited: bool = False\n\n    model: Callable | None = None\n    criterion: Callable | None = None\n    optimizer: Any | None = None\n    scheduler: Any | None = None\n\n    datasets: FlatDict\n    datasamplers: FlatDict\n    dataloaders: FlatDict\n    split: str\n\n    results: NestedDict\n    meters: AverageMeters\n    metrics: Metrics | None = None\n    logger: logging.Logger | None = None\n    writer: Any | None = None\n\n    def __init__(self, config: NestedDict) -&gt; None:\n        self.timestamp = get_time_str()\n        if \"datasets\" not in self.__dict__:\n            self.datasets = FlatDict()\n        if \"datasamplers\" not in self.__dict__:\n            self.datasamplers = FlatDict()\n        if \"dataloaders\" not in self.__dict__:\n            self.dataloaders = FlatDict()\n        if \"results\" not in self.__dict__:\n            self.results = NestedDict()\n        self.meters = AverageMeters()\n        self._mode = RunnerMode.train  # type: ignore[assignment]\n        # must init state at last to avoid name conflicts\n        self._state = RunnerState(config)\n        self.inited = True\n        self.init_distributed()\n        if self.state.seed is not None:\n            self.set_seed()\n        if self.state.deterministic:\n            self.set_deterministic()\n        if self.state.log:\n            self.init_logging()\n        self.init_print()\n        if self.state.tensorboard:\n            self.init_tensorboard()\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Initialise distributed running environment.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def init_deepspeed(  # pylint: disable=too-many-branches, too-many-statements\n        self, config: Dict = None  # type: ignore\n    ) -&gt; Dict:\n        r\"\"\"\n        Preprocess DeepSpeed config.\n        \"\"\"\n\n        if config is None:\n            config = self.state.get(\"deepspeed\")\n        if config is None:\n            return {}\n        if isinstance(config, str):\n            config = NestedDict.load(config)\n        if config.get(\"steps_per_print\", \"auto\") == \"auto\":\n            config[\"steps_per_print\"] = self.state.log_interval\n        if config.get(\"train_micro_batch_size_per_gpu\", \"auto\") == \"auto\":\n            config[\"train_micro_batch_size_per_gpu\"] = self.batch_size\n        if \"amp\" in config:\n            amp = config[\"amp\"]\n            if amp.get(\"enabled\", \"auto\") == \"auto\":\n                amp[\"enabled\"] = \"true\"\n            if amp.get(\"opt_level\", \"auto\") == \"auto\":\n                amp[\"opt_level\"] = \"O1\"\n        if \"zero_optimization\" in config:\n            zero = config[\"zero_optimization\"]\n            if zero.get(\"allgather_bucket_size\") == \"auto\":\n                zero[\"allgather_bucket_size\"] = 1e6\n            if zero.get(\"reduce_bucket_size\") == \"auto\":\n                zero[\"reduce_bucket_size\"] = 1e6\n            if zero.get(\"stage3_max_live_parameters\") == \"auto\":\n                zero[\"stage3_max_live_parameters\"] = 1e8\n            if zero.get(\"stage3_max_live_gradients\") == \"auto\":\n                zero[\"stage3_max_live_gradients\"] = 1e8\n            if zero.get(\"stage3_max_reuse_distance\") == \"auto\":\n                zero[\"stage3_max_reuse_distance\"] = 1e8\n            if zero.get(\"stage3_prefetch_bucket_size\") == \"auto\":\n                zero[\"stage3_prefetch_bucket_size\"] = 1e6\n            if zero.get(\"stage3_param_persistence_threshold\") == \"auto\":\n                zero[\"stage3_param_persistence_threshold\"] = 1e8\n            if \"amp\" in config:\n                if \"fp16\" not in config:\n                    config[\"fp16\"] = {}\n                if config[\"fp16\"].get(\"enabled\", \"auto\"):\n                    config[\"fp16\"][\"enabled\"] = config[\"amp\"][\"enabled\"]\n                warn(\n                    f\"AMP is not compatible with ZeRO. Automatically set 'fp16' to {config['amp']['enabled']}\",\n                    stacklevel=2,\n                )\n                del config[\"amp\"]\n        if \"optimizer\" in config:\n            if \"params\" not in config[\"optimizer\"]:\n                config[\"optimizer\"][\"params\"] = {}\n            optimizer = config[\"optimizer\"][\"params\"]\n            if optimizer.get(\"lr\", \"auto\") == \"auto\":\n                optimizer[\"lr\"] = self.state.get(\"optim.lr\", 1e-3)\n            if optimizer.get(\"weight_decay\", \"auto\") == \"auto\":\n                optimizer[\"weight_decay\"] = self.state.get(\"optim.weight_decay\", 1e-2)\n            if optimizer.get(\"betas\") == \"auto\":\n                optimizer[\"betas\"] = (0.9, 0.999)\n            if optimizer.get(\"eps\") == \"auto\":\n                optimizer[\"eps\"] = 1e-8\n        if \"scheduler\" in config:\n            if \"params\" not in config[\"scheduler\"]:\n                config[\"scheduler\"][\"params\"] = {}\n            scheduler = config[\"scheduler\"][\"params\"]\n            if scheduler.get(\"total_num_steps\", \"auto\") == \"auto\":\n                scheduler[\"total_num_steps\"] = self.total_steps\n            if scheduler.get(\"warmup_num_steps\", \"auto\") == \"auto\":\n                scheduler[\"warmup_num_steps\"] = scheduler[\"total_num_steps\"] // 20\n            if scheduler.get(\"warmup_max_lr\", \"auto\") == \"auto\":\n                if self.optimizer:\n                    scheduler[\"warmup_max_lr\"] = self.optimizer.param_groups[0][\"lr\"]\n                elif \"optimizer\" in config:\n                    scheduler[\"warmup_max_lr\"] = config[\"optimizer\"][\"params\"][\"lr\"]\n                else:\n                    raise ValueError(\"warmup_max_lr is not defined and cannot be inferred\")\n            if scheduler.get(\"warmup_min_lr\", \"auto\") == \"auto\":\n                scheduler[\"warmup_min_lr\"] = 1e-7\n        return config\n\n    @on_main_process\n    def init_logging(self) -&gt; None:\n        r\"\"\"\n        Set up logging.\n        \"\"\"\n\n        os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n        # Why is setting up proper logging so !@?#! ugly?\n        logging.config.dictConfig(\n            {\n                \"version\": 1,\n                \"disable_existing_loggers\": False,\n                \"formatters\": {\n                    \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n                },\n                \"handlers\": {\n                    \"stdout\": {\n                        \"level\": \"INFO\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.StreamHandler\",\n                        \"stream\": \"ext://sys.stdout\",\n                    },\n                    \"logfile\": {\n                        \"level\": \"DEBUG\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.FileHandler\",\n                        \"filename\": self.log_path,\n                        \"mode\": \"a\",\n                    },\n                },\n                \"loggers\": {\n                    \"\": {\n                        \"handlers\": [\"stdout\", \"logfile\"],\n                        \"level\": \"DEBUG\",\n                        \"propagate\": True,\n                    },\n                },\n            }\n        )\n        logging.captureWarnings(True)\n        self.logger = logging.getLogger(\"runner\")\n        self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n\n    def init_print(self, process: int = 0) -&gt; None:\n        r\"\"\"\n        Set up `print`.\n\n        Only print on a specific `process` or when `force = True`.\n\n        Args:\n            process: The process to `print` on.\n\n        Notes\n        -----\n        If `self.state.log = True`, the default `print` function will be override by `logging.info`.\n        \"\"\"\n\n        logger = logging.getLogger(\"print\")\n        logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n        import builtins as __builtin__  # pylint: disable=C0415\n\n        builtin_print = __builtin__.print\n\n        @catch\n        def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=redefined-builtin\n            if self.rank == process or force:\n                if self.state.log:\n                    logger.info(*args, **kwargs)\n                else:\n                    builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n        __builtin__.print = print\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        raise NotImplementedError\n\n    def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Args:\n            seed: Random seed to set.\n                Defaults to `self.state.seed` (`config.seed`).\n\n            bias: Make the seed different for each processes.\n\n                This avoids same data augmentation are applied on every processes.\n\n                Defaults to `self.rank`.\n\n                Set to `False` to disable this feature.\n        \"\"\"\n\n        seed = seed or self.state.seed\n        bias = bias or self.rank\n        if bias:\n            seed += bias\n        if np_random is not None:\n            np_random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def scale_lr(\n        self,\n        lr: float,\n        lr_scale_factor: float | None = None,\n        batch_size_base: int | None = None,\n    ) -&gt; float:\n        r\"\"\"\n        Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n        \"\"\"\n\n        if lr_scale_factor in self.state:\n            lr_scale_factor = self.state.lr_scale_factor\n\n        if lr_scale_factor is None:\n            if batch_size_base is None:\n                batch_size_base = getattr(self, \"batch_size_base\", None)\n                if batch_size_base is None:\n                    raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n            lr_scale_factor = self.batch_size_equivalent / batch_size_base\n        elif batch_size_base is not None:\n            warn(\n                \"batch_size_base will be ignored if lr_scale_factor is specified\", category=RuntimeWarning, stacklevel=2\n            )\n        lr = lr * lr_scale_factor\n        self.state.lr_scale_factor = lr_scale_factor\n        return lr\n\n    def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n        r\"\"\"\n        Backward loss and step optimizer &amp; scheduler.\n\n        This method increment `self.state.steps`.\n\n        This method also increment `self.state.iters` when `batch_size` is specified.\n\n        Args:\n            zero_grad: Whether to zero the gradients.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Return dict of all attributes for checkpoint.\n        \"\"\"\n\n        return cls(self.state)\n\n    def dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Convert state to Mapping.\n\n        Args:\n            cls: Target `clc to convert to.\n        \"\"\"\n\n        return self.state.dict(cls)\n\n    @catch\n    def save(self, obj: Any, file: PathStr, main_process_only: bool = True, *args, **kwargs) -&gt; File:\n        r\"\"\"\n        Save any file with supported extensions.\n\n        `Runner.save` internally calls `dl.save`,\n        but with additional arguments to allow it save only on the main process.\n        Moreover, any error raised by `Runner.save` will be caught and logged.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            return save(obj, file, *args, **kwargs)\n        return file\n\n    @staticmethod\n    def load(file: PathStr, *args, **kwargs) -&gt; Any:\n        r\"\"\"\n        Load any file with supported extensions.\n\n        `Runner.load` is identical to `dl.load`.\n        \"\"\"\n\n        return load(file, *args, **kwargs)\n\n    @catch\n    def json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n        r\"\"\"\n        Dump Runner State to json file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            return self.state.json(file, *args, **kwargs)\n\n    @classmethod\n    def from_json(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from json file.\n\n        This function calls `self.from_jsons()` to construct object from json string.\n        You may overwrite `from_jsons` in case something is not json serializable.\n        \"\"\"\n\n        with FlatDict.open(file) as fp:\n            return cls.from_jsons(fp.read(), *args, **kwargs)\n\n    def jsons(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner State to json string.\n        \"\"\"\n\n        return self.state.jsons(*args, **kwargs)\n\n    @classmethod\n    def from_jsons(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from json string.\n        \"\"\"\n\n        return cls(Config.from_jsons(string, *args, **kwargs))\n\n    @catch\n    def yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n        r\"\"\"\n        Dump Runner State to yaml file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            return self.state.yaml(file, *args, **kwargs)\n\n    @classmethod\n    def from_yaml(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from yaml file.\n\n        This function calls `self.from_yamls()` to construct object from yaml string.\n        You may overwrite `from_yamls` in case something is not yaml serializable.\n        \"\"\"\n\n        with FlatDict.open(file) as fp:\n            return cls.from_yamls(fp.read(), *args, **kwargs)\n\n    def yamls(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner State to yaml string.\n        \"\"\"\n\n        return self.state.yamls(*args, **kwargs)\n\n    @classmethod\n    def from_yamls(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from yaml string.\n        \"\"\"\n\n        return cls(Config.from_yamls(string, *args, **kwargs))\n\n    def check_dir(self, action: str = \"warn\") -&gt; bool:\n        r\"\"\"\n        Check if `self.dir` is not empty.\n\n        Args:\n            action (str): The action to perform if `self.dir` is not empty.\n            Can be one of (\"warn\", \"raise\", \"ignore\"), default is \"warn\".\n        \"\"\"\n\n        if action and action not in (\"warn\", \"raise\", \"ignore\"):\n            raise ValueError(f\"action should be one of warn, raise or ignore, but got {action}\")\n        if os.listdir(self.dir):\n            if action == \"warn\":\n                warn(\n                    f\"Directory `{self.dir}` is not empty\",\n                    category=RuntimeWarning,\n                    stacklevel=2,\n                )\n            if action == \"raise\":\n                raise RuntimeError(f\"Directory `{self.dir}` is not empty\")\n            return False\n        return True\n\n    @catch\n    @on_main_process\n    def save_checkpoint(self, epochs: int | None = None) -&gt; None:\n        r\"\"\"\n        Save checkpoint to `self.checkpoint_dir`.\n\n        The checkpoint will be saved to `self.checkpoint_dir/latest.pth`.\n\n        If `self.state.save_interval` is positive and `self.state.epochs + 1` is a multiple of `save_interval`,\n        the checkpoint will also be copied to `self.checkpoint_dir/epoch-{self.state.epochs}.pth`.\n\n        If `self.is_best` is `True`, the checkpoint will also be copied to `self.checkpoint_dir/best.pth`.\n        \"\"\"\n\n        epochs = epochs or self.state.epochs\n        save_interval = self.state.get(\"save_interval\", -1)\n        latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        self.save(self.state_dict(), latest_path)\n        if save_interval &gt; 0 and (epochs + 1) % save_interval == 0:\n            save_path = os.path.join(self.checkpoint_dir, f\"epoch-{epochs}.pth\")\n            shutil.copy(latest_path, save_path)\n        if self.is_best:\n            best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n            shutil.copy(latest_path, best_path)\n\n    def load_checkpoint(\n        self,\n        checkpoint: Mapping | bytes | str | os.PathLike | None = None,\n        auto_resume: bool | None = None,\n        override_state: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Load info from checkpoint.\n\n        Args:\n            checkpoint: Checkpoint (or its path) to load.\n                Defaults to `self.state.checkpoint`.\n            auto_resume: Automatically resume from latest checkpoint if exists.\n                Defaults to `False`.\n                If is `True` and `checkpoint` is None, will set it to `self.checkpoint_dir/latest.pth`.\n            override_state: If True, override runner state with checkpoint state.\n                Defaults to `False`.\n            *args: Additional arguments to pass to `self.load`.\n            **kwargs: Additional keyword arguments to pass to `self.load`.\n\n        Raises:\n            FileNotFoundError: If `checkpoint` does not exists.\n\n        See Also:\n            [`from_checkpoint`][danling.BaseRunner.from_checkpoint]: Build runner from checkpoint.\n            [`load_pretrained`][danling.BaseRunner.load_pretrained]: Load parameters from pretrained checkpoint.\n        \"\"\"\n\n        checkpoint = checkpoint if checkpoint is not None else self.state.get(\"checkpoint\")\n        auto_resume = auto_resume if auto_resume is not None else self.state.get(\"auto_resume\", False)\n\n        # TODO: Support loading checkpoints in other format\n        if checkpoint is not None:\n            if auto_resume:\n                warn(\n                    \"latest checkpoint is preempted by value specified in checkpoint\",\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n            if isinstance(checkpoint, (bytes, str, os.PathLike)):\n                if not os.path.exists(checkpoint):\n                    raise FileNotFoundError(f\"checkpoint is set to {checkpoint!r} but does not exist\")\n                self.state.checkpoint = checkpoint\n                ckpt = self.load(checkpoint, *args, **kwargs)\n            elif isinstance(checkpoint, Mapping):\n                ckpt = checkpoint\n            else:\n                raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint\")\n        elif auto_resume:\n            checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n            if os.path.exists(checkpoint):\n                self.state.checkpoint = checkpoint\n                ckpt = self.load(checkpoint, *args, **kwargs)\n            else:\n                warn(\"latest checkpoint does not exits\", category=RuntimeWarning, stacklevel=2)\n                return\n        else:\n            raise ValueError(\"checkpoint is not specified and auto_resume is not set to True\")\n\n        # TODO: Wrap state_dict in a dataclass\n        self.state.merge(ckpt[\"runner\"], overwrite=override_state)\n        if self.model is not None and \"model\" in ckpt:\n            model = self.unwrap_model(self.model)\n            model.load_state_dict(ckpt[\"model\"])\n        if self.optimizer is not None and \"optimizer\" in ckpt:\n            self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n        if self.scheduler is not None and \"scheduler\" in ckpt:\n            self.scheduler.load_state_dict(ckpt[\"scheduler\"])\n        self.state.iter_begin = self.state.iters\n        self.state.step_begin = self.state.steps\n        self.state.epoch_begin = self.state.epochs\n\n    @classmethod\n    def from_checkpoint(cls, checkpoint: Mapping | bytes | str | os.PathLike, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Build BaseRunner from checkpoint.\n\n        Args:\n            checkpoint: Checkpoint (or its path) to load.\n            *args: Additional arguments to pass to `cls.load`.\n            **kwargs: Additional keyword arguments to pass to `cls.load`.\n\n        Returns:\n            (BaseRunner):\n        \"\"\"\n\n        if isinstance(checkpoint, (bytes, str, os.PathLike)):\n            ckpt = cls.load(checkpoint, *args, **kwargs)\n        elif isinstance(checkpoint, Mapping):\n            ckpt = checkpoint\n        else:\n            raise ValueError(f\"checkpoint is set to {checkpoint} but is not a valid checkpoint\")\n        runner = cls(**ckpt[\"runner\"])\n        runner.load_checkpoint(ckpt, override_state=False)\n        return runner\n\n    def load_pretrained(self, checkpoint: Mapping | bytes | str | os.PathLike | None = None, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Load parameters from pretrained checkpoint.\n\n        This method only loads the model weights.\n\n        Args:\n            checkpoint: Pretrained checkpoint (or its path) to load.\n                Defaults to `self.state.pretrained`.\n            *args: Additional arguments to pass to `self.load`.\n            **kwargs: Additional keyword arguments to pass to `self.load`.\n\n        Raises:\n            FileNotFoundError: If `checkpoint` does not exists.\n\n        See Also:\n            [`load_checkpoint`][danling.BaseRunner.load_checkpoint]: Load info from checkpoint.\n        \"\"\"\n\n        # TODO: Support loading checkpoints in other format\n        checkpoint = checkpoint if checkpoint is not None else self.state.get(\"pretrained\")\n        if checkpoint is None:\n            raise ValueError(\"pretrained is not specified\")\n        if isinstance(checkpoint, (bytes, str, os.PathLike)):\n            if not os.path.exists(checkpoint):\n                raise FileNotFoundError(f\"pretrained is set to {checkpoint!r} but does not exist\")\n            ckpt = self.load(checkpoint, *args, **kwargs)\n        elif isinstance(checkpoint, Mapping):\n            ckpt = checkpoint\n        else:\n            raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint\")\n        if self.model is not None and \"model\" in ckpt:\n            model = self.unwrap_model(self.model)\n            model.load_state_dict(ckpt[\"model\"])\n        else:\n            raise ValueError(f\"Unable to find model weights in {checkpoint!r}\")\n\n    def append_result(self, result: NestedDict, index: int | None = None) -&gt; None:\n        r\"\"\"\n        Append result to `self.results`.\n\n        Warnings:\n            `self.results` is heavily relied upon for computing metrics.\n\n            Failed to use this method may lead to unexpected behavior.\n        \"\"\"\n\n        if index is None:\n            index = self.state.epochs\n            global __APPEND_RESULT_COUNTER__  # pylint: disable=global-statement\n            __APPEND_RESULT_COUNTER__ += 1\n            if index == 0 and __APPEND_RESULT_COUNTER__ &gt; 1:\n                warn(\n                    \"\"\"\n                    Automatically set index to `self.state.epochs`.\n                    Please ensure `self.state.epochs` updates before calling `append_result`\n                    \"\"\",\n                    category=RuntimeWarning,\n                    stacklevel=2,\n                )\n        if index in self.results:\n            self.results[index].merge(result)\n        else:\n            self.results[index] = result\n\n    def print_result(self) -&gt; None:\n        r\"\"\"\n        Print latest and best result.\n        \"\"\"\n\n        print(f\"latest result: {self.latest_result}\")\n        print(f\"best result: {self.best_result}\")\n\n    def step_log(self, split: str, iteration: int, length: int | None = None):\n        if length is None:\n            length = len(self.dataloaders[split]) - 1\n        result = self.meters.val\n        if self.metrics is not None:\n            result.merge(self.metrics.val)\n        print(self.format_step_result(result, split, iteration, length))\n        if self.mode == \"train\":\n            self.write_result(result, split)\n        return result\n\n    def format_step_result(\n        self, result: NestedDict, split: str, steps: int, length: int, format_spec: str = \".4f\"\n    ) -&gt; str:\n        result = NestedDict(result).clone()\n        repr_str = \"\"\n        if split is not None:\n            if self.mode == \"train\":\n                repr_str = f\"training on {split} \"\n            elif self.mode == \"eval\":\n                repr_str = f\"evaluating on {split} \"\n            else:\n                repr_str = f\"running in {self.mode} mode on {split} \"\n        repr_str += f\"[{steps}/{length}]\\t\"\n        return repr_str + self.format_result(result, format_spec=format_spec)\n\n    def format_epoch_result(\n        self, result: NestedDict, epochs: int | None = None, epoch_end: int | None = None, format_spec: str = \".4f\"\n    ) -&gt; str:\n        result = NestedDict(result).clone()\n        epochs = epochs or self.state.epochs\n        epoch_end = epoch_end or self.state.epoch_end\n        repr_str = f\"epoch [{epochs}/{epoch_end - 1}]\\n\" if epochs is not None and epoch_end else \"\"\n        repr_str += \"\\n\".join([f\"{k}:\\t{self.format_result(v, format_spec=format_spec)}\" for k, v in result.items()])\n        return repr_str\n\n    def format_result(self, result, format_spec: str = \".4f\") -&gt; str:\n        return \"\\t\".join([f\"{k}: {format(v, format_spec)}\" for k, v in result.items()])\n\n    def write_result(self, result: NestedDict, split: str, steps: int | None = None):\n        if steps is None:\n            steps = self.steps\n        for name, score in result.all_items():\n            name = name.replace(\".\", \"/\")\n            if name == \"loss\" and isinstance(score, AverageMeter):\n                score = score.avg\n            if isinstance(score, Sequence):\n                for i, s in enumerate(score):\n                    self.write_score(f\"{name}/{i}\", s, split, steps)\n            elif isinstance(score, Mapping):\n                for k, s in score.items():\n                    self.write_score(f\"{name}/{k}\", s, split, steps)\n            else:\n                self.write_score(name, score, split, steps)\n\n    def write_score(self, name: str, score: float, split: str, steps: int):\n        if self.writer:\n            self.writer.add_scalar(f\"{split}/{name}\", score, steps)\n\n    @catch\n    @on_main_process\n    def save_result(self) -&gt; None:\n        r\"\"\"\n        Save result to `self.dir`.\n\n        This method will save latest and best result to\n        `self.dir/latest.json` and `self.dir/best.json` respectively.\n        \"\"\"\n\n        results_path = os.path.join(self.dir, \"results.json\")\n        self.save(\n            {\n                \"name\": self.name,\n                \"id\": self.id,\n                \"timestamp\": self.timestamp,\n                \"results\": self.results,\n            },\n            results_path,\n            indent=4,\n        )\n        ret = {\"name\": self.name, \"id\": self.id, \"timestamp\": self.timestamp}\n        result = self.latest_result\n        if isinstance(result, FlatDict):\n            result = result.dict()\n        # This is slower but ensure id is the first key\n        if result is not None:\n            ret.update(result)\n        latest_path = os.path.join(self.dir, \"latest.json\")\n        self.save(ret, latest_path, indent=4)\n        if self.is_best:\n            best_path = os.path.join(self.dir, \"best.json\")\n            shutil.copy(latest_path, best_path)\n\n    @cached_property\n    def name(self):\n        if \"name\" in self._state:\n            return self.state[\"name\"]\n        return f\"{self.state.experiment_name}-{self.state.run_name}\"\n\n    @cached_property\n    def id(self):\n        return f\"{self.state.experiment_id:.8}{self.state.run_id:.8}\"\n\n    @cached_property\n    def uuid(self) -&gt; UUID:\n        r\"\"\"\n        UUID of the state.\n        \"\"\"\n\n        return uuid5(self.run_uuid, self.id)\n\n    @property\n    def mode(self) -&gt; RunnerMode:\n        return self._mode\n\n    @mode.setter\n    def mode(self, mode: str | RunnerMode) -&gt; None:\n        if isinstance(mode, str):\n            mode = RunnerMode(mode)\n        self._mode = mode\n\n    @property\n    def state(self) -&gt; RunnerState:\n        return self._state\n\n    @property\n    def batch_size(self) -&gt; int:\n        r\"\"\"\n        Batch size.\n\n        Notes:\n            If `train` is in `dataloaders`, then `batch_size` is the batch size of `train`.\n            Otherwise, `batch_size` is the batch size of the first dataloader.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        if self.dataloaders and self.split:\n            return self.dataloaders[self.split].batch_size\n        batch_size = self.state.get(\"dataloader.batch_size\")\n        if batch_size:\n            return batch_size\n        raise AttributeError(\"batch_size could not be inferred and is not in config\")\n\n    @property\n    def batch_size_equivalent(self) -&gt; int:\n        r\"\"\"\n        Actual batch size.\n\n        Returns:\n            (int): `batch_size` * `world_size` * `accum_steps`\n        \"\"\"\n\n        return self.batch_size * self.world_size * self.accum_steps\n\n    @cached_property\n    def total_epochs(self) -&gt; int:\n        if self.state.epoch_end:\n            return self.state.epoch_end - self.state.epoch_begin + 1\n        raise ValueError(\"epoch_end is not specified\")\n\n    @cached_property\n    def total_steps(self) -&gt; int:\n        if self.state.step_end:\n            return self.state.step_end - self.state.step_begin\n        dataset = self.datasets.get(\"train\", next(iter(self.datasets.values())))\n        return self.total_epochs * ceil(len(dataset) / self.batch_size) + 1\n\n    @cached_property\n    def accum_steps(self) -&gt; int:\n        r\"\"\"\n        Accumulated steps.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        return self.state.get(\"accum_steps\", 1)\n\n    @property\n    def progress(self) -&gt; float:\n        r\"\"\"\n        Training Progress.\n\n        Returns:\n            (float):\n\n        Raises:\n            RuntimeError: If no terminal is defined.\n        \"\"\"\n\n        return self.steps / self.total_steps\n\n    @property\n    def device(self) -&gt; Any:\n        r\"\"\"\n        Device of runner.\n        \"\"\"\n\n        return \"cpu\"\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of processes.\n        \"\"\"\n\n        return 1\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index of all processes.\n        \"\"\"\n\n        return 0\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index of local processes.\n        \"\"\"\n\n        return 0\n\n    @property\n    def distributed(self) -&gt; bool:\n        r\"\"\"\n        If runner is running in distributed mode.\n        \"\"\"\n\n        return self.world_size &gt; 1\n\n    @property\n    def is_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process of all processes.\n        \"\"\"\n\n        return self.rank == 0\n\n    @property\n    def is_local_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process of local processes.\n        \"\"\"\n\n        return self.local_rank == 0\n\n    @property\n    def best_fn(self) -&gt; Callable:\n        r\"\"\"\n        Function to determine the best score from a list of scores.\n\n        By default, the `best_fn` returns `min` if `self.state.score_name` is `loss`,\n        otherwise, returns `max`.\n\n        Subclass can override this method to accommodate needs, such as `min`.\n\n        Returns:\n            (callable):\n        \"\"\"\n\n        return max if self.state.score_name != \"loss\" else min\n\n    @property\n    def best_index(self) -&gt; int:\n        r\"\"\"\n        Find the best index from all scores.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        if not self.scores:\n            return 0\n        values = list(self.scores.values())\n        return self.best_fn(range(len(values)), key=values.__getitem__)\n\n    @property\n    def latest_result(self) -&gt; NestedDict | None:\n        r\"\"\"\n        Latest result.\n        \"\"\"\n\n        if not self.results:\n            return None\n        latest_index = next(reversed(self.results if PY38_PLUS else list(self.results)))  # type: ignore\n        ret = self.results[latest_index].clone()\n        ret[\"index\"] = latest_index\n        return ret\n\n    @property\n    def best_result(self) -&gt; NestedDict | None:\n        r\"\"\"\n        Best result.\n        \"\"\"\n\n        if not self.results:\n            return None\n        best_index = self.best_index\n        ret = self.results[best_index].clone()\n        ret[\"index\"] = best_index\n        return ret\n\n    @property\n    def scores(self) -&gt; FlatDict | None:\n        r\"\"\"\n        All scores.\n\n        Scores are extracted from results by `score_split` and `runner.state.score_name`,\n        following `[r[score_split][self.state.score_name] for r in self.results]`.\n\n        Scores are considered as the index of the performance of the model.\n        It is useful to determine the best model and the best hyper-parameters.\n\n        `score_split` is defined in `self.state.score_split`.\n        If it is not set, `DanLing` will use `val` or `validate` if they appear in the `latest_result`.\n        If `DanLing` still could not find, it will fall back to the second key in the `latest_result`\n        if it contains more that one element, or the first key.\n\n        Note that certain keys are ignored when falling back, they are defined in {IGNORED_SET_NAMES}.\n        \"\"\"\n\n        if not self.results:\n            return None\n        subsets = [i for i in self.latest_result.keys() if i not in IGNORED_SET_NAMES]  # type: ignore\n        score_split = self.state.get(\"score_split\")\n        if score_split is None and \"val\" in subsets:\n            score_split = \"val\"\n        if score_split is None and \"validate\" in subsets:\n            score_split = \"validate\"\n        if score_split is None:\n            score_split = subsets[1] if len(subsets) &gt; 1 else subsets[0]\n        return FlatDict({k: v[score_split][self.state.score_name] for k, v in self.results.items()})\n\n    @property\n    def latest_score(self) -&gt; float | None:\n        r\"\"\"\n        Latest score.\n        \"\"\"\n\n        if not self.results:\n            return None\n        if not PY38_PLUS:\n            return next(reversed(list(self.scores.values())))  # type: ignore\n        return next(reversed(self.scores.values()))  # type: ignore\n\n    @property\n    def best_score(self) -&gt; float | None:\n        r\"\"\"\n        Best score.\n        \"\"\"\n\n        if not self.results:\n            return None\n        return self.scores[self.best_index]  # type: ignore\n\n    @property\n    def is_best(self) -&gt; bool:\n        r\"\"\"\n        If current epoch is the best epoch.\n        \"\"\"\n\n        if not self.results:\n            return True\n        try:\n            return abs(self.latest_score - self.best_score) &lt; 1e-7  # type: ignore\n        except TypeError:\n            return True\n\n    @property\n    @ensure_dir\n    def dir(self) -&gt; str:\n        r\"\"\"\n        Directory of the run.\n        \"\"\"\n\n        if \"dir\" in self.state:\n            return self.state.dir\n        return os.path.join(self.project_root, f\"{self.name}-{self.id}\", self.timestamp)\n\n    @cached_property\n    def log_path(self) -&gt; str:\n        r\"\"\"\n        Path of log file.\n        \"\"\"\n\n        if \"log_path\" in self.state:\n            return self.state.log_path\n        return os.path.join(self.dir, \"run.log\")\n\n    @property\n    @ensure_dir\n    def checkpoint_dir(self) -&gt; str:\n        r\"\"\"\n        Directory of checkpoints.\n        \"\"\"\n\n        if \"checkpoint_dir\" in self.state:\n            return self.state.checkpoint_dir\n        return os.path.join(self.dir, self.state.checkpoint_dir_name)\n\n    # def __getattribute__(self, name) -&gt; Any:\n    #     if name in (\"__class__\", \"__dict__\"):\n    #         return super().__getattribute__(name)\n    #     if name in self.__dict__:\n    #         return self.__dict__[name]\n    #     if name in dir(self):\n    #         return super().__getattribute__(name)\n    #     if \"state\" in self and name in self.state:\n    #         return self.state[name]\n    #     return super().__getattribute__(name)\n\n    def __getattr__(self, name) -&gt; Any:\n        if self.inited:\n            if name in self._state:\n                return self.state[name]\n            if name in dir(self.state):\n                return getattr(self.state, name)\n        return super().__getattribute__(name)\n\n    def __setattr__(self, name, value) -&gt; None:\n        if name in self.__dict__:\n            if isinstance(self.__dict__[name], Variable):\n                self.__dict__[name].set(value)\n            else:\n                self.__dict__[name] = value\n            return\n        if name in dir(self):\n            if isinstance(super().__getattribute__(name), Variable):\n                super().__getattribute__(name).set(value)\n            else:\n                object.__setattr__(self, name, value)\n            return\n        if self.inited:\n            if name in self.state:\n                if isinstance(self.state[name], Variable):\n                    self.state[name].set(value)\n                else:\n                    self.state[name] = value\n                return\n            if name in dir(self.state):\n                setattr(self.state, name, value)\n                return\n        object.__setattr__(self, name, value)\n\n    def __contains__(self, name) -&gt; bool:\n        return name in dir(self) or (\"state\" in self.__dict__ and name in dir(self.state))\n\n    def __repr__(self):\n        lines = []\n        for key, value in self.__dict__.items():\n            value_str = repr(value)\n            value_str = self._add_indent(value_str)\n            lines.append(\"(\" + key + \"): \" + value_str)\n\n        main_str = self.__class__.__name__ + \"(\"\n        if lines:\n            main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n        main_str += \")\"\n        return main_str\n\n    def _add_indent(self, text):\n        lines = text.split(\"\\n\")\n        # don't do anything for single-line stuff\n        if len(lines) == 1:\n            return text\n        first = lines.pop(0)\n        # add 2 spaces to each line but the first\n        lines = [(2 * \" \") + line for line in lines]\n        lines = \"\\n\".join(lines)\n        lines = first + \"\\n\" + lines\n        return lines\n</code></pre>"},{"location":"package/#danling.BaseRunner.accum_steps","title":"<code>accum_steps: int</code>  <code>cached</code> <code>property</code>","text":"<p>Accumulated steps.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"package/#danling.BaseRunner.batch_size","title":"<code>batch_size: int</code>  <code>property</code>","text":"<p>Batch size.</p> Notes <p>If <code>train</code> is in <code>dataloaders</code>, then <code>batch_size</code> is the batch size of <code>train</code>. Otherwise, <code>batch_size</code> is the batch size of the first dataloader.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"package/#danling.BaseRunner.batch_size_equivalent","title":"<code>batch_size_equivalent: int</code>  <code>property</code>","text":"<p>Actual batch size.</p> <p>Returns:</p> Type Description <code>int</code> <p><code>batch_size</code> * <code>world_size</code> * <code>accum_steps</code></p>"},{"location":"package/#danling.BaseRunner.best_fn","title":"<code>best_fn: Callable</code>  <code>property</code>","text":"<p>Function to determine the best score from a list of scores.</p> <p>By default, the <code>best_fn</code> returns <code>min</code> if <code>self.state.score_name</code> is <code>loss</code>, otherwise, returns <code>max</code>.</p> <p>Subclass can override this method to accommodate needs, such as <code>min</code>.</p> <p>Returns:</p> Type Description <code>callable</code>"},{"location":"package/#danling.BaseRunner.best_index","title":"<code>best_index: int</code>  <code>property</code>","text":"<p>Find the best index from all scores.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"package/#danling.BaseRunner.best_result","title":"<code>best_result: NestedDict | None</code>  <code>property</code>","text":"<p>Best result.</p>"},{"location":"package/#danling.BaseRunner.best_score","title":"<code>best_score: float | None</code>  <code>property</code>","text":"<p>Best score.</p>"},{"location":"package/#danling.BaseRunner.checkpoint_dir","title":"<code>checkpoint_dir: str</code>  <code>property</code>","text":"<p>Directory of checkpoints.</p>"},{"location":"package/#danling.BaseRunner.device","title":"<code>device: Any</code>  <code>property</code>","text":"<p>Device of runner.</p>"},{"location":"package/#danling.BaseRunner.dir","title":"<code>dir: str</code>  <code>property</code>","text":"<p>Directory of the run.</p>"},{"location":"package/#danling.BaseRunner.distributed","title":"<code>distributed: bool</code>  <code>property</code>","text":"<p>If runner is running in distributed mode.</p>"},{"location":"package/#danling.BaseRunner.is_best","title":"<code>is_best: bool</code>  <code>property</code>","text":"<p>If current epoch is the best epoch.</p>"},{"location":"package/#danling.BaseRunner.is_local_main_process","title":"<code>is_local_main_process: bool</code>  <code>property</code>","text":"<p>If current process is the main process of local processes.</p>"},{"location":"package/#danling.BaseRunner.is_main_process","title":"<code>is_main_process: bool</code>  <code>property</code>","text":"<p>If current process is the main process of all processes.</p>"},{"location":"package/#danling.BaseRunner.latest_result","title":"<code>latest_result: NestedDict | None</code>  <code>property</code>","text":"<p>Latest result.</p>"},{"location":"package/#danling.BaseRunner.latest_score","title":"<code>latest_score: float | None</code>  <code>property</code>","text":"<p>Latest score.</p>"},{"location":"package/#danling.BaseRunner.local_rank","title":"<code>local_rank: int</code>  <code>property</code>","text":"<p>Process index of local processes.</p>"},{"location":"package/#danling.BaseRunner.log_path","title":"<code>log_path: str</code>  <code>cached</code> <code>property</code>","text":"<p>Path of log file.</p>"},{"location":"package/#danling.BaseRunner.progress","title":"<code>progress: float</code>  <code>property</code>","text":"<p>Training Progress.</p> <p>Returns:</p> Type Description <code>float</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no terminal is defined.</p>"},{"location":"package/#danling.BaseRunner.rank","title":"<code>rank: int</code>  <code>property</code>","text":"<p>Process index of all processes.</p>"},{"location":"package/#danling.BaseRunner.scores","title":"<code>scores: FlatDict | None</code>  <code>property</code>","text":"<p>All scores.</p> <p>Scores are extracted from results by <code>score_split</code> and <code>runner.state.score_name</code>, following <code>[r[score_split][self.state.score_name] for r in self.results]</code>.</p> <p>Scores are considered as the index of the performance of the model. It is useful to determine the best model and the best hyper-parameters.</p> <p><code>score_split</code> is defined in <code>self.state.score_split</code>. If it is not set, <code>DanLing</code> will use <code>val</code> or <code>validate</code> if they appear in the <code>latest_result</code>. If <code>DanLing</code> still could not find, it will fall back to the second key in the <code>latest_result</code> if it contains more that one element, or the first key.</p> <p>Note that certain keys are ignored when falling back, they are defined in {IGNORED_SET_NAMES}.</p>"},{"location":"package/#danling.BaseRunner.uuid","title":"<code>uuid: UUID</code>  <code>cached</code> <code>property</code>","text":"<p>UUID of the state.</p>"},{"location":"package/#danling.BaseRunner.world_size","title":"<code>world_size: int</code>  <code>property</code>","text":"<p>Number of processes.</p>"},{"location":"package/#danling.BaseRunner.append_result","title":"<code>append_result(result, index=None)</code>","text":"<p>Append result to <code>self.results</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def append_result(self, result: NestedDict, index: int | None = None) -&gt; None:\n    r\"\"\"\n    Append result to `self.results`.\n\n    Warnings:\n        `self.results` is heavily relied upon for computing metrics.\n\n        Failed to use this method may lead to unexpected behavior.\n    \"\"\"\n\n    if index is None:\n        index = self.state.epochs\n        global __APPEND_RESULT_COUNTER__  # pylint: disable=global-statement\n        __APPEND_RESULT_COUNTER__ += 1\n        if index == 0 and __APPEND_RESULT_COUNTER__ &gt; 1:\n            warn(\n                \"\"\"\n                Automatically set index to `self.state.epochs`.\n                Please ensure `self.state.epochs` updates before calling `append_result`\n                \"\"\",\n                category=RuntimeWarning,\n                stacklevel=2,\n            )\n    if index in self.results:\n        self.results[index].merge(result)\n    else:\n        self.results[index] = result\n</code></pre>"},{"location":"package/#danling.BaseRunner.check_dir","title":"<code>check_dir(action='warn')</code>","text":"<p>Check if <code>self.dir</code> is not empty.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>str</code> <p>The action to perform if <code>self.dir</code> is not empty.</p> <code>'warn'</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def check_dir(self, action: str = \"warn\") -&gt; bool:\n    r\"\"\"\n    Check if `self.dir` is not empty.\n\n    Args:\n        action (str): The action to perform if `self.dir` is not empty.\n        Can be one of (\"warn\", \"raise\", \"ignore\"), default is \"warn\".\n    \"\"\"\n\n    if action and action not in (\"warn\", \"raise\", \"ignore\"):\n        raise ValueError(f\"action should be one of warn, raise or ignore, but got {action}\")\n    if os.listdir(self.dir):\n        if action == \"warn\":\n            warn(\n                f\"Directory `{self.dir}` is not empty\",\n                category=RuntimeWarning,\n                stacklevel=2,\n            )\n        if action == \"raise\":\n            raise RuntimeError(f\"Directory `{self.dir}` is not empty\")\n        return False\n    return True\n</code></pre>"},{"location":"package/#danling.BaseRunner.dict","title":"<code>dict(cls=dict)</code>","text":"<p>Convert state to Mapping.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Callable</code> <p>Target `clc to convert to.</p> <code>dict</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Convert state to Mapping.\n\n    Args:\n        cls: Target `clc to convert to.\n    \"\"\"\n\n    return self.state.dict(cls)\n</code></pre>"},{"location":"package/#danling.BaseRunner.from_checkpoint","title":"<code>from_checkpoint(checkpoint, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Build BaseRunner from checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Mapping | bytes | str | PathLike</code> <p>Checkpoint (or its path) to load.</p> required <code>*args</code> <p>Additional arguments to pass to <code>cls.load</code>.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>cls.load</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseRunner</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_checkpoint(cls, checkpoint: Mapping | bytes | str | os.PathLike, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Build BaseRunner from checkpoint.\n\n    Args:\n        checkpoint: Checkpoint (or its path) to load.\n        *args: Additional arguments to pass to `cls.load`.\n        **kwargs: Additional keyword arguments to pass to `cls.load`.\n\n    Returns:\n        (BaseRunner):\n    \"\"\"\n\n    if isinstance(checkpoint, (bytes, str, os.PathLike)):\n        ckpt = cls.load(checkpoint, *args, **kwargs)\n    elif isinstance(checkpoint, Mapping):\n        ckpt = checkpoint\n    else:\n        raise ValueError(f\"checkpoint is set to {checkpoint} but is not a valid checkpoint\")\n    runner = cls(**ckpt[\"runner\"])\n    runner.load_checkpoint(ckpt, override_state=False)\n    return runner\n</code></pre>"},{"location":"package/#danling.BaseRunner.from_json","title":"<code>from_json(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json file.</p> <p>This function calls <code>self.from_jsons()</code> to construct object from json string. You may overwrite <code>from_jsons</code> in case something is not json serializable.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_json(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from json file.\n\n    This function calls `self.from_jsons()` to construct object from json string.\n    You may overwrite `from_jsons` in case something is not json serializable.\n    \"\"\"\n\n    with FlatDict.open(file) as fp:\n        return cls.from_jsons(fp.read(), *args, **kwargs)\n</code></pre>"},{"location":"package/#danling.BaseRunner.from_jsons","title":"<code>from_jsons(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_jsons(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from json string.\n    \"\"\"\n\n    return cls(Config.from_jsons(string, *args, **kwargs))\n</code></pre>"},{"location":"package/#danling.BaseRunner.from_yaml","title":"<code>from_yaml(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml file.</p> <p>This function calls <code>self.from_yamls()</code> to construct object from yaml string. You may overwrite <code>from_yamls</code> in case something is not yaml serializable.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_yaml(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from yaml file.\n\n    This function calls `self.from_yamls()` to construct object from yaml string.\n    You may overwrite `from_yamls` in case something is not yaml serializable.\n    \"\"\"\n\n    with FlatDict.open(file) as fp:\n        return cls.from_yamls(fp.read(), *args, **kwargs)\n</code></pre>"},{"location":"package/#danling.BaseRunner.from_yamls","title":"<code>from_yamls(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_yamls(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from yaml string.\n    \"\"\"\n\n    return cls(Config.from_yamls(string, *args, **kwargs))\n</code></pre>"},{"location":"package/#danling.BaseRunner.init_deepspeed","title":"<code>init_deepspeed(config=None)</code>","text":"<p>Preprocess DeepSpeed config.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_deepspeed(  # pylint: disable=too-many-branches, too-many-statements\n    self, config: Dict = None  # type: ignore\n) -&gt; Dict:\n    r\"\"\"\n    Preprocess DeepSpeed config.\n    \"\"\"\n\n    if config is None:\n        config = self.state.get(\"deepspeed\")\n    if config is None:\n        return {}\n    if isinstance(config, str):\n        config = NestedDict.load(config)\n    if config.get(\"steps_per_print\", \"auto\") == \"auto\":\n        config[\"steps_per_print\"] = self.state.log_interval\n    if config.get(\"train_micro_batch_size_per_gpu\", \"auto\") == \"auto\":\n        config[\"train_micro_batch_size_per_gpu\"] = self.batch_size\n    if \"amp\" in config:\n        amp = config[\"amp\"]\n        if amp.get(\"enabled\", \"auto\") == \"auto\":\n            amp[\"enabled\"] = \"true\"\n        if amp.get(\"opt_level\", \"auto\") == \"auto\":\n            amp[\"opt_level\"] = \"O1\"\n    if \"zero_optimization\" in config:\n        zero = config[\"zero_optimization\"]\n        if zero.get(\"allgather_bucket_size\") == \"auto\":\n            zero[\"allgather_bucket_size\"] = 1e6\n        if zero.get(\"reduce_bucket_size\") == \"auto\":\n            zero[\"reduce_bucket_size\"] = 1e6\n        if zero.get(\"stage3_max_live_parameters\") == \"auto\":\n            zero[\"stage3_max_live_parameters\"] = 1e8\n        if zero.get(\"stage3_max_live_gradients\") == \"auto\":\n            zero[\"stage3_max_live_gradients\"] = 1e8\n        if zero.get(\"stage3_max_reuse_distance\") == \"auto\":\n            zero[\"stage3_max_reuse_distance\"] = 1e8\n        if zero.get(\"stage3_prefetch_bucket_size\") == \"auto\":\n            zero[\"stage3_prefetch_bucket_size\"] = 1e6\n        if zero.get(\"stage3_param_persistence_threshold\") == \"auto\":\n            zero[\"stage3_param_persistence_threshold\"] = 1e8\n        if \"amp\" in config:\n            if \"fp16\" not in config:\n                config[\"fp16\"] = {}\n            if config[\"fp16\"].get(\"enabled\", \"auto\"):\n                config[\"fp16\"][\"enabled\"] = config[\"amp\"][\"enabled\"]\n            warn(\n                f\"AMP is not compatible with ZeRO. Automatically set 'fp16' to {config['amp']['enabled']}\",\n                stacklevel=2,\n            )\n            del config[\"amp\"]\n    if \"optimizer\" in config:\n        if \"params\" not in config[\"optimizer\"]:\n            config[\"optimizer\"][\"params\"] = {}\n        optimizer = config[\"optimizer\"][\"params\"]\n        if optimizer.get(\"lr\", \"auto\") == \"auto\":\n            optimizer[\"lr\"] = self.state.get(\"optim.lr\", 1e-3)\n        if optimizer.get(\"weight_decay\", \"auto\") == \"auto\":\n            optimizer[\"weight_decay\"] = self.state.get(\"optim.weight_decay\", 1e-2)\n        if optimizer.get(\"betas\") == \"auto\":\n            optimizer[\"betas\"] = (0.9, 0.999)\n        if optimizer.get(\"eps\") == \"auto\":\n            optimizer[\"eps\"] = 1e-8\n    if \"scheduler\" in config:\n        if \"params\" not in config[\"scheduler\"]:\n            config[\"scheduler\"][\"params\"] = {}\n        scheduler = config[\"scheduler\"][\"params\"]\n        if scheduler.get(\"total_num_steps\", \"auto\") == \"auto\":\n            scheduler[\"total_num_steps\"] = self.total_steps\n        if scheduler.get(\"warmup_num_steps\", \"auto\") == \"auto\":\n            scheduler[\"warmup_num_steps\"] = scheduler[\"total_num_steps\"] // 20\n        if scheduler.get(\"warmup_max_lr\", \"auto\") == \"auto\":\n            if self.optimizer:\n                scheduler[\"warmup_max_lr\"] = self.optimizer.param_groups[0][\"lr\"]\n            elif \"optimizer\" in config:\n                scheduler[\"warmup_max_lr\"] = config[\"optimizer\"][\"params\"][\"lr\"]\n            else:\n                raise ValueError(\"warmup_max_lr is not defined and cannot be inferred\")\n        if scheduler.get(\"warmup_min_lr\", \"auto\") == \"auto\":\n            scheduler[\"warmup_min_lr\"] = 1e-7\n    return config\n</code></pre>"},{"location":"package/#danling.BaseRunner.init_distributed","title":"<code>init_distributed()</code>","text":"<p>Initialise distributed running environment.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_distributed(self) -&gt; None:\n    r\"\"\"\n    Initialise distributed running environment.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"package/#danling.BaseRunner.init_logging","title":"<code>init_logging()</code>","text":"<p>Set up logging.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@on_main_process\ndef init_logging(self) -&gt; None:\n    r\"\"\"\n    Set up logging.\n    \"\"\"\n\n    os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n    # Why is setting up proper logging so !@?#! ugly?\n    logging.config.dictConfig(\n        {\n            \"version\": 1,\n            \"disable_existing_loggers\": False,\n            \"formatters\": {\n                \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n            },\n            \"handlers\": {\n                \"stdout\": {\n                    \"level\": \"INFO\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.StreamHandler\",\n                    \"stream\": \"ext://sys.stdout\",\n                },\n                \"logfile\": {\n                    \"level\": \"DEBUG\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.FileHandler\",\n                    \"filename\": self.log_path,\n                    \"mode\": \"a\",\n                },\n            },\n            \"loggers\": {\n                \"\": {\n                    \"handlers\": [\"stdout\", \"logfile\"],\n                    \"level\": \"DEBUG\",\n                    \"propagate\": True,\n                },\n            },\n        }\n    )\n    logging.captureWarnings(True)\n    self.logger = logging.getLogger(\"runner\")\n    self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n</code></pre>"},{"location":"package/#danling.BaseRunner.init_print","title":"<code>init_print(process=0)</code>","text":"<p>Set up <code>print</code>.</p> <p>Only print on a specific <code>process</code> or when <code>force = True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>process</code> <code>int</code> <p>The process to <code>print</code> on.</p> <code>0</code>"},{"location":"package/#danling.BaseRunner.init_print--notes","title":"Notes","text":"<p>If <code>self.state.log = True</code>, the default <code>print</code> function will be override by <code>logging.info</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_print(self, process: int = 0) -&gt; None:\n    r\"\"\"\n    Set up `print`.\n\n    Only print on a specific `process` or when `force = True`.\n\n    Args:\n        process: The process to `print` on.\n\n    Notes\n    -----\n    If `self.state.log = True`, the default `print` function will be override by `logging.info`.\n    \"\"\"\n\n    logger = logging.getLogger(\"print\")\n    logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n    import builtins as __builtin__  # pylint: disable=C0415\n\n    builtin_print = __builtin__.print\n\n    @catch\n    def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=redefined-builtin\n        if self.rank == process or force:\n            if self.state.log:\n                logger.info(*args, **kwargs)\n            else:\n                builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n    __builtin__.print = print\n</code></pre>"},{"location":"package/#danling.BaseRunner.init_tensorboard","title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"package/#danling.BaseRunner.json","title":"<code>json(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner State to json file.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\ndef json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n    r\"\"\"\n    Dump Runner State to json file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        return self.state.json(file, *args, **kwargs)\n</code></pre>"},{"location":"package/#danling.BaseRunner.jsons","title":"<code>jsons(*args, **kwargs)</code>","text":"<p>Dump Runner State to json string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def jsons(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner State to json string.\n    \"\"\"\n\n    return self.state.jsons(*args, **kwargs)\n</code></pre>"},{"location":"package/#danling.BaseRunner.load","title":"<code>load(file, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load any file with supported extensions.</p> <p><code>Runner.load</code> is identical to <code>dl.load</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@staticmethod\ndef load(file: PathStr, *args, **kwargs) -&gt; Any:\n    r\"\"\"\n    Load any file with supported extensions.\n\n    `Runner.load` is identical to `dl.load`.\n    \"\"\"\n\n    return load(file, *args, **kwargs)\n</code></pre>"},{"location":"package/#danling.BaseRunner.load_checkpoint","title":"<code>load_checkpoint(checkpoint=None, auto_resume=None, override_state=False, *args, **kwargs)</code>","text":"<p>Load info from checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Mapping | bytes | str | PathLike | None</code> <p>Checkpoint (or its path) to load. Defaults to <code>self.state.checkpoint</code>.</p> <code>None</code> <code>auto_resume</code> <code>bool | None</code> <p>Automatically resume from latest checkpoint if exists. Defaults to <code>False</code>. If is <code>True</code> and <code>checkpoint</code> is None, will set it to <code>self.checkpoint_dir/latest.pth</code>.</p> <code>None</code> <code>override_state</code> <code>bool</code> <p>If True, override runner state with checkpoint state. Defaults to <code>False</code>.</p> <code>False</code> <code>*args</code> <p>Additional arguments to pass to <code>self.load</code>.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>self.load</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>checkpoint</code> does not exists.</p> See Also <p><code>from_checkpoint</code>: Build runner from checkpoint. <code>load_pretrained</code>: Load parameters from pretrained checkpoint.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def load_checkpoint(\n    self,\n    checkpoint: Mapping | bytes | str | os.PathLike | None = None,\n    auto_resume: bool | None = None,\n    override_state: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Load info from checkpoint.\n\n    Args:\n        checkpoint: Checkpoint (or its path) to load.\n            Defaults to `self.state.checkpoint`.\n        auto_resume: Automatically resume from latest checkpoint if exists.\n            Defaults to `False`.\n            If is `True` and `checkpoint` is None, will set it to `self.checkpoint_dir/latest.pth`.\n        override_state: If True, override runner state with checkpoint state.\n            Defaults to `False`.\n        *args: Additional arguments to pass to `self.load`.\n        **kwargs: Additional keyword arguments to pass to `self.load`.\n\n    Raises:\n        FileNotFoundError: If `checkpoint` does not exists.\n\n    See Also:\n        [`from_checkpoint`][danling.BaseRunner.from_checkpoint]: Build runner from checkpoint.\n        [`load_pretrained`][danling.BaseRunner.load_pretrained]: Load parameters from pretrained checkpoint.\n    \"\"\"\n\n    checkpoint = checkpoint if checkpoint is not None else self.state.get(\"checkpoint\")\n    auto_resume = auto_resume if auto_resume is not None else self.state.get(\"auto_resume\", False)\n\n    # TODO: Support loading checkpoints in other format\n    if checkpoint is not None:\n        if auto_resume:\n            warn(\n                \"latest checkpoint is preempted by value specified in checkpoint\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        if isinstance(checkpoint, (bytes, str, os.PathLike)):\n            if not os.path.exists(checkpoint):\n                raise FileNotFoundError(f\"checkpoint is set to {checkpoint!r} but does not exist\")\n            self.state.checkpoint = checkpoint\n            ckpt = self.load(checkpoint, *args, **kwargs)\n        elif isinstance(checkpoint, Mapping):\n            ckpt = checkpoint\n        else:\n            raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint\")\n    elif auto_resume:\n        checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        if os.path.exists(checkpoint):\n            self.state.checkpoint = checkpoint\n            ckpt = self.load(checkpoint, *args, **kwargs)\n        else:\n            warn(\"latest checkpoint does not exits\", category=RuntimeWarning, stacklevel=2)\n            return\n    else:\n        raise ValueError(\"checkpoint is not specified and auto_resume is not set to True\")\n\n    # TODO: Wrap state_dict in a dataclass\n    self.state.merge(ckpt[\"runner\"], overwrite=override_state)\n    if self.model is not None and \"model\" in ckpt:\n        model = self.unwrap_model(self.model)\n        model.load_state_dict(ckpt[\"model\"])\n    if self.optimizer is not None and \"optimizer\" in ckpt:\n        self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n    if self.scheduler is not None and \"scheduler\" in ckpt:\n        self.scheduler.load_state_dict(ckpt[\"scheduler\"])\n    self.state.iter_begin = self.state.iters\n    self.state.step_begin = self.state.steps\n    self.state.epoch_begin = self.state.epochs\n</code></pre>"},{"location":"package/#danling.BaseRunner.load_pretrained","title":"<code>load_pretrained(checkpoint=None, *args, **kwargs)</code>","text":"<p>Load parameters from pretrained checkpoint.</p> <p>This method only loads the model weights.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Mapping | bytes | str | PathLike | None</code> <p>Pretrained checkpoint (or its path) to load. Defaults to <code>self.state.pretrained</code>.</p> <code>None</code> <code>*args</code> <p>Additional arguments to pass to <code>self.load</code>.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>self.load</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>checkpoint</code> does not exists.</p> See Also <p><code>load_checkpoint</code>: Load info from checkpoint.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def load_pretrained(self, checkpoint: Mapping | bytes | str | os.PathLike | None = None, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Load parameters from pretrained checkpoint.\n\n    This method only loads the model weights.\n\n    Args:\n        checkpoint: Pretrained checkpoint (or its path) to load.\n            Defaults to `self.state.pretrained`.\n        *args: Additional arguments to pass to `self.load`.\n        **kwargs: Additional keyword arguments to pass to `self.load`.\n\n    Raises:\n        FileNotFoundError: If `checkpoint` does not exists.\n\n    See Also:\n        [`load_checkpoint`][danling.BaseRunner.load_checkpoint]: Load info from checkpoint.\n    \"\"\"\n\n    # TODO: Support loading checkpoints in other format\n    checkpoint = checkpoint if checkpoint is not None else self.state.get(\"pretrained\")\n    if checkpoint is None:\n        raise ValueError(\"pretrained is not specified\")\n    if isinstance(checkpoint, (bytes, str, os.PathLike)):\n        if not os.path.exists(checkpoint):\n            raise FileNotFoundError(f\"pretrained is set to {checkpoint!r} but does not exist\")\n        ckpt = self.load(checkpoint, *args, **kwargs)\n    elif isinstance(checkpoint, Mapping):\n        ckpt = checkpoint\n    else:\n        raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint\")\n    if self.model is not None and \"model\" in ckpt:\n        model = self.unwrap_model(self.model)\n        model.load_state_dict(ckpt[\"model\"])\n    else:\n        raise ValueError(f\"Unable to find model weights in {checkpoint!r}\")\n</code></pre>"},{"location":"package/#danling.BaseRunner.print_result","title":"<code>print_result()</code>","text":"<p>Print latest and best result.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def print_result(self) -&gt; None:\n    r\"\"\"\n    Print latest and best result.\n    \"\"\"\n\n    print(f\"latest result: {self.latest_result}\")\n    print(f\"best result: {self.best_result}\")\n</code></pre>"},{"location":"package/#danling.BaseRunner.save","title":"<code>save(obj, file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Save any file with supported extensions.</p> <p><code>Runner.save</code> internally calls <code>dl.save</code>, but with additional arguments to allow it save only on the main process. Moreover, any error raised by <code>Runner.save</code> will be caught and logged.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, file: PathStr, main_process_only: bool = True, *args, **kwargs) -&gt; File:\n    r\"\"\"\n    Save any file with supported extensions.\n\n    `Runner.save` internally calls `dl.save`,\n    but with additional arguments to allow it save only on the main process.\n    Moreover, any error raised by `Runner.save` will be caught and logged.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        return save(obj, file, *args, **kwargs)\n    return file\n</code></pre>"},{"location":"package/#danling.BaseRunner.save_checkpoint","title":"<code>save_checkpoint(epochs=None)</code>","text":"<p>Save checkpoint to <code>self.checkpoint_dir</code>.</p> <p>The checkpoint will be saved to <code>self.checkpoint_dir/latest.pth</code>.</p> <p>If <code>self.state.save_interval</code> is positive and <code>self.state.epochs + 1</code> is a multiple of <code>save_interval</code>, the checkpoint will also be copied to <code>self.checkpoint_dir/epoch-{self.state.epochs}.pth</code>.</p> <p>If <code>self.is_best</code> is <code>True</code>, the checkpoint will also be copied to <code>self.checkpoint_dir/best.pth</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_checkpoint(self, epochs: int | None = None) -&gt; None:\n    r\"\"\"\n    Save checkpoint to `self.checkpoint_dir`.\n\n    The checkpoint will be saved to `self.checkpoint_dir/latest.pth`.\n\n    If `self.state.save_interval` is positive and `self.state.epochs + 1` is a multiple of `save_interval`,\n    the checkpoint will also be copied to `self.checkpoint_dir/epoch-{self.state.epochs}.pth`.\n\n    If `self.is_best` is `True`, the checkpoint will also be copied to `self.checkpoint_dir/best.pth`.\n    \"\"\"\n\n    epochs = epochs or self.state.epochs\n    save_interval = self.state.get(\"save_interval\", -1)\n    latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n    self.save(self.state_dict(), latest_path)\n    if save_interval &gt; 0 and (epochs + 1) % save_interval == 0:\n        save_path = os.path.join(self.checkpoint_dir, f\"epoch-{epochs}.pth\")\n        shutil.copy(latest_path, save_path)\n    if self.is_best:\n        best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n        shutil.copy(latest_path, best_path)\n</code></pre>"},{"location":"package/#danling.BaseRunner.save_result","title":"<code>save_result()</code>","text":"<p>Save result to <code>self.dir</code>.</p> <p>This method will save latest and best result to <code>self.dir/latest.json</code> and <code>self.dir/best.json</code> respectively.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_result(self) -&gt; None:\n    r\"\"\"\n    Save result to `self.dir`.\n\n    This method will save latest and best result to\n    `self.dir/latest.json` and `self.dir/best.json` respectively.\n    \"\"\"\n\n    results_path = os.path.join(self.dir, \"results.json\")\n    self.save(\n        {\n            \"name\": self.name,\n            \"id\": self.id,\n            \"timestamp\": self.timestamp,\n            \"results\": self.results,\n        },\n        results_path,\n        indent=4,\n    )\n    ret = {\"name\": self.name, \"id\": self.id, \"timestamp\": self.timestamp}\n    result = self.latest_result\n    if isinstance(result, FlatDict):\n        result = result.dict()\n    # This is slower but ensure id is the first key\n    if result is not None:\n        ret.update(result)\n    latest_path = os.path.join(self.dir, \"latest.json\")\n    self.save(ret, latest_path, indent=4)\n    if self.is_best:\n        best_path = os.path.join(self.dir, \"best.json\")\n        shutil.copy(latest_path, best_path)\n</code></pre>"},{"location":"package/#danling.BaseRunner.scale_lr","title":"<code>scale_lr(lr, lr_scale_factor=None, batch_size_base=None)</code>","text":"<p>Scale learning rate according to linear scaling rule.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def scale_lr(\n    self,\n    lr: float,\n    lr_scale_factor: float | None = None,\n    batch_size_base: int | None = None,\n) -&gt; float:\n    r\"\"\"\n    Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n    \"\"\"\n\n    if lr_scale_factor in self.state:\n        lr_scale_factor = self.state.lr_scale_factor\n\n    if lr_scale_factor is None:\n        if batch_size_base is None:\n            batch_size_base = getattr(self, \"batch_size_base\", None)\n            if batch_size_base is None:\n                raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n        lr_scale_factor = self.batch_size_equivalent / batch_size_base\n    elif batch_size_base is not None:\n        warn(\n            \"batch_size_base will be ignored if lr_scale_factor is specified\", category=RuntimeWarning, stacklevel=2\n        )\n    lr = lr * lr_scale_factor\n    self.state.lr_scale_factor = lr_scale_factor\n    return lr\n</code></pre>"},{"location":"package/#danling.BaseRunner.set_deterministic","title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"package/#danling.BaseRunner.set_seed","title":"<code>set_seed(seed=None, bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Random seed to set. Defaults to <code>self.state.seed</code> (<code>config.seed</code>).</p> <code>None</code> <code>bias</code> <code>int | None</code> <p>Make the seed different for each processes.</p> <p>This avoids same data augmentation are applied on every processes.</p> <p>Defaults to <code>self.rank</code>.</p> <p>Set to <code>False</code> to disable this feature.</p> <code>None</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Args:\n        seed: Random seed to set.\n            Defaults to `self.state.seed` (`config.seed`).\n\n        bias: Make the seed different for each processes.\n\n            This avoids same data augmentation are applied on every processes.\n\n            Defaults to `self.rank`.\n\n            Set to `False` to disable this feature.\n    \"\"\"\n\n    seed = seed or self.state.seed\n    bias = bias or self.rank\n    if bias:\n        seed += bias\n    if np_random is not None:\n        np_random.seed(seed)\n    random.seed(seed)\n</code></pre>"},{"location":"package/#danling.BaseRunner.state_dict","title":"<code>state_dict(cls=dict)</code>","text":"<p>Return dict of all attributes for checkpoint.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Return dict of all attributes for checkpoint.\n    \"\"\"\n\n    return cls(self.state)\n</code></pre>"},{"location":"package/#danling.BaseRunner.step","title":"<code>step(loss, batch_size=None, zero_grad=True)</code>","text":"<p>Backward loss and step optimizer &amp; scheduler.</p> <p>This method increment <code>self.state.steps</code>.</p> <p>This method also increment <code>self.state.iters</code> when <code>batch_size</code> is specified.</p> <p>Parameters:</p> Name Type Description Default <code>zero_grad</code> <code>bool</code> <p>Whether to zero the gradients.</p> <code>True</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n    r\"\"\"\n    Backward loss and step optimizer &amp; scheduler.\n\n    This method increment `self.state.steps`.\n\n    This method also increment `self.state.iters` when `batch_size` is specified.\n\n    Args:\n        zero_grad: Whether to zero the gradients.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"package/#danling.BaseRunner.yaml","title":"<code>yaml(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner State to yaml file.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\ndef yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n    r\"\"\"\n    Dump Runner State to yaml file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        return self.state.yaml(file, *args, **kwargs)\n</code></pre>"},{"location":"package/#danling.BaseRunner.yamls","title":"<code>yamls(*args, **kwargs)</code>","text":"<p>Dump Runner State to yaml string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def yamls(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner State to yaml string.\n    \"\"\"\n\n    return self.state.yamls(*args, **kwargs)\n</code></pre>"},{"location":"package/#danling.Metrics","title":"<code>Metrics</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Metric class wraps around multiple metrics that share the same states.</p> <p>Typically, there are many metrics that we want to compute for a single task. For example, we usually needs to compute <code>pearson</code> and <code>spearman</code> for a regression task. Unlike <code>accuracy</code>, which can uses an average meter to compute the average accuracy, <code>pearson</code> and <code>spearman</code> cannot be computed by averaging the results of multiple batches. They need access to all the data to compute the correct results. And saving all intermediate results for each tasks is quite inefficient.</p> <p><code>Metrics</code> solves this problem by maintaining a shared state for multiple metric functions.</p> <p>Attributes:</p> Name Type Description <code>metrics</code> <code>FlatDict[str, Callable]</code> <p>A dictionary of metrics to be computed.</p> <code>val</code> <code>FlatDict[str, float]</code> <p>Metric results of current batch on current device.</p> <code>bat</code> <code>FlatDict[str, float]</code> <p>Metric results of current batch on all devices.</p> <code>avg</code> <code>FlatDict[str, float]</code> <p>Metric results of all results on all devices.</p> <code>input</code> <p>The input tensor of latest batch.</p> <code>target</code> <p>The target tensor of latest batch.</p> <code>inputs</code> <p>All input tensors.</p> <code>targets</code> <p>All target tensors.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>A single mapping of metrics.</p> <code>()</code> <code>**metrics</code> <code>FlatDict[str, Callable]</code> <p>Metrics.</p> <code>{}</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from danling.metrics.functional import auroc, auprc\n&gt;&gt;&gt; metrics = Metrics(auroc=auroc, auprc=auprc)\n&gt;&gt;&gt; metrics\nMetrics('auroc', 'auprc')\n&gt;&gt;&gt; metrics.update([0.2, 0.3, 0.5, 0.7], [0, 1, 0, 1])\n&gt;&gt;&gt; metrics.input  # predicted values of current batch\ntensor([0.2000, 0.3000, 0.5000, 0.7000])\n&gt;&gt;&gt; metrics.target  # ground truth of current batch\ntensor([0, 1, 0, 1])\n&gt;&gt;&gt; metrics.inputs  # predicted values of all data\ntensor([0.2000, 0.3000, 0.5000, 0.7000])\n&gt;&gt;&gt; metrics.targets  # ground truth of all data\ntensor([0, 1, 0, 1])\n&gt;&gt;&gt; metrics.val  # Metrics of current batch on current device\nFlatDict(\n  ('auroc'): 0.75\n  ('auprc'): 0.8333333730697632\n)\n&gt;&gt;&gt; metrics.bat  # Metrics of current batch on all devices\nFlatDict(\n  ('auroc'): 0.75\n  ('auprc'): 0.8333333730697632\n)\n&gt;&gt;&gt; metrics.avg  # Metrics of all data on all devices\nFlatDict(\n  ('auroc'): 0.75\n  ('auprc'): 0.8333333730697632\n)\n&gt;&gt;&gt; metrics.update([0.1, 0.4, 0.6, 0.8], [0, 0, 1, 0])\n&gt;&gt;&gt; metrics.input  # predicted values of current batch\ntensor([0.1000, 0.4000, 0.6000, 0.8000])\n&gt;&gt;&gt; metrics.target  # ground truth of current batch\ntensor([0, 0, 1, 0])\n&gt;&gt;&gt; metrics.inputs  # predicted values of all data\ntensor([0.2000, 0.3000, 0.5000, 0.7000, 0.1000, 0.4000, 0.6000, 0.8000])\n&gt;&gt;&gt; metrics.targets  # ground truth of all data\ntensor([0, 1, 0, 1, 0, 0, 1, 0])\n&gt;&gt;&gt; metrics.val  # Metrics of current batch on current device\nFlatDict(\n  ('auroc'): 0.6666666666666666\n  ('auprc'): 0.5\n)\n&gt;&gt;&gt; metrics.bat  # Metrics of current batch on all devices\nFlatDict(\n  ('auroc'): 0.6666666666666666\n  ('auprc'): 0.5\n)\n&gt;&gt;&gt; metrics.avg  # Metrics of all data on all devices\nFlatDict(\n  ('auroc'): 0.6666666666666666\n  ('auprc'): 0.5555555820465088\n)\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'auroc: 0.6667 (0.6667)\\tauprc: 0.5000 (0.5556)'\n</code></pre> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>class Metrics(Metric):\n    r\"\"\"\n    Metric class wraps around multiple metrics that share the same states.\n\n    Typically, there are many metrics that we want to compute for a single task.\n    For example, we usually needs to compute `pearson` and `spearman` for a regression task.\n    Unlike `accuracy`, which can uses an average meter to compute the average accuracy,\n    `pearson` and `spearman` cannot be computed by averaging the results of multiple batches.\n    They need access to all the data to compute the correct results.\n    And saving all intermediate results for each tasks is quite inefficient.\n\n    `Metrics` solves this problem by maintaining a shared state for multiple metric functions.\n\n    Attributes:\n        metrics: A dictionary of metrics to be computed.\n        val: Metric results of current batch on current device.\n        bat: Metric results of current batch on all devices.\n        avg: Metric results of all results on all devices.\n        input: The input tensor of latest batch.\n        target: The target tensor of latest batch.\n        inputs: All input tensors.\n        targets: All target tensors.\n\n    Args:\n        *args: A single mapping of metrics.\n        **metrics: Metrics.\n\n    Examples:\n        &gt;&gt;&gt; from danling.metrics.functional import auroc, auprc\n        &gt;&gt;&gt; metrics = Metrics(auroc=auroc, auprc=auprc)\n        &gt;&gt;&gt; metrics\n        Metrics('auroc', 'auprc')\n        &gt;&gt;&gt; metrics.update([0.2, 0.3, 0.5, 0.7], [0, 1, 0, 1])\n        &gt;&gt;&gt; metrics.input  # predicted values of current batch\n        tensor([0.2000, 0.3000, 0.5000, 0.7000])\n        &gt;&gt;&gt; metrics.target  # ground truth of current batch\n        tensor([0, 1, 0, 1])\n        &gt;&gt;&gt; metrics.inputs  # predicted values of all data\n        tensor([0.2000, 0.3000, 0.5000, 0.7000])\n        &gt;&gt;&gt; metrics.targets  # ground truth of all data\n        tensor([0, 1, 0, 1])\n        &gt;&gt;&gt; metrics.val  # Metrics of current batch on current device\n        FlatDict(\n          ('auroc'): 0.75\n          ('auprc'): 0.8333333730697632\n        )\n        &gt;&gt;&gt; metrics.bat  # Metrics of current batch on all devices\n        FlatDict(\n          ('auroc'): 0.75\n          ('auprc'): 0.8333333730697632\n        )\n        &gt;&gt;&gt; metrics.avg  # Metrics of all data on all devices\n        FlatDict(\n          ('auroc'): 0.75\n          ('auprc'): 0.8333333730697632\n        )\n        &gt;&gt;&gt; metrics.update([0.1, 0.4, 0.6, 0.8], [0, 0, 1, 0])\n        &gt;&gt;&gt; metrics.input  # predicted values of current batch\n        tensor([0.1000, 0.4000, 0.6000, 0.8000])\n        &gt;&gt;&gt; metrics.target  # ground truth of current batch\n        tensor([0, 0, 1, 0])\n        &gt;&gt;&gt; metrics.inputs  # predicted values of all data\n        tensor([0.2000, 0.3000, 0.5000, 0.7000, 0.1000, 0.4000, 0.6000, 0.8000])\n        &gt;&gt;&gt; metrics.targets  # ground truth of all data\n        tensor([0, 1, 0, 1, 0, 0, 1, 0])\n        &gt;&gt;&gt; metrics.val  # Metrics of current batch on current device\n        FlatDict(\n          ('auroc'): 0.6666666666666666\n          ('auprc'): 0.5\n        )\n        &gt;&gt;&gt; metrics.bat  # Metrics of current batch on all devices\n        FlatDict(\n          ('auroc'): 0.6666666666666666\n          ('auprc'): 0.5\n        )\n        &gt;&gt;&gt; metrics.avg  # Metrics of all data on all devices\n        FlatDict(\n          ('auroc'): 0.6666666666666666\n          ('auprc'): 0.5555555820465088\n        )\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'auroc: 0.6667 (0.6667)\\tauprc: 0.5000 (0.5556)'\n    \"\"\"\n\n    metrics: FlatDict[str, Callable]\n    _input: Tensor\n    _target: Tensor\n    _inputs: flist\n    _targets: flist\n    _input_buffer: flist\n    _target_buffer: flist\n    score_name: str\n    best_fn: Callable\n    merge_dict: bool = True\n    return_nested: bool = False\n\n    def __init__(\n        self,\n        *args,\n        merge_dict: bool | None = None,\n        return_nested: bool | None = None,\n        device: torch.device | None = None,\n        **metrics: FlatDict[str, Callable],\n    ):\n        super().__init__(device=device)\n        self._add_state(\"_input\", torch.empty(0))\n        self._add_state(\"_target\", torch.empty(0))\n        self._add_state(\"_inputs\", flist())\n        self._add_state(\"_targets\", flist())\n        self._add_state(\"_input_buffer\", flist())\n        self._add_state(\"_target_buffer\", flist())\n        self.metrics = FlatDict(*args, **metrics)\n        if merge_dict is not None:\n            self.merge_dict = merge_dict\n        if return_nested is not None:\n            self.return_nested = return_nested\n\n    @torch.inference_mode()\n    def update(self, input: Any, target: Any) -&gt; None:  # pylint: disable=W0221\n        if isinstance(input, NestedTensor):\n            self._input = input\n            self._input_buffer.extend(input.cpu().storage())\n        else:\n            if not isinstance(input, torch.Tensor):\n                input = torch.tensor(input)\n            self._input = input\n            self._input_buffer.append(input.cpu())\n        if isinstance(target, NestedTensor):\n            self._target = target\n            self._target_buffer.extend(target.cpu().storage())\n        else:\n            if not isinstance(target, torch.Tensor):\n                target = torch.tensor(target)\n            self._target = target\n            self._target_buffer.append(target.cpu())\n\n    def compute(self) -&gt; FlatDict[str, float]:\n        return self.calculate(self.inputs.to(self.device), self.targets.to(self.device))\n\n    def value(self) -&gt; FlatDict[str, float]:\n        return self.calculate(self._input, self._target)\n\n    def batch(self) -&gt; FlatDict[str, float]:\n        return self.calculate(self.input, self.target)\n\n    def average(self) -&gt; FlatDict[str, float]:\n        return self.calculate(self.inputs.to(self.device), self.targets.to(self.device))\n\n    @property\n    def val(self) -&gt; FlatDict[str, float]:\n        return self.value()\n\n    @property\n    def bat(self) -&gt; FlatDict[str, float]:\n        return self.batch()\n\n    @property\n    def avg(self) -&gt; FlatDict[str, float]:\n        return self.average()\n\n    @torch.inference_mode()\n    def calculate(self, input: Tensor, target: Tensor) -&gt; flist | float:\n        if (\n            isinstance(input, (Tensor, NestedTensor))\n            and input.numel() == 0 == target.numel()\n            or isinstance(input, (list, dict))\n            and len(input) == 0 == len(target)\n        ):\n            return FlatDict({name: nan for name in self.metrics.keys()})\n        ret = FlatDict()\n        for name, metric in self.metrics.items():\n            score = self._calculate(metric, input, target)\n            if isinstance(score, Mapping):\n                if self.merge_dict:\n                    ret.merge(score)\n                else:\n                    for n, s in score.items():\n                        ret[f\"{name}.{n}\"] = s\n            else:\n                ret[name] = score\n        return ret\n\n    @torch.inference_mode()\n    def _calculate(self, metric, input: Tensor, target: Tensor) -&gt; flist | float:\n        score = metric(input, target)\n        if isinstance(score, Tensor):\n            return score.item() if score.numel() == 1 else flist(score.tolist())\n        return score\n\n    @torch.inference_mode()\n    def merge_state(self, metrics: Iterable):\n        raise NotImplementedError()\n\n    # Due to an issue with PyTorch, we cannot decorate input/target with @torch.inference_mode()\n    # Otherwise, we will encounter the following error when using \"gloo\" backend:\n    # Inplace update to inference tensor outside InferenceMode is not allowed\n    @property\n    def input(self):\n        world_size = get_world_size()\n        if world_size == 1:\n            if isinstance(self._input, NestedTensor) and not self.return_nested:\n                return torch.cat(self._input.storage(), 0)\n            return self._input\n        if isinstance(self._input, Tensor):\n            synced_tensor = [torch.zeros_like(self._input) for _ in range(world_size)]\n            dist.all_gather(synced_tensor, self._input)\n            return torch.cat(synced_tensor, 0)\n        if isinstance(self._input, NestedTensor):\n            synced_tensors = [None for _ in range(world_size)]\n            dist.all_gather_object(synced_tensors, self._input.storage())\n            synced_tensors = flist(i.to(self.device) for j in synced_tensors for i in j)\n            try:\n                return torch.cat(synced_tensors, 0)\n            except RuntimeError:\n                if self.return_nested:\n                    return NestedTensor(synced_tensor)\n                return synced_tensors\n        raise ValueError(f\"Expected _input to be a Tensor or a NestedTensor, but got {type(self._input)}\")\n\n    @property\n    def target(self):\n        world_size = get_world_size()\n        if world_size == 1:\n            if isinstance(self._target, NestedTensor) and not self.return_nested:\n                return torch.cat(self._target.storage(), 0)\n            return self._target\n        if isinstance(self._target, Tensor):\n            synced_tensor = [torch.zeros_like(self._target) for _ in range(world_size)]\n            dist.all_gather(synced_tensor, self._target)\n            return torch.cat(synced_tensor, 0)\n        if isinstance(self._target, NestedTensor):\n            synced_tensors = [None for _ in range(world_size)]\n            dist.all_gather_object(synced_tensors, self._target.storage())\n            synced_tensors = flist(i.to(self.device) for j in synced_tensors for i in j)\n            try:\n                return torch.cat(synced_tensors, 0)\n            except RuntimeError:\n                if self.return_nested:\n                    return NestedTensor(synced_tensor)\n                return synced_tensors\n        raise ValueError(f\"Expected _target to be a Tensor or a NestedTensor, but got {type(self._target)}\")\n\n    @property\n    def inputs(self):\n        if not self._inputs and not self._input_buffer:\n            return torch.empty(0)\n        if self._input_buffer:\n            world_size = get_world_size()\n            if world_size &gt; 1:\n                synced_tensors = [None for _ in range(world_size)]\n                dist.all_gather_object(synced_tensors, self._input_buffer)\n                self._inputs.extend([i for j in synced_tensors for i in j])\n            else:\n                self._inputs.extend(self._input_buffer)\n            self._input_buffer = flist()\n        try:\n            return torch.cat(self._inputs, 0)\n        except RuntimeError:\n            if self.return_nested:\n                return NestedTensor(self._inputs)\n            return self._inputs\n\n    @property\n    def targets(self):\n        if not self._targets and not self._target_buffer:\n            return torch.empty(0)\n        if self._target_buffer:\n            world_size = get_world_size()\n            if world_size &gt; 1:\n                synced_tensors = [None for _ in range(world_size)]\n                dist.all_gather_object(synced_tensors, self._target_buffer)\n                self._targets.extend([i for j in synced_tensors for i in j])\n            else:\n                self._targets.extend(self._target_buffer)\n            self._target_buffer = flist()\n        try:\n            return torch.cat(self._targets, 0)\n        except RuntimeError:\n            if self.return_nested:\n                return NestedTensor(self._inputs)\n            return self._targets\n\n    def __repr__(self):\n        keys = tuple(i for i in self.metrics.keys())\n        return f\"{self.__class__.__name__}{keys}\"\n\n    def __format__(self, format_spec):\n        val, avg = self.value(), self.average()\n        return \"\\t\".join(\n            [f\"{key}: {val[key].__format__(format_spec)} ({avg[key].__format__(format_spec)})\" for key in self.metrics]\n        )\n\n    def reset(self: Self) -&gt; Self:  # pragma: no cover\n        r\"\"\"\n        Reset the metric state variables to their default value.\n        The tensors in the default values are also moved to the device of\n        the last ``self.to(device)`` call.\n        \"\"\"\n        for state_name, default in self._state_name_to_default.items():\n            if isinstance(default, torch.Tensor):\n                setattr(self, state_name, default.clone().to(self.device))\n            elif isinstance(default, list):\n                setattr(\n                    self,\n                    state_name,\n                    flist(tensor.clone().to(self.device) for tensor in default),\n                )\n            elif isinstance(default, dict):\n                setattr(\n                    self,\n                    state_name,\n                    DefaultDict(\n                        lambda: torch.tensor(0.0, device=self.device),\n                        {key: tensor.clone().to(self.device) for key, tensor in default.items()},\n                    ),\n                )\n            elif isinstance(default, (int, float)):\n                setattr(self, state_name, default)\n            else:\n                raise TypeError(\n                    f\"Invalid type for default value for {state_name}. Received {type(default)},\"\n                    \"but expected ``torch.Tensor``, a list of ``torch.Tensor``,\"\n                    \"a dictionary with ``torch.Tensor``, int, or float.\"\n                )\n        return self\n</code></pre>"},{"location":"package/#danling.Metrics.reset","title":"<code>reset()</code>","text":"<p>Reset the metric state variables to their default value. The tensors in the default values are also moved to the device of the last <code>self.to(device)</code> call.</p> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>def reset(self: Self) -&gt; Self:  # pragma: no cover\n    r\"\"\"\n    Reset the metric state variables to their default value.\n    The tensors in the default values are also moved to the device of\n    the last ``self.to(device)`` call.\n    \"\"\"\n    for state_name, default in self._state_name_to_default.items():\n        if isinstance(default, torch.Tensor):\n            setattr(self, state_name, default.clone().to(self.device))\n        elif isinstance(default, list):\n            setattr(\n                self,\n                state_name,\n                flist(tensor.clone().to(self.device) for tensor in default),\n            )\n        elif isinstance(default, dict):\n            setattr(\n                self,\n                state_name,\n                DefaultDict(\n                    lambda: torch.tensor(0.0, device=self.device),\n                    {key: tensor.clone().to(self.device) for key, tensor in default.items()},\n                ),\n            )\n        elif isinstance(default, (int, float)):\n            setattr(self, state_name, default)\n        else:\n            raise TypeError(\n                f\"Invalid type for default value for {state_name}. Received {type(default)},\"\n                \"but expected ``torch.Tensor``, a list of ``torch.Tensor``,\"\n                \"a dictionary with ``torch.Tensor``, int, or float.\"\n            )\n    return self\n</code></pre>"},{"location":"package/#danling.MultiTaskAverageMeters","title":"<code>MultiTaskAverageMeters</code>","text":"<p>               Bases: <code>MultiTaskDict</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = MultiTaskAverageMeters()\n&gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.6000 (0.6000)\ndataset1.cls.auroc: 0.7000 (0.7000)\ndataset1.reg.r2: 0.8000 (0.8000)\ndataset2.r2: 0.9000 (0.9000)\n&gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.9000 (0.7500)\ndataset1.cls.auroc: 0.7000 (0.7000)\ndataset1.reg.r2: 0.8000 (0.8000)\ndataset2.r2: 0.9000 (0.9000)\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 1.5, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 2, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n&gt;&gt;&gt; meters.reset()\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.0000 (nan)\ndataset1.cls.auroc: 0.0000 (nan)\ndataset1.reg.r2: 0.0000 (nan)\ndataset2.r2: 0.0000 (nan)\n&gt;&gt;&gt; meters = MultiTaskAverageMeters(return_average=True)\n&gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.a.auroc\": 0.7, \"dataset1.b.auroc\": 0.8, \"dataset2.auroc\": 0.9})\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.6000 (0.6000)\ndataset1.a.auroc: 0.7000 (0.7000)\ndataset1.b.auroc: 0.8000 (0.8000)\ndataset2.auroc: 0.9000 (0.9000)\n&gt;&gt;&gt; meters.update({\"loss\": 0.9, \"dataset1.a.auroc\": 0.8, \"dataset1.b.auroc\": 0.9, \"dataset2.auroc\": 1.0})\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.9000 (0.7500)\ndataset1.a.auroc: 0.8000 (0.7500)\ndataset1.b.auroc: 0.9000 (0.8500)\ndataset2.auroc: 1.0000 (0.9500)\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>class MultiTaskAverageMeters(MultiTaskDict):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; meters = MultiTaskAverageMeters()\n        &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.6000 (0.6000)\n        dataset1.cls.auroc: 0.7000 (0.7000)\n        dataset1.reg.r2: 0.8000 (0.8000)\n        dataset2.r2: 0.9000 (0.9000)\n        &gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.9000 (0.7500)\n        dataset1.cls.auroc: 0.7000 (0.7000)\n        dataset1.reg.r2: 0.8000 (0.8000)\n        dataset2.r2: 0.9000 (0.9000)\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 1.5, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 2, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n        &gt;&gt;&gt; meters.reset()\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.0000 (nan)\n        dataset1.cls.auroc: 0.0000 (nan)\n        dataset1.reg.r2: 0.0000 (nan)\n        dataset2.r2: 0.0000 (nan)\n        &gt;&gt;&gt; meters = MultiTaskAverageMeters(return_average=True)\n        &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.a.auroc\": 0.7, \"dataset1.b.auroc\": 0.8, \"dataset2.auroc\": 0.9})\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.6000 (0.6000)\n        dataset1.a.auroc: 0.7000 (0.7000)\n        dataset1.b.auroc: 0.8000 (0.8000)\n        dataset2.auroc: 0.9000 (0.9000)\n        &gt;&gt;&gt; meters.update({\"loss\": 0.9, \"dataset1.a.auroc\": 0.8, \"dataset1.b.auroc\": 0.9, \"dataset2.auroc\": 1.0})\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.9000 (0.7500)\n        dataset1.a.auroc: 0.8000 (0.7500)\n        dataset1.b.auroc: 0.9000 (0.8500)\n        dataset2.auroc: 1.0000 (0.9500)\n    \"\"\"\n\n    @property\n    def sum(self) -&gt; NestedDict[str, float]:\n        return NestedDict({key: meter.sum for key, meter in self.all_items()})\n\n    @property\n    def count(self) -&gt; NestedDict[str, int]:\n        return NestedDict({key: meter.count for key, meter in self.all_items()})\n\n    def update(self, values: Dict, *, n: int = 1) -&gt; None:  # pylint: disable=W0237\n        r\"\"\"\n        Updates the average and current value in all meters.\n\n        Args:\n            values: Dict of values to be added to the average.\n            n: Number of values to be added.\n\n        Raises:\n            ValueError: If the value is not an instance of (int, float, Mapping).\n\n        Examples:\n            &gt;&gt;&gt; meters = MultiTaskAverageMeters()\n            &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 0.6, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 1, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n            &gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 1.5, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 2, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n            &gt;&gt;&gt; meters.update({\"loss\": 0.8, \"dataset1.cls.auroc\": 0.9, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.7})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 2.3, 'dataset1': {'cls': {'auroc': 1.6}, 'reg': {'r2': 1.6}}, 'dataset2': {'r2': 1.6}}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 3, 'dataset1': {'cls': {'auroc': 2}, 'reg': {'r2': 2}}, 'dataset2': {'r2': 2}}\n            &gt;&gt;&gt; meters.update({\"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.7, \"dataset2.r2\": 0.9})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 2.3, 'dataset1': {'cls': {'auroc': 2.3}, 'reg': {'r2': 2.3}}, 'dataset2': {'r2': 2.5}}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 3, 'dataset1': {'cls': {'auroc': 3}, 'reg': {'r2': 3}}, 'dataset2': {'r2': 3}}\n            &gt;&gt;&gt; meters.update({\"dataset1\": {\"cls.auroc\": 0.9}, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n            Traceback (most recent call last):\n            ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'dict'&gt;\n            This is likely due to nested dictionary in the values.\n            Nested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\n            &gt;&gt;&gt; meters.update(dict(loss=\"\"))\n            Traceback (most recent call last):\n            ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'str'&gt;\n        \"\"\"  # noqa: E501\n\n        for meter, value in values.items():\n            if isinstance(value, (int, float)):\n                self[meter].update(value, n)\n            elif isinstance(value, Dict):\n                value.setdefault(\"n\", n)\n                try:\n                    self[meter].update(**value)\n                except TypeError:\n                    raise ValueError(\n                        f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\\n\"\n                        \"This is likely due to nested dictionary in the values.\\n\"\n                        \"Nested dictionaries cannot be processed due to the method's design, which uses Mapping \"\n                        \"to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\"\n                    ) from None\n            else:\n                raise ValueError(f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\")\n\n    # evil hack, as the default_factory must not be set to make `NestedDict` happy\n    # this have some side effects, it will break attribute style intermediate nested dict auto creation\n    # but everything has a price\n    def get(self, name: Any, default=None) -&gt; Any:\n        if not name.startswith(\"_\") and not name.endswith(\"_\"):\n            return self.setdefault(name, AverageMeter())\n        return super().get(name, default)\n</code></pre>"},{"location":"package/#danling.MultiTaskAverageMeters.update","title":"<code>update(values, *, n=1)</code>","text":"<p>Updates the average and current value in all meters.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict</code> <p>Dict of values to be added to the average.</p> required <code>n</code> <code>int</code> <p>Number of values to be added.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is not an instance of (int, float, Mapping).</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = MultiTaskAverageMeters()\n&gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 0.6, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 1, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n&gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 1.5, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 2, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n&gt;&gt;&gt; meters.update({\"loss\": 0.8, \"dataset1.cls.auroc\": 0.9, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.7})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 2.3, 'dataset1': {'cls': {'auroc': 1.6}, 'reg': {'r2': 1.6}}, 'dataset2': {'r2': 1.6}}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 3, 'dataset1': {'cls': {'auroc': 2}, 'reg': {'r2': 2}}, 'dataset2': {'r2': 2}}\n&gt;&gt;&gt; meters.update({\"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.7, \"dataset2.r2\": 0.9})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 2.3, 'dataset1': {'cls': {'auroc': 2.3}, 'reg': {'r2': 2.3}}, 'dataset2': {'r2': 2.5}}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 3, 'dataset1': {'cls': {'auroc': 3}, 'reg': {'r2': 3}}, 'dataset2': {'r2': 3}}\n&gt;&gt;&gt; meters.update({\"dataset1\": {\"cls.auroc\": 0.9}, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\nTraceback (most recent call last):\nValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'dict'&gt;\nThis is likely due to nested dictionary in the values.\nNested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\n&gt;&gt;&gt; meters.update(dict(loss=\"\"))\nTraceback (most recent call last):\nValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'str'&gt;\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def update(self, values: Dict, *, n: int = 1) -&gt; None:  # pylint: disable=W0237\n    r\"\"\"\n    Updates the average and current value in all meters.\n\n    Args:\n        values: Dict of values to be added to the average.\n        n: Number of values to be added.\n\n    Raises:\n        ValueError: If the value is not an instance of (int, float, Mapping).\n\n    Examples:\n        &gt;&gt;&gt; meters = MultiTaskAverageMeters()\n        &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 0.6, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 1, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n        &gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 1.5, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 2, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n        &gt;&gt;&gt; meters.update({\"loss\": 0.8, \"dataset1.cls.auroc\": 0.9, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.7})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 2.3, 'dataset1': {'cls': {'auroc': 1.6}, 'reg': {'r2': 1.6}}, 'dataset2': {'r2': 1.6}}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 3, 'dataset1': {'cls': {'auroc': 2}, 'reg': {'r2': 2}}, 'dataset2': {'r2': 2}}\n        &gt;&gt;&gt; meters.update({\"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.7, \"dataset2.r2\": 0.9})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 2.3, 'dataset1': {'cls': {'auroc': 2.3}, 'reg': {'r2': 2.3}}, 'dataset2': {'r2': 2.5}}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 3, 'dataset1': {'cls': {'auroc': 3}, 'reg': {'r2': 3}}, 'dataset2': {'r2': 3}}\n        &gt;&gt;&gt; meters.update({\"dataset1\": {\"cls.auroc\": 0.9}, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n        Traceback (most recent call last):\n        ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'dict'&gt;\n        This is likely due to nested dictionary in the values.\n        Nested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\n        &gt;&gt;&gt; meters.update(dict(loss=\"\"))\n        Traceback (most recent call last):\n        ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'str'&gt;\n    \"\"\"  # noqa: E501\n\n    for meter, value in values.items():\n        if isinstance(value, (int, float)):\n            self[meter].update(value, n)\n        elif isinstance(value, Dict):\n            value.setdefault(\"n\", n)\n            try:\n                self[meter].update(**value)\n            except TypeError:\n                raise ValueError(\n                    f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\\n\"\n                    \"This is likely due to nested dictionary in the values.\\n\"\n                    \"Nested dictionaries cannot be processed due to the method's design, which uses Mapping \"\n                    \"to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\"\n                ) from None\n        else:\n            raise ValueError(f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\")\n</code></pre>"},{"location":"package/#danling.MultiTaskMetrics","title":"<code>MultiTaskMetrics</code>","text":"<p>               Bases: <code>MultiTaskDict</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from danling.metrics.functional import auroc, auprc, pearson, spearman, accuracy, matthews_corrcoef\n&gt;&gt;&gt; metrics = MultiTaskMetrics()\n&gt;&gt;&gt; metrics.dataset1.cls = Metrics(auroc=auroc, auprc=auprc)\n&gt;&gt;&gt; metrics.dataset1.reg = Metrics(pearson=pearson, spearman=spearman)\n&gt;&gt;&gt; metrics.dataset2 = Metrics(auroc=auroc, auprc=auprc)\n&gt;&gt;&gt; metrics\nMultiTaskMetrics(&lt;class 'danling.metrics.metrics.MultiTaskMetrics'&gt;,\n  ('dataset1'): MultiTaskMetrics(&lt;class 'danling.metrics.metrics.MultiTaskMetrics'&gt;,\n    ('cls'): Metrics('auroc', 'auprc')\n    ('reg'): Metrics('pearson', 'spearman')\n  )\n  ('dataset2'): Metrics('auroc', 'auprc')\n)\n&gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.2, 0.4, 0.5, 0.7], \"target\": [0, 1, 0, 1]}, \"dataset1.reg\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0.2, 0.3, 0.5, 0.7]}, \"dataset2\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 1, 0, 1]}})\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n&gt;&gt;&gt; metrics.setattr(\"return_average\", True)\n&gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 0, 1, 0]}, \"dataset1.reg\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0.2, 0.4, 0.6, 0.8]}, \"dataset2\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0, 0, 1, 0]}})\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9898 (0.9146)\\tspearman: 1.0000 (0.9222)\\ndataset2: auroc: 0.6667 (0.7333)\\tauprc: 0.5000 (0.7000)'\n</code></pre> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>class MultiTaskMetrics(MultiTaskDict):\n    r\"\"\"\n    Examples:\n        &gt;&gt;&gt; from danling.metrics.functional import auroc, auprc, pearson, spearman, accuracy, matthews_corrcoef\n        &gt;&gt;&gt; metrics = MultiTaskMetrics()\n        &gt;&gt;&gt; metrics.dataset1.cls = Metrics(auroc=auroc, auprc=auprc)\n        &gt;&gt;&gt; metrics.dataset1.reg = Metrics(pearson=pearson, spearman=spearman)\n        &gt;&gt;&gt; metrics.dataset2 = Metrics(auroc=auroc, auprc=auprc)\n        &gt;&gt;&gt; metrics\n        MultiTaskMetrics(&lt;class 'danling.metrics.metrics.MultiTaskMetrics'&gt;,\n          ('dataset1'): MultiTaskMetrics(&lt;class 'danling.metrics.metrics.MultiTaskMetrics'&gt;,\n            ('cls'): Metrics('auroc', 'auprc')\n            ('reg'): Metrics('pearson', 'spearman')\n          )\n          ('dataset2'): Metrics('auroc', 'auprc')\n        )\n        &gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.2, 0.4, 0.5, 0.7], \"target\": [0, 1, 0, 1]}, \"dataset1.reg\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0.2, 0.3, 0.5, 0.7]}, \"dataset2\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 1, 0, 1]}})\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n        &gt;&gt;&gt; metrics.setattr(\"return_average\", True)\n        &gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 0, 1, 0]}, \"dataset1.reg\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0.2, 0.4, 0.6, 0.8]}, \"dataset2\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0, 0, 1, 0]}})\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9898 (0.9146)\\tspearman: 1.0000 (0.9222)\\ndataset2: auroc: 0.6667 (0.7333)\\tauprc: 0.5000 (0.7000)'\n    \"\"\"  # noqa: E501\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, default_factory=MultiTaskMetrics, **kwargs)\n\n    def update(self, values: Mapping[str, Mapping[str, Tensor]]) -&gt; None:  # pylint: disable=W0237\n        r\"\"\"\n        Updates the average and current value in all metrics.\n\n        Args:\n            values: Dict of values to be added to the average.\n\n        Raises:\n            ValueError: If the value is not an instance of (Mapping).\n\n        Examples:\n            &gt;&gt;&gt; from danling.metrics.functional import auroc, auprc, pearson, spearman\n            &gt;&gt;&gt; metrics = MultiTaskMetrics()\n            &gt;&gt;&gt; metrics.dataset1.cls = Metrics(auroc=auroc, auprc=auprc)\n            &gt;&gt;&gt; metrics.dataset1.reg = Metrics(pearson=pearson, spearman=spearman)\n            &gt;&gt;&gt; metrics.dataset2 = Metrics(auroc=auroc, auprc=auprc)\n            &gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.2, 0.4, 0.5, 0.7], \"target\": [0, 1, 0, 1]}, \"dataset1.reg\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0.2, 0.3, 0.5, 0.7]}})\n            &gt;&gt;&gt; f\"{metrics:.4f}\"\n            'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: nan (nan)\\tauprc: nan (nan)'\n            &gt;&gt;&gt; metrics.update({\"dataset2\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 1, 0, 1]}})\n            &gt;&gt;&gt; f\"{metrics:.4f}\"\n            'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n            &gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 0, 1, 0]}})\n            &gt;&gt;&gt; f\"{metrics:.4f}\"\n            'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n            &gt;&gt;&gt; metrics.update({\"dataset1.reg\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0.2, 0.4, 0.6, 0.8]}})\n            &gt;&gt;&gt; f\"{metrics:.4f}\"\n            'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9898 (0.9146)\\tspearman: 1.0000 (0.9222)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n            &gt;&gt;&gt; metrics.update({\"dataset1\": {\"cls\": {\"input\": [0.1, 0.4, 0.6, 0.8]}}})\n            Traceback (most recent call last):\n            ValueError: Expected values to be a flat dictionary, but got &lt;class 'dict'&gt;\n            This is likely due to nested dictionary in the values.\n            Nested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both input and target. Ensure your input is a flat dictionary or a single value.\n            &gt;&gt;&gt; metrics.update(dict(loss=\"\"))\n            Traceback (most recent call last):\n            ValueError: Expected values to be a flat dictionary, but got &lt;class 'str'&gt;\n        \"\"\"  # noqa: E501\n\n        for metric, value in values.items():\n            if isinstance(value, Mapping):\n                if metric not in self:\n                    raise ValueError(f\"Metric {metric} not found in {self}\")\n                try:\n                    self[metric].update(**value)\n                except TypeError:\n                    raise ValueError(\n                        f\"Expected values to be a flat dictionary, but got {type(value)}\\n\"\n                        \"This is likely due to nested dictionary in the values.\\n\"\n                        \"Nested dictionaries cannot be processed due to the method's design, which uses Mapping \"\n                        \"to pass both input and target. Ensure your input is a flat dictionary or a single value.\"\n                    ) from None\n            else:\n                raise ValueError(f\"Expected values to be a flat dictionary, but got {type(value)}\")\n</code></pre>"},{"location":"package/#danling.MultiTaskMetrics.update","title":"<code>update(values)</code>","text":"<p>Updates the average and current value in all metrics.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Mapping[str, Mapping[str, Tensor]]</code> <p>Dict of values to be added to the average.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is not an instance of (Mapping).</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from danling.metrics.functional import auroc, auprc, pearson, spearman\n&gt;&gt;&gt; metrics = MultiTaskMetrics()\n&gt;&gt;&gt; metrics.dataset1.cls = Metrics(auroc=auroc, auprc=auprc)\n&gt;&gt;&gt; metrics.dataset1.reg = Metrics(pearson=pearson, spearman=spearman)\n&gt;&gt;&gt; metrics.dataset2 = Metrics(auroc=auroc, auprc=auprc)\n&gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.2, 0.4, 0.5, 0.7], \"target\": [0, 1, 0, 1]}, \"dataset1.reg\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0.2, 0.3, 0.5, 0.7]}})\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: nan (nan)\\tauprc: nan (nan)'\n&gt;&gt;&gt; metrics.update({\"dataset2\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 1, 0, 1]}})\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n&gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 0, 1, 0]}})\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n&gt;&gt;&gt; metrics.update({\"dataset1.reg\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0.2, 0.4, 0.6, 0.8]}})\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9898 (0.9146)\\tspearman: 1.0000 (0.9222)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n&gt;&gt;&gt; metrics.update({\"dataset1\": {\"cls\": {\"input\": [0.1, 0.4, 0.6, 0.8]}}})\nTraceback (most recent call last):\nValueError: Expected values to be a flat dictionary, but got &lt;class 'dict'&gt;\nThis is likely due to nested dictionary in the values.\nNested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both input and target. Ensure your input is a flat dictionary or a single value.\n&gt;&gt;&gt; metrics.update(dict(loss=\"\"))\nTraceback (most recent call last):\nValueError: Expected values to be a flat dictionary, but got &lt;class 'str'&gt;\n</code></pre> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>def update(self, values: Mapping[str, Mapping[str, Tensor]]) -&gt; None:  # pylint: disable=W0237\n    r\"\"\"\n    Updates the average and current value in all metrics.\n\n    Args:\n        values: Dict of values to be added to the average.\n\n    Raises:\n        ValueError: If the value is not an instance of (Mapping).\n\n    Examples:\n        &gt;&gt;&gt; from danling.metrics.functional import auroc, auprc, pearson, spearman\n        &gt;&gt;&gt; metrics = MultiTaskMetrics()\n        &gt;&gt;&gt; metrics.dataset1.cls = Metrics(auroc=auroc, auprc=auprc)\n        &gt;&gt;&gt; metrics.dataset1.reg = Metrics(pearson=pearson, spearman=spearman)\n        &gt;&gt;&gt; metrics.dataset2 = Metrics(auroc=auroc, auprc=auprc)\n        &gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.2, 0.4, 0.5, 0.7], \"target\": [0, 1, 0, 1]}, \"dataset1.reg\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0.2, 0.3, 0.5, 0.7]}})\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: nan (nan)\\tauprc: nan (nan)'\n        &gt;&gt;&gt; metrics.update({\"dataset2\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 1, 0, 1]}})\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n        &gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 0, 1, 0]}})\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n        &gt;&gt;&gt; metrics.update({\"dataset1.reg\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0.2, 0.4, 0.6, 0.8]}})\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9898 (0.9146)\\tspearman: 1.0000 (0.9222)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n        &gt;&gt;&gt; metrics.update({\"dataset1\": {\"cls\": {\"input\": [0.1, 0.4, 0.6, 0.8]}}})\n        Traceback (most recent call last):\n        ValueError: Expected values to be a flat dictionary, but got &lt;class 'dict'&gt;\n        This is likely due to nested dictionary in the values.\n        Nested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both input and target. Ensure your input is a flat dictionary or a single value.\n        &gt;&gt;&gt; metrics.update(dict(loss=\"\"))\n        Traceback (most recent call last):\n        ValueError: Expected values to be a flat dictionary, but got &lt;class 'str'&gt;\n    \"\"\"  # noqa: E501\n\n    for metric, value in values.items():\n        if isinstance(value, Mapping):\n            if metric not in self:\n                raise ValueError(f\"Metric {metric} not found in {self}\")\n            try:\n                self[metric].update(**value)\n            except TypeError:\n                raise ValueError(\n                    f\"Expected values to be a flat dictionary, but got {type(value)}\\n\"\n                    \"This is likely due to nested dictionary in the values.\\n\"\n                    \"Nested dictionaries cannot be processed due to the method's design, which uses Mapping \"\n                    \"to pass both input and target. Ensure your input is a flat dictionary or a single value.\"\n                ) from None\n        else:\n            raise ValueError(f\"Expected values to be a flat dictionary, but got {type(value)}\")\n</code></pre>"},{"location":"package/#danling.NestedTensor","title":"<code>NestedTensor</code>","text":"<p>Wrap an iterable of tensors into a single tensor with a mask.</p> <p>In sequence to sequence tasks, elements of a batch are usually not of the same length. This made it tricky to use a single tensor to represent a batch of sequences.</p> <p><code>NestedTensor</code> allows to store a sequence of tensors of different lengths in a single object. It also provides a mask that can be used to retrieve the original sequence of tensors.</p> <p>When calling <code>__getitem__(arg)</code> on a <code>NestedTensor</code>, it has two return type: 1. if arg is <code>int</code> or <code>slice</code>, returns a tuple of two <code>tensor</code>s, representing data and padding mask. 2. if arg is a <code>tuple</code>, return a new <code>NestedTensor</code> with specified shape.</p> <p>Attributes:</p> Name Type Description <code>_storage</code> <p>The sequence of tensors.</p> <code>tensor</code> <code>Tensor</code> <p>padded tensor.</p> <code>mask</code> <code>Tensor</code> <p>mask tensor.</p> <code>concat</code> <code>Tensor</code> <p>concatenated tensor.</p> <code>batch_first</code> <code>bool</code> <p>Whether the first dimension of the tensors is the batch dimension.</p> <p>If <code>True</code>, the first dimension is the batch dimension, i.e., <code>B, N, *</code>.</p> <p>If <code>False</code>, the first dimension is the sequence dimension, i.e., <code>N, B, *</code></p> <code>padding_value</code> <code>SupportsFloat</code> <p>The padding value used to in padded tensor.</p> <code>mask_value</code> <code>bool</code> <p>The mask value used in mask tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Iterable[Tensor]</code> <code>()</code> <code>batch_first</code> <code>bool</code> <code>True</code> <code>padding_value</code> <code>SupportsFloat</code> <code>0.0</code> <code>mask_value</code> <code>bool</code> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensors</code> is not an iterable.</p> <code>ValueError</code> <p>If <code>tensors</code> is empty.</p> Notes <p>We have rewritten the <code>__getattr__</code> function to support as much native tensor operations as possible. However, not all operations are tested.</p> <p>Please file an issue if you find any bugs.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor(torch.tensor([1, 2, 3]), torch.tensor([4, 5]))\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n&gt;&gt;&gt; nested_tensor.dtype\ntorch.int64\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n&gt;&gt;&gt; nested_tensor.concat\ntensor([1, 2, 3, 4, 5])\n&gt;&gt;&gt; nested_tensor.to(torch.float).tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]])\n&gt;&gt;&gt; nested_tensor.half().tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]], dtype=torch.float16)\n&gt;&gt;&gt; nested_tensor[:]\n(tensor([[1, 2, 3],\n        [4, 5, 0]]), tensor([[ True,  True,  True],\n        [ True,  True, False]]))\n&gt;&gt;&gt; nested_tensor[1]\n(tensor([4, 5]), tensor([True, True]))\n&gt;&gt;&gt; nested_tensor[:, 1:]\nNestedTensor([[2, 3],\n        [5, 0]])\n&gt;&gt;&gt; nested_tensor.tolist()\n[[1, 2, 3], [4, 5]]\n&gt;&gt;&gt; NestedTensor(*[[1, 2, 3], [4, 5]])\nNestedTensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class NestedTensor:\n    r\"\"\"\n    Wrap an iterable of tensors into a single tensor with a mask.\n\n    In sequence to sequence tasks, elements of a batch are usually not of the same length.\n    This made it tricky to use a single tensor to represent a batch of sequences.\n\n    `NestedTensor` allows to store a sequence of tensors of different lengths in a single object.\n    It also provides a mask that can be used to retrieve the original sequence of tensors.\n\n    When calling `__getitem__(arg)` on a `NestedTensor`, it has two return type:\n    1. if arg is `int` or `slice`, returns a tuple of two `tensor`s, representing data and padding mask.\n    2. if arg is a `tuple`, return a new `NestedTensor` with specified shape.\n\n    Attributes:\n        _storage: The sequence of tensors.\n        tensor: padded tensor.\n        mask: mask tensor.\n        concat: concatenated tensor.\n        batch_first:  Whether the first dimension of the tensors is the batch dimension.\n\n            If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.\n\n            If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`\n        padding_value: The padding value used to in padded tensor.\n        mask_value: The mask value used in mask tensor.\n\n    Args:\n        tensors:\n        batch_first:\n        padding_value:\n        mask_value:\n\n    Raises:\n        ValueError: If `tensors` is not an iterable.\n        ValueError: If `tensors` is empty.\n\n    Notes:\n        We have rewritten the `__getattr__` function to support as much native tensor operations as possible.\n        However, not all operations are tested.\n\n        Please file an issue if you find any bugs.\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor(torch.tensor([1, 2, 3]), torch.tensor([4, 5]))\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.device\n        device(type='cpu')\n        &gt;&gt;&gt; nested_tensor.dtype\n        torch.int64\n        &gt;&gt;&gt; nested_tensor.tensor\n        tensor([[1, 2, 3],\n                [4, 5, 0]])\n        &gt;&gt;&gt; nested_tensor.mask\n        tensor([[ True,  True,  True],\n                [ True,  True, False]])\n        &gt;&gt;&gt; nested_tensor.concat\n        tensor([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; nested_tensor.to(torch.float).tensor\n        tensor([[1., 2., 3.],\n                [4., 5., 0.]])\n        &gt;&gt;&gt; nested_tensor.half().tensor\n        tensor([[1., 2., 3.],\n                [4., 5., 0.]], dtype=torch.float16)\n        &gt;&gt;&gt; nested_tensor[:]\n        (tensor([[1, 2, 3],\n                [4, 5, 0]]), tensor([[ True,  True,  True],\n                [ True,  True, False]]))\n        &gt;&gt;&gt; nested_tensor[1]\n        (tensor([4, 5]), tensor([True, True]))\n        &gt;&gt;&gt; nested_tensor[:, 1:]\n        NestedTensor([[2, 3],\n                [5, 0]])\n        &gt;&gt;&gt; nested_tensor.tolist()\n        [[1, 2, 3], [4, 5]]\n        &gt;&gt;&gt; NestedTensor(*[[1, 2, 3], [4, 5]])\n        NestedTensor([[1, 2, 3],\n                [4, 5, 0]])\n    \"\"\"\n\n    __storage: Sequence[Tensor]\n    batch_first: bool = True\n    padding_value: SupportsFloat = 0.0\n    mask_value: bool = False\n\n    def __init__(\n        self,\n        *tensors: Iterable[Tensor],\n        batch_first: bool = True,\n        padding_value: SupportsFloat = 0.0,\n        mask_value: bool = False,\n    ) -&gt; None:\n        if len(tensors) == 1 and isinstance(tensors, Sequence):\n            tensors = tensors[0]  # type: ignore\n        self._storage = tensors\n        self.batch_first = batch_first\n        self.padding_value = padding_value\n        self.mask_value = mask_value\n\n    @property\n    def _storage(self):\n        return self.__storage\n\n    @_storage.setter\n    def _storage(self, tensors: Sequence):\n        if not isinstance(tensors, Iterable):\n            raise ValueError(f\"tensors must be an Iterable, bug got {type(tensors)}.\")\n        tensors = list(tensors)\n        if len(tensors) == 0:\n            raise ValueError(\"tensors must be a non-empty Iterable.\")\n        if not isinstance(tensors[0], Tensor):\n            tensors = [tensor(t) for t in tensors]\n        self.__storage = tensors\n\n    def storage(self):\n        return self._storage\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Return a single tensor by padding all the tensors.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.tensor\n            tensor([[1, 2, 3],\n                    [4, 5, 0]])\n        \"\"\"\n\n        return self._tensor(tuple(self._storage), self.batch_first, self.padding_value)\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Padding mask of `tensor`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.mask\n            tensor([[ True,  True,  True],\n                    [ True,  True, False]])\n        \"\"\"\n\n        return self._mask(tuple(self._storage), self.batch_first, self.mask_value)\n\n    @property\n    def concat(self) -&gt; Tensor:\n        r\"\"\"\n        Concat `tensor` in padding dim.\n\n        This is particularly useful when calculating loss or passing `Linear` to avoid unnecessary computation.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 8), torch.randn(11, 8)])\n            &gt;&gt;&gt; nested_tensor.concat.shape\n            torch.Size([20, 8])\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 9, 8), torch.randn(11, 11, 8)])\n            &gt;&gt;&gt; nested_tensor.concat.shape\n            torch.Size([202, 8])\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 9, 8, 6), torch.randn(11, 11, 8, 6)])\n            &gt;&gt;&gt; nested_tensor.concat.shape\n            torch.Size([202, 8, 6])\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 9, 8, 7), torch.randn(11, 11, 8, 6)])\n            &gt;&gt;&gt; nested_tensor.concat.shape\n            torch.Size([1293, 8])\n        \"\"\"\n        shape = list(self.size())  # type: ignore[arg-type]\n        shape = shape[1:] if self.batch_first else shape[0] + shape[2:]\n        elem = self._storage[0]\n        if elem.shape == shape:\n            return torch.cat(self._storage, dim=1 if self.batch_first else 0)\n\n        static_dims = set(range(len(shape)))\n        for i, s in enumerate(shape):\n            if not all(t.shape[i] == s for t in self._storage):\n                shape[i] = -1\n                static_dims.remove(i)\n        target_shape = [-1] + [s for s in shape if s != -1]\n        storage = [i.view(target_shape) for i in self._storage]\n        return torch.cat(storage, dim=0 if self.batch_first else 1)\n\n    @classmethod\n    def from_tensor_mask(cls, tensor: Tensor, mask: Tensor):\n        r\"\"\"\n        Build a `NestedTensor` object from a padded `Tensor` and corresponding mask `Tensor`.\n\n        Args:\n            tensor: Padded Tensor.\n            mask: Tensor Mask.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; padded_tensor = torch.tensor([[1, 2, 3, 0, 0],\n            ...                                [4, 5, 0, 0, 0],\n            ...                                [6, 7, 8, 9, 0]])\n            &gt;&gt;&gt; mask_tensor = torch.tensor([[1, 1, 1, 0, 0],\n            ...                             [1, 1, 0, 0, 0],\n            ...                             [1, 1, 1, 1, 0]])\n            &gt;&gt;&gt; nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)\n            &gt;&gt;&gt; nested_tensor\n            NestedTensor([[1, 2, 3, 0],\n                    [4, 5, 0, 0],\n                    [6, 7, 8, 9]])\n        \"\"\"\n\n        if mask.ndim == 2:\n            return cls(t[slice(0, m.sum())] for t, m in zip(tensor, mask))\n        return cls(\n            t[[slice(0, (m.sum(dim=dim) &gt; 0).sum().item()) for dim in reversed(range(m.dim()))]]\n            for t, m in zip(tensor, mask)\n        )\n\n    def nested_like(self, tensor: Tensor, strict: bool = True) -&gt; NestedTensor:\n        r\"\"\"\n        Create a new `NestedTensor` from a `Tensor`.\n        The newly created `NestedTensor` will have the same shape as current `NestedTensor`.\n\n        Args:\n            tensor: The tensor to be converted to `NestedTensor`.\n            strict: Check if the shape of `tensor` is the same as the current `NestedTensor`.\n\n        Returns:\n            (NestedTensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; (nested_tensor == nested_tensor.nested_like(nested_tensor)).all()\n            tensor(True)\n            &gt;&gt;&gt; tensor = nested_tensor.tensor\n            &gt;&gt;&gt; (nested_tensor == nested_tensor.nested_like(tensor)).all()\n            tensor(True)\n            &gt;&gt;&gt; f = nested_tensor.nested_like(torch.randn(2, 2))\n            Traceback (most recent call last):\n            ValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])\n            &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(2, 2), False)\n            &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(3, 3), False)\n            Traceback (most recent call last):\n            ValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\n        \"\"\"  # noqa: E501\n\n        if isinstance(tensor, NestedTensor):\n            return tensor.clone()\n\n        if strict and self.shape != tensor.shape:\n            raise ValueError(\n                f\"The shape of NestedTensor and input tensor does not match, {self.shape} != {tensor.shape}\"\n            )\n        if self.size(0) != tensor.size(0):\n            raise ValueError(\n                f\"The batch size of NestedTensor and input tensor does not match, {self.size(0)} != {tensor.size(0)}\"\n            )\n        return NestedTensor([o[tuple(slice(0, dim) for dim in t.shape)] for t, o in zip(self._storage, tensor)])\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of the NestedTensor.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.device\n            device(type='cpu')\n        \"\"\"\n\n        return self._device(tuple(self._storage))\n\n    @property\n    def shape(self) -&gt; torch.Size | int:\n        r\"\"\"\n        Alias for `size()`.\n        \"\"\"\n\n        return self.size()\n\n    @property\n    def ndim(self) -&gt; int:\n        r\"\"\"\n        Alias for `dim()`.\n        \"\"\"\n\n        return self.dim()\n\n    def size(self, dim: int | None = None) -&gt; torch.Size | int:\n        r\"\"\"\n        Returns the size of the self `NestedTensor`.\n\n        Args:\n            dim: If not specified, the returned value is a `torch.Size`, a subclass of `tuple`.\n                If specified, returns an `int` holding the size of that dimension.\n                Defaults to `None`.\n\n        Returns:\n            (torch.Size | int):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.size()\n            torch.Size([2, 3])\n            &gt;&gt;&gt; nested_tensor.size(0)\n            2\n            &gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n            &gt;&gt;&gt; nested_tensor.shape\n            torch.Size([2, 4])\n            &gt;&gt;&gt; nested_tensor.size(1)\n            4\n        \"\"\"\n\n        return self._size(tuple(self._storage), dim, self.batch_first)\n\n    def dim(self) -&gt; int:\n        r\"\"\"\n        Number of dimension of the NestedTensor.\n\n        Returns:\n            (int):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.dim()\n            2\n            &gt;&gt;&gt; nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))\n            &gt;&gt;&gt; nested_tensor.ndim\n            2\n        \"\"\"\n\n        return self._dim(tuple(self._storage))\n\n    def tolist(self) -&gt; list:\n        r\"\"\"\n        Convert a NestedTensor to a list of lists of values.\n\n        Returns:\n            (list):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.tolist()\n            [[1, 2, 3], [4, 5]]\n        \"\"\"\n\n        return [t.tolist() for t in self._storage]\n\n    def all(self, dim: int | None = None, keepdim: bool = False) -&gt; bool | Tensor | NestedTensor:\n        r\"\"\"\n        Tests if all elements in NestedTensor evaluate to True.\n\n        Returns:\n            (bool | Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.ones(2, 4, dtype=torch.bool), torch.ones(3, 5, dtype=torch.bool)])\n            &gt;&gt;&gt; nested_tensor.all()\n            tensor(True)\n            &gt;&gt;&gt; nested_tensor.all(dim=0)\n            tensor([True, True])\n            &gt;&gt;&gt; nested_tensor.all(dim=0, keepdim=True)\n            tensor([[True, True]])\n            &gt;&gt;&gt; nested_tensor.all(dim=1)\n            NestedTensor([[ True,  True,  True,  True, False],\n                    [ True,  True,  True,  True,  True]])\n            &gt;&gt;&gt; nested_tensor.all(dim=1, keepdim=True)\n            NestedTensor([[[ True,  True,  True,  True, False]],\n            &lt;BLANKLINE&gt;\n                    [[ True,  True,  True,  True,  True]]])\n            &gt;&gt;&gt; nested_tensor.batch_first = False\n            &gt;&gt;&gt; nested_tensor.all(dim=1)\n            tensor([True, True])\n            &gt;&gt;&gt; nested_tensor.batch_first = False\n            &gt;&gt;&gt; nested_tensor.all(dim=0)\n            NestedTensor([[ True,  True,  True,  True, False],\n                    [ True,  True,  True,  True,  True]])\n            &gt;&gt;&gt; nested_tensor.all(dim=1)\n            tensor([True, True])\n        \"\"\"\n\n        if dim is None:\n            return torch.tensor(all(i.all() for i in self._storage))\n        if (self.batch_first and dim == 0) or (not self.batch_first and dim == 1):\n            if keepdim:\n                return torch.tensor([i.all() for i in self._storage]).unsqueeze(0 if self.batch_first else 1)\n            return torch.tensor([i.all() for i in self._storage])\n        if self.batch_first or dim != 0:\n            dim -= 1\n        return NestedTensor([i.all(dim=dim, keepdim=keepdim) for i in self._storage])\n\n    def where(self, condition: Tensor | NestedTensor, other: Tensor | NestedTensor | SupportsFloat) -&gt; NestedTensor:\n        r\"\"\"\n        Return a NestedTensor of elements selected from either self or other, depending on condition.\n\n        Returns:\n            (NestedTensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.where(nested_tensor &gt; 2, torch.tensor([[6, 5, 4], [3, 2, 1]]))\n            NestedTensor([[6, 5, 3],\n                    [4, 5, 0]])\n            &gt;&gt;&gt; nested_tensor.where(nested_tensor &gt; 2, NestedTensor([[6, 5, 4], [3, 2]]))\n            NestedTensor([[6, 5, 3],\n                    [4, 5, 0]])\n            &gt;&gt;&gt; nested_tensor.where(torch.tensor(True), NestedTensor([[6, 5, 4], [3, 2]]))\n            NestedTensor([[1, 2, 3],\n                    [4, 5, 0]])\n        \"\"\"\n\n        if isinstance(condition, Tensor) and self.shape == condition.shape:\n            condition = self.nested_like(condition)\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(condition, NestedTensor) and isinstance(other, NestedTensor):\n            return NestedTensor(\n                [x.where(c, y) for x, c, y in zip(self._storage, condition._storage, other._storage)], **self._state\n            )\n        if isinstance(condition, NestedTensor):\n            return NestedTensor([x.where(c, other) for x, c in zip(self._storage, condition._storage)], **self._state)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x.where(condition, y) for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor(x.where(condition, other) for x in self._storage)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        if func not in NestedTensorFunc or not all(issubclass(t, (torch.Tensor, NestedTensor)) for t in types):\n            args = [a.tensor if hasattr(a, \"tensor\") else a for a in args]\n            return func(*args, **kwargs)\n        return NestedTensorFunc[func](*args, **kwargs)\n\n    def __getitem__(self, index: int | slice | tuple) -&gt; tuple[Tensor, Tensor] | NestedTensor:\n        if isinstance(index, tuple):\n            return NestedTensor([t[index[0]][index[1:]] for t in self._storage])\n        if isinstance(index, (int, slice)):\n            ret = self._storage[index]\n            if isinstance(ret, Tensor):\n                return ret, torch.ones_like(ret, dtype=torch.bool)\n            return self.tensor, self.mask\n        raise ValueError(f\"Unsupported index type {type(index)}\")\n\n    def __getattr__(self, name: str) -&gt; Any:\n        if not self._storage:\n            raise ValueError(f\"Unable to get {name} from an empty {self.__class__.__name__}\")\n        ret = [getattr(i, name) for i in self._storage]\n        elem = ret[0]\n        if isinstance(elem, Tensor):\n            return NestedTensor(ret, **self._state)\n        if callable(elem):\n            return NestedTensorFuncWrapper(ret, state=self._state)\n        if elem.__hash__ is not None and len(set(ret)) == 1:\n            return elem\n        return ret\n\n    @property\n    def _state(self) -&gt; Mapping:\n        return {k: v for k, v in self.__dict__.items() if not (k.startswith(\"_\") or k.endswith(\"_\"))}\n\n    def __state__(self) -&gt; Mapping:\n        return self.__dict__\n\n    def __setstate__(self, state: Mapping) -&gt; None:\n        self.__dict__.update(state)\n\n    def __len__(self) -&gt; int:\n        return len(self._storage)\n\n    def __repr__(self):\n        return self.__class__.__name__ + repr(self.tensor)[len(self.tensor.__class__.__name__) :]  # noqa: E203\n\n    def __bool__(self) -&gt; int:\n        return all(bool(x) for x in self._storage)\n\n    def __gt__(  # type: ignore[override]\n        self, other: Tensor | NestedTensor | SupportsFloat\n    ) -&gt; bool | Tensor | NestedTensor:\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor(i &gt; j for i, j in zip(self._storage, other._storage))\n        if isinstance(other, (int, float, Tensor)):\n            return NestedTensor([x &gt; other for x in self._storage], **self._state)\n        raise TypeError(f\"&gt; not supported between instances of '{type(self)}' and '{type(other)}'\")\n\n    def __ge__(  # type: ignore[override]\n        self, other: Tensor | NestedTensor | SupportsFloat\n    ) -&gt; bool | Tensor | NestedTensor:\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor(i &gt;= j for i, j in zip(self._storage, other._storage))\n        if isinstance(other, (int, float, Tensor)):\n            return NestedTensor([x &gt;= other for x in self._storage], **self._state)\n        raise TypeError(f\"&gt;= not supported between instances of '{type(self)}' and '{type(other)}'\")\n\n    def __eq__(  # type: ignore[override]\n        self, other: Tensor | NestedTensor | SupportsFloat\n    ) -&gt; bool | Tensor | NestedTensor:\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor(i == j for i, j in zip(self._storage, other._storage))\n        if isinstance(other, (int, float, Tensor)):\n            return NestedTensor([x == other for x in self._storage], **self._state)\n        return False\n\n    def __le__(  # type: ignore[override]\n        self, other: Tensor | NestedTensor | SupportsFloat\n    ) -&gt; bool | Tensor | NestedTensor:\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor(i &lt;= j for i, j in zip(self._storage, other._storage))\n        if isinstance(other, (int, float, Tensor)):\n            return NestedTensor([x &lt;= other for x in self._storage], **self._state)\n        raise TypeError(f\"&lt;= not supported between instances of '{type(self)}' and '{type(other)}'\")\n\n    def __lt__(  # type: ignore[override]\n        self, other: Tensor | NestedTensor | SupportsFloat\n    ) -&gt; bool | Tensor | NestedTensor:\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor(i &lt; j for i, j in zip(self._storage, other._storage))\n        if isinstance(other, (int, float, Tensor)):\n            return NestedTensor([x &lt; other for x in self._storage], **self._state)\n        raise TypeError(f\"&lt; not supported between instances of '{type(self)}' and '{type(other)}'\")\n\n    def __abs__(self):\n        return NestedTensor([abs(value) for value in self._storage], **self._state)\n\n    def __add__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x + y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value + other for value in self._storage], **self._state)\n\n    def __radd__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y + x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other + value for value in self._storage], **self._state)\n\n    def __iadd__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x += y\n        else:\n            for value in self._storage:\n                value += other\n        return self\n\n    def __sub__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x - y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value - other for value in self._storage], **self._state)\n\n    def __rsub__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y - x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other - value for value in self._storage], **self._state)\n\n    def __isub__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x -= y\n        else:\n            for value in self._storage:\n                value -= other\n        return self\n\n    def __pos__(self):\n        return NestedTensor([+x for x in self._storage])\n\n    def __neg__(self):\n        return NestedTensor([-x for x in self._storage])\n\n    def __invert__(self):\n        return NestedTensor([~x for x in self._storage])\n\n    def __mul__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x * y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value * other for value in self._storage], **self._state)\n\n    def __rmul__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y * x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other * value for value in self._storage], **self._state)\n\n    def __imul__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x *= y\n        else:\n            for value in self._storage:\n                value *= other\n        return self\n\n    def __pow__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x**y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value**other for value in self._storage], **self._state)\n\n    def __rpow__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y**x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other**value for value in self._storage], **self._state)\n\n    def __ipow__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x **= y\n        else:\n            for value in self._storage:\n                value **= other\n        return self\n\n    def __matmul__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x @ y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value @ other for value in self._storage], **self._state)\n\n    def __rmatmul__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y @ x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other @ value for value in self._storage], **self._state)\n\n    def __imatmul__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x @= y\n        else:\n            for value in self._storage:\n                value @= other\n        return self\n\n    def __truediv__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x / y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value / other for value in self._storage], **self._state)\n\n    def __rtruediv__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y / x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other / value for value in self._storage], **self._state)\n\n    def __itruediv__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x /= y\n        else:\n            for value in self._storage:\n                value /= other\n        return self\n\n    def __floordiv__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x // y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value // other for value in self._storage], **self._state)\n\n    def __rfloordiv__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y // x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other // value for value in self._storage], **self._state)\n\n    def __ifloordiv__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x //= y\n        else:\n            for value in self._storage:\n                value //= other\n        return self\n\n    def __mod__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x % y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value % other for value in self._storage], **self._state)\n\n    def __rmod__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y % x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other % value for value in self._storage], **self._state)\n\n    def __imod__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x %= y\n        else:\n            for value in self._storage:\n                value %= other\n        return self\n\n    def __and__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x &amp; y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value &amp; other for value in self._storage], **self._state)\n\n    def __rand__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y &amp; x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other &amp; value for value in self._storage], **self._state)\n\n    def __iand__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x &amp;= y\n        else:\n            for value in self._storage:\n                value &amp;= other\n        return self\n\n    def __or__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x | y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value | other for value in self._storage], **self._state)\n\n    def __ror__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y | x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other | value for value in self._storage], **self._state)\n\n    def __ior__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x |= y\n        else:\n            for value in self._storage:\n                value |= other\n        return self\n\n    def __xor__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x ^ y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value ^ other for value in self._storage], **self._state)\n\n    def __rxor__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y ^ x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other ^ value for value in self._storage], **self._state)\n\n    def __ixor__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x ^= y\n        else:\n            for value in self._storage:\n                value ^= other\n        return self\n\n    def __lshift__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x &lt;&lt; y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value &lt;&lt; other for value in self._storage], **self._state)\n\n    def __rlshift__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y &lt;&lt; x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other &lt;&lt; value for value in self._storage], **self._state)\n\n    def __ilshift__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x &lt;&lt;= y\n        else:\n            for value in self._storage:\n                value &lt;&lt;= other\n        return self\n\n    def __rshift__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x &gt;&gt; y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value &gt;&gt; other for value in self._storage], **self._state)\n\n    def __rrshift__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y &gt;&gt; x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other &gt;&gt; value for value in self._storage], **self._state)\n\n    def __irshift__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x &gt;&gt;= y\n        else:\n            for value in self._storage:\n                value &gt;&gt;= other\n        return self\n\n    @method_cache(maxsize=1)\n    def _tensor(self, storage: tuple, batch_first: bool, padding_value: SupportsFloat) -&gt; Tensor:\n        if storage[0].dim() == 0:\n            return torch.stack(storage, dim=0)\n        return pad_tensor(storage, size=self.size(), batch_first=batch_first, padding_value=float(padding_value))\n\n    @method_cache(maxsize=1)\n    def _mask(self, storage: tuple, batch_first: bool, mask_value: bool) -&gt; Tensor:\n        if storage[0].dim() == 0:\n            return torch.full((len(storage),), not mask_value, dtype=torch.bool, device=self.device)\n        size = self.size()\n        # ignore channel dimension\n        if storage[0].dim() &gt; 1 and len({t.size(-1) for t in storage}) == 1:\n            size = size[:-1]  # type: ignore\n        return mask_tensor(storage, size=size, batch_first=batch_first, mask_value=mask_value)\n\n    @method_cache(maxsize=1)\n    def _device(self, storage) -&gt; torch.device:\n        return storage[0].device\n\n    @method_cache(maxsize=1)\n    def _size(self, storage, dim: int | None = None, batch_first: bool = True) -&gt; torch.Size | int:\n        if dim is not None:\n            if dim == 0:\n                return len(storage)\n            return max(t.size(dim - 1) for t in storage)\n        if max(t.dim() for t in storage) == 0:\n            return torch.Size((len(storage),))\n        ndim = max(t.dim() for t in storage)\n        size = [max(t.shape[i] if i &lt; len(t.shape) else 0 for t in storage) for i in range(ndim)]\n        size.insert(0 if batch_first else 1, len(storage))\n        return torch.Size(size)\n\n    @method_cache(maxsize=1)\n    def _dim(self, storage) -&gt; torch.Size:\n        return max(t.dim() for t in storage) + 1\n</code></pre>"},{"location":"package/#danling.NestedTensor.concat","title":"<code>concat: Tensor</code>  <code>property</code>","text":"<p>Concat <code>tensor</code> in padding dim.</p> <p>This is particularly useful when calculating loss or passing <code>Linear</code> to avoid unnecessary computation.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 8), torch.randn(11, 8)])\n&gt;&gt;&gt; nested_tensor.concat.shape\ntorch.Size([20, 8])\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 9, 8), torch.randn(11, 11, 8)])\n&gt;&gt;&gt; nested_tensor.concat.shape\ntorch.Size([202, 8])\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 9, 8, 6), torch.randn(11, 11, 8, 6)])\n&gt;&gt;&gt; nested_tensor.concat.shape\ntorch.Size([202, 8, 6])\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 9, 8, 7), torch.randn(11, 11, 8, 6)])\n&gt;&gt;&gt; nested_tensor.concat.shape\ntorch.Size([1293, 8])\n</code></pre>"},{"location":"package/#danling.NestedTensor.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Device of the NestedTensor.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n</code></pre>"},{"location":"package/#danling.NestedTensor.mask","title":"<code>mask: Tensor</code>  <code>property</code>","text":"<p>Padding mask of <code>tensor</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n</code></pre>"},{"location":"package/#danling.NestedTensor.ndim","title":"<code>ndim: int</code>  <code>property</code>","text":"<p>Alias for <code>dim()</code>.</p>"},{"location":"package/#danling.NestedTensor.shape","title":"<code>shape: torch.Size | int</code>  <code>property</code>","text":"<p>Alias for <code>size()</code>.</p>"},{"location":"package/#danling.NestedTensor.tensor","title":"<code>tensor: Tensor</code>  <code>property</code>","text":"<p>Return a single tensor by padding all the tensors.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre>"},{"location":"package/#danling.NestedTensor.all","title":"<code>all(dim=None, keepdim=False)</code>","text":"<p>Tests if all elements in NestedTensor evaluate to True.</p> <p>Returns:</p> Type Description <code>bool | Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.ones(2, 4, dtype=torch.bool), torch.ones(3, 5, dtype=torch.bool)])\n&gt;&gt;&gt; nested_tensor.all()\ntensor(True)\n&gt;&gt;&gt; nested_tensor.all(dim=0)\ntensor([True, True])\n&gt;&gt;&gt; nested_tensor.all(dim=0, keepdim=True)\ntensor([[True, True]])\n&gt;&gt;&gt; nested_tensor.all(dim=1)\nNestedTensor([[ True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True]])\n&gt;&gt;&gt; nested_tensor.all(dim=1, keepdim=True)\nNestedTensor([[[ True,  True,  True,  True, False]],\n\n        [[ True,  True,  True,  True,  True]]])\n&gt;&gt;&gt; nested_tensor.batch_first = False\n&gt;&gt;&gt; nested_tensor.all(dim=1)\ntensor([True, True])\n&gt;&gt;&gt; nested_tensor.batch_first = False\n&gt;&gt;&gt; nested_tensor.all(dim=0)\nNestedTensor([[ True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True]])\n&gt;&gt;&gt; nested_tensor.all(dim=1)\ntensor([True, True])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def all(self, dim: int | None = None, keepdim: bool = False) -&gt; bool | Tensor | NestedTensor:\n    r\"\"\"\n    Tests if all elements in NestedTensor evaluate to True.\n\n    Returns:\n        (bool | Tensor):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.ones(2, 4, dtype=torch.bool), torch.ones(3, 5, dtype=torch.bool)])\n        &gt;&gt;&gt; nested_tensor.all()\n        tensor(True)\n        &gt;&gt;&gt; nested_tensor.all(dim=0)\n        tensor([True, True])\n        &gt;&gt;&gt; nested_tensor.all(dim=0, keepdim=True)\n        tensor([[True, True]])\n        &gt;&gt;&gt; nested_tensor.all(dim=1)\n        NestedTensor([[ True,  True,  True,  True, False],\n                [ True,  True,  True,  True,  True]])\n        &gt;&gt;&gt; nested_tensor.all(dim=1, keepdim=True)\n        NestedTensor([[[ True,  True,  True,  True, False]],\n        &lt;BLANKLINE&gt;\n                [[ True,  True,  True,  True,  True]]])\n        &gt;&gt;&gt; nested_tensor.batch_first = False\n        &gt;&gt;&gt; nested_tensor.all(dim=1)\n        tensor([True, True])\n        &gt;&gt;&gt; nested_tensor.batch_first = False\n        &gt;&gt;&gt; nested_tensor.all(dim=0)\n        NestedTensor([[ True,  True,  True,  True, False],\n                [ True,  True,  True,  True,  True]])\n        &gt;&gt;&gt; nested_tensor.all(dim=1)\n        tensor([True, True])\n    \"\"\"\n\n    if dim is None:\n        return torch.tensor(all(i.all() for i in self._storage))\n    if (self.batch_first and dim == 0) or (not self.batch_first and dim == 1):\n        if keepdim:\n            return torch.tensor([i.all() for i in self._storage]).unsqueeze(0 if self.batch_first else 1)\n        return torch.tensor([i.all() for i in self._storage])\n    if self.batch_first or dim != 0:\n        dim -= 1\n    return NestedTensor([i.all(dim=dim, keepdim=keepdim) for i in self._storage])\n</code></pre>"},{"location":"package/#danling.NestedTensor.dim","title":"<code>dim()</code>","text":"<p>Number of dimension of the NestedTensor.</p> <p>Returns:</p> Type Description <code>int</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.dim()\n2\n&gt;&gt;&gt; nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))\n&gt;&gt;&gt; nested_tensor.ndim\n2\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def dim(self) -&gt; int:\n    r\"\"\"\n    Number of dimension of the NestedTensor.\n\n    Returns:\n        (int):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.dim()\n        2\n        &gt;&gt;&gt; nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))\n        &gt;&gt;&gt; nested_tensor.ndim\n        2\n    \"\"\"\n\n    return self._dim(tuple(self._storage))\n</code></pre>"},{"location":"package/#danling.NestedTensor.from_tensor_mask","title":"<code>from_tensor_mask(tensor, mask)</code>  <code>classmethod</code>","text":"<p>Build a <code>NestedTensor</code> object from a padded <code>Tensor</code> and corresponding mask <code>Tensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Padded Tensor.</p> required <code>mask</code> <code>Tensor</code> <p>Tensor Mask.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; padded_tensor = torch.tensor([[1, 2, 3, 0, 0],\n...                                [4, 5, 0, 0, 0],\n...                                [6, 7, 8, 9, 0]])\n&gt;&gt;&gt; mask_tensor = torch.tensor([[1, 1, 1, 0, 0],\n...                             [1, 1, 0, 0, 0],\n...                             [1, 1, 1, 1, 0]])\n&gt;&gt;&gt; nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)\n&gt;&gt;&gt; nested_tensor\nNestedTensor([[1, 2, 3, 0],\n        [4, 5, 0, 0],\n        [6, 7, 8, 9]])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@classmethod\ndef from_tensor_mask(cls, tensor: Tensor, mask: Tensor):\n    r\"\"\"\n    Build a `NestedTensor` object from a padded `Tensor` and corresponding mask `Tensor`.\n\n    Args:\n        tensor: Padded Tensor.\n        mask: Tensor Mask.\n\n    Returns:\n        (torch.Tensor):\n\n    Examples:\n        &gt;&gt;&gt; padded_tensor = torch.tensor([[1, 2, 3, 0, 0],\n        ...                                [4, 5, 0, 0, 0],\n        ...                                [6, 7, 8, 9, 0]])\n        &gt;&gt;&gt; mask_tensor = torch.tensor([[1, 1, 1, 0, 0],\n        ...                             [1, 1, 0, 0, 0],\n        ...                             [1, 1, 1, 1, 0]])\n        &gt;&gt;&gt; nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)\n        &gt;&gt;&gt; nested_tensor\n        NestedTensor([[1, 2, 3, 0],\n                [4, 5, 0, 0],\n                [6, 7, 8, 9]])\n    \"\"\"\n\n    if mask.ndim == 2:\n        return cls(t[slice(0, m.sum())] for t, m in zip(tensor, mask))\n    return cls(\n        t[[slice(0, (m.sum(dim=dim) &gt; 0).sum().item()) for dim in reversed(range(m.dim()))]]\n        for t, m in zip(tensor, mask)\n    )\n</code></pre>"},{"location":"package/#danling.NestedTensor.nested_like","title":"<code>nested_like(tensor, strict=True)</code>","text":"<p>Create a new <code>NestedTensor</code> from a <code>Tensor</code>. The newly created <code>NestedTensor</code> will have the same shape as current <code>NestedTensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to be converted to <code>NestedTensor</code>.</p> required <code>strict</code> <code>bool</code> <p>Check if the shape of <code>tensor</code> is the same as the current <code>NestedTensor</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>NestedTensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; (nested_tensor == nested_tensor.nested_like(nested_tensor)).all()\ntensor(True)\n&gt;&gt;&gt; tensor = nested_tensor.tensor\n&gt;&gt;&gt; (nested_tensor == nested_tensor.nested_like(tensor)).all()\ntensor(True)\n&gt;&gt;&gt; f = nested_tensor.nested_like(torch.randn(2, 2))\nTraceback (most recent call last):\nValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])\n&gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(2, 2), False)\n&gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(3, 3), False)\nTraceback (most recent call last):\nValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def nested_like(self, tensor: Tensor, strict: bool = True) -&gt; NestedTensor:\n    r\"\"\"\n    Create a new `NestedTensor` from a `Tensor`.\n    The newly created `NestedTensor` will have the same shape as current `NestedTensor`.\n\n    Args:\n        tensor: The tensor to be converted to `NestedTensor`.\n        strict: Check if the shape of `tensor` is the same as the current `NestedTensor`.\n\n    Returns:\n        (NestedTensor):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; (nested_tensor == nested_tensor.nested_like(nested_tensor)).all()\n        tensor(True)\n        &gt;&gt;&gt; tensor = nested_tensor.tensor\n        &gt;&gt;&gt; (nested_tensor == nested_tensor.nested_like(tensor)).all()\n        tensor(True)\n        &gt;&gt;&gt; f = nested_tensor.nested_like(torch.randn(2, 2))\n        Traceback (most recent call last):\n        ValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])\n        &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(2, 2), False)\n        &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(3, 3), False)\n        Traceback (most recent call last):\n        ValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\n    \"\"\"  # noqa: E501\n\n    if isinstance(tensor, NestedTensor):\n        return tensor.clone()\n\n    if strict and self.shape != tensor.shape:\n        raise ValueError(\n            f\"The shape of NestedTensor and input tensor does not match, {self.shape} != {tensor.shape}\"\n        )\n    if self.size(0) != tensor.size(0):\n        raise ValueError(\n            f\"The batch size of NestedTensor and input tensor does not match, {self.size(0)} != {tensor.size(0)}\"\n        )\n    return NestedTensor([o[tuple(slice(0, dim) for dim in t.shape)] for t, o in zip(self._storage, tensor)])\n</code></pre>"},{"location":"package/#danling.NestedTensor.size","title":"<code>size(dim=None)</code>","text":"<p>Returns the size of the self <code>NestedTensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int | None</code> <p>If not specified, the returned value is a <code>torch.Size</code>, a subclass of <code>tuple</code>. If specified, returns an <code>int</code> holding the size of that dimension. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Size | int</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.size(0)\n2\n&gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 4])\n&gt;&gt;&gt; nested_tensor.size(1)\n4\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def size(self, dim: int | None = None) -&gt; torch.Size | int:\n    r\"\"\"\n    Returns the size of the self `NestedTensor`.\n\n    Args:\n        dim: If not specified, the returned value is a `torch.Size`, a subclass of `tuple`.\n            If specified, returns an `int` holding the size of that dimension.\n            Defaults to `None`.\n\n    Returns:\n        (torch.Size | int):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.size(0)\n        2\n        &gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 4])\n        &gt;&gt;&gt; nested_tensor.size(1)\n        4\n    \"\"\"\n\n    return self._size(tuple(self._storage), dim, self.batch_first)\n</code></pre>"},{"location":"package/#danling.NestedTensor.tolist","title":"<code>tolist()</code>","text":"<p>Convert a NestedTensor to a list of lists of values.</p> <p>Returns:</p> Type Description <code>list</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.tolist()\n[[1, 2, 3], [4, 5]]\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def tolist(self) -&gt; list:\n    r\"\"\"\n    Convert a NestedTensor to a list of lists of values.\n\n    Returns:\n        (list):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.tolist()\n        [[1, 2, 3], [4, 5]]\n    \"\"\"\n\n    return [t.tolist() for t in self._storage]\n</code></pre>"},{"location":"package/#danling.NestedTensor.where","title":"<code>where(condition, other)</code>","text":"<p>Return a NestedTensor of elements selected from either self or other, depending on condition.</p> <p>Returns:</p> Type Description <code>NestedTensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.where(nested_tensor &gt; 2, torch.tensor([[6, 5, 4], [3, 2, 1]]))\nNestedTensor([[6, 5, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.where(nested_tensor &gt; 2, NestedTensor([[6, 5, 4], [3, 2]]))\nNestedTensor([[6, 5, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.where(torch.tensor(True), NestedTensor([[6, 5, 4], [3, 2]]))\nNestedTensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def where(self, condition: Tensor | NestedTensor, other: Tensor | NestedTensor | SupportsFloat) -&gt; NestedTensor:\n    r\"\"\"\n    Return a NestedTensor of elements selected from either self or other, depending on condition.\n\n    Returns:\n        (NestedTensor):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.where(nested_tensor &gt; 2, torch.tensor([[6, 5, 4], [3, 2, 1]]))\n        NestedTensor([[6, 5, 3],\n                [4, 5, 0]])\n        &gt;&gt;&gt; nested_tensor.where(nested_tensor &gt; 2, NestedTensor([[6, 5, 4], [3, 2]]))\n        NestedTensor([[6, 5, 3],\n                [4, 5, 0]])\n        &gt;&gt;&gt; nested_tensor.where(torch.tensor(True), NestedTensor([[6, 5, 4], [3, 2]]))\n        NestedTensor([[1, 2, 3],\n                [4, 5, 0]])\n    \"\"\"\n\n    if isinstance(condition, Tensor) and self.shape == condition.shape:\n        condition = self.nested_like(condition)\n    if isinstance(other, Tensor) and self.shape == other.shape:\n        other = self.nested_like(other)\n    if isinstance(condition, NestedTensor) and isinstance(other, NestedTensor):\n        return NestedTensor(\n            [x.where(c, y) for x, c, y in zip(self._storage, condition._storage, other._storage)], **self._state\n        )\n    if isinstance(condition, NestedTensor):\n        return NestedTensor([x.where(c, other) for x, c in zip(self._storage, condition._storage)], **self._state)\n    if isinstance(other, NestedTensor):\n        return NestedTensor([x.where(condition, y) for x, y in zip(self._storage, other._storage)], **self._state)\n    return NestedTensor(x.where(condition, other) for x in self._storage)\n</code></pre>"},{"location":"package/#danling.PNTensor","title":"<code>PNTensor</code>","text":"<p>               Bases: <code>Tensor</code></p> <p>Wrapper for tensors to be converted to <code>NestedTensor</code>.</p> <p><code>PNTensor</code> is a subclass of <code>torch.Tensor</code>. It implements three additional property as <code>NestedTensor</code>: <code>tensor</code>, <code>mask</code>, and <code>concat</code>.</p> <p>Although it is possible to directly construct <code>NestedTensor</code> in dataset, the best practice is to do so is in <code>collate_fn</code>. <code>PNTensor</code> is introduced to smoothen the process.</p> <p>Convert tensors that will be converted to <code>NestedTensor</code> to a <code>PNTensor</code>, and PyTorch Dataloader will automatically collate <code>PNTensor</code> to <code>NestedTensor</code>.</p> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class PNTensor(Tensor):\n    r\"\"\"\n    Wrapper for tensors to be converted to `NestedTensor`.\n\n    `PNTensor` is a subclass of `torch.Tensor`.\n    It implements three additional property as `NestedTensor`: `tensor`, `mask`, and `concat`.\n\n    Although it is possible to directly construct `NestedTensor` in dataset,\n    the best practice is to do so is in `collate_fn`.\n    `PNTensor` is introduced to smoothen the process.\n\n    Convert tensors that will be converted to `NestedTensor` to a `PNTensor`,\n    and PyTorch Dataloader will automatically collate `PNTensor` to `NestedTensor`.\n    \"\"\"\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Identical to `self`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n            &gt;&gt;&gt; pn_tensor = PNTensor([1, 2, 3])\n            &gt;&gt;&gt; bool((tensor == pn_tensor).all())\n            True\n            &gt;&gt;&gt; bool((tensor == pn_tensor.tensor).all())\n            True\n        \"\"\"\n\n        return self\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Identical to `torch.ones_like(self)`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n            &gt;&gt;&gt; pn_tensor = PNTensor([1, 2, 3])\n            &gt;&gt;&gt; bool((pn_tensor.mask == torch.ones_like(pn_tensor)).all().item())\n            True\n        \"\"\"\n\n        return torch.ones_like(self)\n\n    @property\n    def contact(self) -&gt; Tensor:\n        r\"\"\"\n        Identical to `self`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n            &gt;&gt;&gt; pn_tensor = PNTensor([1, 2, 3])\n            &gt;&gt;&gt; bool((tensor == pn_tensor).all())\n            True\n            &gt;&gt;&gt; bool((tensor == pn_tensor.contact).all())\n            True\n        \"\"\"\n\n        return self\n\n    def new_empty(self, *args, **kwargs):\n        return PNTensor(super().new_empty(*args, **kwargs))\n</code></pre>"},{"location":"package/#danling.PNTensor.contact","title":"<code>contact: Tensor</code>  <code>property</code>","text":"<p>Identical to <code>self</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; pn_tensor = PNTensor([1, 2, 3])\n&gt;&gt;&gt; bool((tensor == pn_tensor).all())\nTrue\n&gt;&gt;&gt; bool((tensor == pn_tensor.contact).all())\nTrue\n</code></pre>"},{"location":"package/#danling.PNTensor.mask","title":"<code>mask: Tensor</code>  <code>property</code>","text":"<p>Identical to <code>torch.ones_like(self)</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; pn_tensor = PNTensor([1, 2, 3])\n&gt;&gt;&gt; bool((pn_tensor.mask == torch.ones_like(pn_tensor)).all().item())\nTrue\n</code></pre>"},{"location":"package/#danling.PNTensor.tensor","title":"<code>tensor: Tensor</code>  <code>property</code>","text":"<p>Identical to <code>self</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; pn_tensor = PNTensor([1, 2, 3])\n&gt;&gt;&gt; bool((tensor == pn_tensor).all())\nTrue\n&gt;&gt;&gt; bool((tensor == pn_tensor.tensor).all())\nTrue\n</code></pre>"},{"location":"package/#danling.TorchRunner","title":"<code>TorchRunner</code>","text":"<p>               Bases: <code>BaseRunner</code></p> <p>Set up everything for running a job.</p> <p><code>AccelerateRunner</code> uses <code>accelerate</code> as distributed backend to provide seamless distributed training experience.</p> <p><code>AccelerateRunner</code> will automatically <code>prepare</code> everything, including <code>model</code>, <code>criterion</code>, <code>optimizer</code>, <code>scheduler</code>, and <code>dataloaders</code> for distribute training, mixed precision, and deepspeed (optional).</p> <p>In fact, you don\u2019t even need to create <code>dataloaders</code>, just define <code>datasets</code> and <code>AccelerateRunner</code> will create <code>dataloaders</code> for you. <code>AccelerateRunner</code> will inspect the <code>train</code> flag in corresponding dataset to automatically set <code>shuffle</code>.</p> <p>Attributes:</p> Name Type Description <code>accelerator</code> <code>Accelerator</code> <code>accelerate</code> <code>dict</code> <p>Arguments to pass when building accelerator. Defaults to <code>{}</code>.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>class AccelerateRunner(BaseRunner):  # pylint: disable=too-many-public-methods\n    r\"\"\"\n    Set up everything for running a job.\n\n    `AccelerateRunner` uses [`accelerate`][accelerate] as distributed backend to\n    provide seamless distributed training experience.\n\n    `AccelerateRunner` will automatically [`prepare`][accelerate.Accelerator.prepare] everything,\n    including `model`, `criterion`, `optimizer`, `scheduler`, and `dataloaders` for distribute training,\n    mixed precision, and deepspeed (optional).\n\n    In fact, you don't even need to create `dataloaders`, just define\n    `datasets` and `AccelerateRunner` will create `dataloaders` for you.\n    `AccelerateRunner` will inspect the `train` flag in corresponding dataset to\n    automatically set `shuffle`.\n\n    Attributes:\n        accelerator (Accelerator):\n        accelerate: Arguments to pass when building accelerator. Defaults to `{}`.\n    \"\"\"\n\n    accelerator: Accelerator\n    accelerate: dict\n\n    model: nn.Module\n    criterion: nn.Module\n    optimizer: optim.Optimizer\n    scheduler: optim.lr_scheduler._LRScheduler\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        if len(args) != 1 or kwargs:\n            message = (\n                \"Passing multiple args &amp; kwargs to build Runner is deprecated and will be removed in DanLing v0.3.\\n\"\n                \"Please only pass a config dict instead.\"\n            )\n            warn(message, DeprecationWarning, stacklevel=2)\n            config = NestedDict(*args, **kwargs)\n        else:\n            config = args[0]\n        if \"accelerate\" not in self:  # class attributes\n            self.accelerate = {}\n        self.accelerate.update(config.get(\"accelerate\", {}))\n        super().__init__(config)\n\n    def __post_init__(self) -&gt; None:\n        self.model, self.criterion, self.optimizer = self.prepare(self.model, self.criterion, self.optimizer)\n        self.scheduler = self.prepare(self.scheduler)\n        if self.datasets:\n            datasets = {k: d for k, d in self.datasets.items() if k not in self.dataloaders}\n            default_kwargs = self.state.setdefault(\"dataloader\", NestedDict())\n            dataloader_kwargs = NestedDict({k: default_kwargs.pop(k) for k in self.datasets if k in default_kwargs})\n            for k, d in datasets.items():\n                dataloader_kwargs.setdefault(k, NestedDict())\n                dataloader_kwargs[k].merge(default_kwargs, overwrite=False)\n                dataloader_kwargs[k].setdefault(\"shuffle\", getattr(d, \"train\", True))\n                dataloader_kwargs[k].setdefault(\"drop_last\", not getattr(d, \"train\", True))\n                self.dataloaders[k] = utils.data.DataLoader(d, **dataloader_kwargs[k])\n            default_kwargs.update(dataloader_kwargs)\n        for k, d in self.dataloaders.items():\n            self.dataloaders[k] = self.prepare(d)\n        if self.state.get(\"log_interval\") is None:\n            self.state.log_interval = max(len(d) for d in self.dataloaders.values()) // 10\n\n    @property\n    def deepspeed(self) -&gt; dict | None:\n        if \"accelerator\" not in self:\n            raise ValueError(\"accelerator is not used\")\n        if self.accelerator.state.deepspeed_plugin is not None:\n            return self.accelerator.state.deepspeed_plugin.deepspeed_config\n        return None\n\n    def train(self, train_splits: list[str] | None = None, eval_splits: list[str] | None = None) -&gt; NestedDict:\n        r\"\"\"\n        Perform training on `split`.\n\n        Args:\n            train_splits (list[str]): list of split to run train.\n                Defaults to `[\"train\"]`.\n            eval_splits (list[str]): list of split to run evaluate.\n                Defaults to `self.dataloaders` except for those in `train_splits`.\n\n        Return:\n            NestedDict: train results\n        \"\"\"\n\n        early_stop_counter = 0\n        if train_splits is None:\n            train_splits = [\"train\"]\n        if eval_splits is None:\n            eval_splits = [s for s in self.dataloaders if s not in train_splits]\n        self.state.epoch_begin = self.state.epochs\n        print(f\"Begin training from {self.state.epoch_begin} to {self.state.epoch_end}\")\n        print(f\"Training splits: {train_splits}\")\n        print(f\"Evaluation splits: {eval_splits}\")\n        patience = self.state.get(\"patience\", float(\"inf\"))\n        for epochs in range(self.state.epoch_begin, self.state.epoch_end):  # type: ignore\n            self.state.epochs = epochs\n            result = NestedDict()\n            result.setattr(\"convert_mapping\", True)\n            for split in train_splits:\n                result[split] = self.train_epoch(split)\n            for split in eval_splits:\n                result[split] = self.evaluate_epoch(split)\n            self.append_result(result)\n            print(self.format_epoch_result(result))\n            self.save_result()\n            if self.state.save_interval is not None:\n                self.save_checkpoint(epochs)\n            \"\"\"@nni.report_intermediate_result(self.latest_score)\"\"\"\n            early_stop_counter = 0 if self.is_best else early_stop_counter + 1\n            if early_stop_counter &gt; patience:\n                print(\"early stop\")\n                break\n        \"\"\"@nni.report_final_result(self.latest_score)\"\"\"\n        return self.results\n\n    def train_epoch(self, split: str = \"train\") -&gt; NestedDict:\n        r\"\"\"\n        Train one epoch on `split`.\n\n        Args:\n            split (str): split to run train\n\n        Return:\n            NestedDict: train result\n        \"\"\"\n\n        self.mode = \"train\"  # type: ignore\n        self.split = split\n        loader = self.dataloaders[split]\n        length = len(loader) - 1\n        last_print_iteration = -1\n        log_interval = self.state.get(\"log_interval\", -1)\n        self.meters.reset()\n        if self.metrics is not None:\n            self.metrics.reset()\n        batch_time = time()\n        if hasattr(loader.batch_sampler, \"set_epoch\"):\n            loader.batch_sampler.set_epoch(self.epochs)\n        if hasattr(loader.sampler, \"set_epoch\"):\n            loader.sampler.set_epoch(self.epochs)\n\n        for iteration, data in enumerate(loader):\n            with self.autocast(), self.accumulate():\n                input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n                target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n                pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n                loss = self.criterion(pred, target)\n                if self.metrics is not None:\n                    self.metrics.update(pred.squeeze(-1), target)\n                self.step(loss)\n\n            if log_interval &gt; 0 and (iteration &gt; 0 and iteration % log_interval == 0 or iteration == length):\n                interval = iteration - last_print_iteration\n                if self.device == torch.device(\"cuda\"):\n                    torch.cuda.synchronize()\n                self.meters.time.update((time() - batch_time) / interval)\n                batch_time = time()\n                reduced_loss = self.reduce(loss).item()\n                self.meters.loss.update(reduced_loss)\n                self.step_log(split, iteration, length)\n                last_print_iteration = iteration\n\n        result = self.meters.average()\n        if self.metrics is not None:\n            result.merge(self.metrics.average())\n        return result\n\n    def evaluate(self, eval_splits: list[str] | None = None) -&gt; NestedDict:\n        r\"\"\"\n        Perform evaluation on `eval_splits`.\n\n        Args:\n            eval_splits (list[str]): list of split to run evaluate.\n                Defaults to `[\"eval\"]`.\n\n        Return:\n            NestedDict: evaluation result\n        \"\"\"\n\n        if eval_splits is None:\n            eval_splits = [\"eval\"]\n\n        print(\"Begin evaluation\")\n        print(f\"Evaluation splits: {eval_splits}\")\n        result = NestedDict()\n        result.setattr(\"convert_mapping\", True)\n        for split in eval_splits:\n            result[split] = self.evaluate_epoch(split=split)\n        print(self.format_epoch_result(result))\n        return result\n\n    @torch.inference_mode()\n    def evaluate_epoch(self, split: str = \"val\") -&gt; NestedDict:\n        r\"\"\"\n        Evaluate one epoch on `split`.\n\n        Args:\n            split (str): split to run evaluate\n\n        Return:\n            NestedDict: evaluation result\n        \"\"\"\n\n        self.mode = \"eval\"  # type: ignore\n        self.split = split\n        loader = self.dataloaders[split]\n        length = len(loader) - 1\n        last_print_iteration = -1\n        log_interval = self.state.get(\"log_interval\", -1)\n        self.meters.reset()\n        if self.metrics is not None:\n            self.metrics.reset()\n        batch_time = time()\n\n        for iteration, data in enumerate(loader):\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            loss = self.criterion(pred, target)\n            if self.metrics is not None:\n                self.metrics.update(pred.squeeze(-1), target)\n\n            if log_interval &gt; 0 and (iteration &gt; 0 and iteration % log_interval == 0 or iteration == length):\n                interval = iteration - last_print_iteration\n                if self.device == torch.device(\"cuda\"):\n                    torch.cuda.synchronize()\n                self.meters.time.update((time() - batch_time) / interval)\n                batch_time = time()\n                reduced_loss = self.reduce(loss).item()\n                self.meters.loss.update(reduced_loss)\n                self.step_log(split, iteration, length)\n                last_print_iteration = iteration\n\n        result = self.meters.average()\n        if self.metrics is not None:\n            result.merge(self.metrics.average())\n        self.write_result(result, split, self.state.epochs)\n        return result\n\n    @torch.inference_mode()\n    def inference(self, split: str = \"inf\") -&gt; list:\n        r\"\"\"\n        Perform inference on `split`.\n\n        Args:\n            split (str): split to run inference\n\n        Return:\n            Tensor: inference outputs\n        \"\"\"\n\n        # pylint: disable=E1102, W0622\n        self.mode = \"inf\"  # type: ignore\n        loader = self.dataloaders[split]\n        self.meters.reset()\n        output = []\n        for _, data in tqdm(enumerate(loader), total=len(loader)):\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            output.extend(pred.squeeze(-1).tolist())\n\n        if self.distributed:\n            torch.cuda.synchronize()\n            output = self.gather_for_metrics(output)\n        return output\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Set up distributed training.\n\n        Initialise process group and set up DDP variables.\n        \"\"\"\n\n        if os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\").lower() == \"true\":\n            deepspeed_config = self.state.get(\"deepspeed\", os.environ.get(\"ACCELERATE_DEEPSPEED_CONFIG_FILE\"))\n            self.accelerate[\"deepspeed_plugin\"] = DeepSpeedPlugin(hf_ds_config=self.init_deepspeed(deepspeed_config))\n        self.accelerator = Accelerator(**self.accelerate)\n        if self.distributed:\n            object_list = [self.id, self.timestamp]\n            dist.broadcast_object_list(object_list)\n            self.id, self.timestamp = object_list\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n        if \"log_dir\" not in kwargs:\n            kwargs[\"log_dir\"] = self.dir\n\n        self.writer = SummaryWriter(*args, **kwargs)\n        self.writer.add_scalar = catch(OSError, verbose=False)(self.writer.add_scalar)\n\n    def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Args:\n            seed: Random seed to set.\n                Defaults to `self.state.seed` (`config.seed`).\n\n            bias: Make the seed different for each processes.\n                This is used to ensure the data augmentation are applied differently on every processes.\n                Defaults to `self.rank`.\n                Set to `False` to disable this feature.\n        \"\"\"\n\n        seed = seed or self.state.seed\n        if self.distributed:\n            object_list = [seed]\n            dist.broadcast_object_list(object_list)\n            seed = object_list[0]\n        bias = bias or self.rank\n        if bias:\n            seed += bias\n        self.state.seed = seed\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        if np_random is not None:\n            np_random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n        if torch.__version__ &gt;= \"1.8.0\":\n            torch.use_deterministic_algorithms(True)\n\n    def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n        r\"\"\"\n        Backward loss and step optimizer &amp; scheduler.\n\n        This method increment `self.state.steps`.\n\n        This method also increment `self.state.iters` when `batch_size` is specified.\n\n        Args:\n            zero_grad: Whether to zero the gradients.\n        \"\"\"\n\n        self.accelerator.backward(loss)\n        if self.sync_gradients:\n            if self.state.get(\"max_grad_value\") is not None:\n                self.clip_grad_value_(self.model.parameters(), self.state.get(\"max_grad_value\"))\n            if self.state.get(\"max_grad_norm\") is not None:\n                self.clip_grad_norm_(self.model.parameters(), self.state.get(\"max_grad_norm\"))\n        if self.optimizer is not None:\n            self.optimizer.step()\n            if zero_grad:\n                self.optimizer.zero_grad()\n        if self.scheduler is not None:\n            self.scheduler.step()\n        self.state.steps += 1\n        if batch_size is None:\n            batch_size = self.batch_size_equivalent\n        self.state.iters += batch_size\n        # TODO: Support `drop_last = False`\n        # self.state.iters += self.batch_size_equivalent\n\n    def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Return dict of all attributes for checkpoint.\n        \"\"\"\n\n        if self.model is None:\n            raise ValueError(\"Model must be defined when calling state_dict\")\n        model = self.accelerator.unwrap_model(self.model)\n        return cls(\n            runner=self.state.dict(),\n            model=model.state_dict(),\n            optimizer=self.optimizer.state_dict() if self.optimizer else None,\n            scheduler=self.scheduler.state_dict() if self.scheduler else None,\n        )\n\n    def prepare(self, *args: list[Any], device_placement: list[bool] | None = None) -&gt; list[Any]:\n        r\"\"\"\n        Prepare all objects passed in `args` for distributed training and mixed precision,\n        then return them in the same order.\n        \"\"\"\n\n        return self.accelerator.prepare(*args, device_placement=device_placement)\n\n    def accumulate(self, model: nn.Module | None = None):\n        r\"\"\"\n        Context manager that enables gradient accumulate.\n        \"\"\"\n\n        model = model or self.model\n        return self.accelerator.accumulate(model)\n\n    def autocast(self):\n        r\"\"\"\n        Context manager that enables auto-casting for the forward pass (and maybe backward pass).\n        \"\"\"\n\n        return self.accelerator.autocast()\n\n    def backward(self, loss) -&gt; None:\n        r\"\"\"\n        Backward loss to compute gradients.\n        \"\"\"\n\n        return self.accelerator.backward(loss)\n\n    def unwrap_model(self, model: nn.Module | None = None) -&gt; nn.Module:\n        r\"\"\"\n        Unwrap DDP model.\n\n        Args:\n            model (Optional[nn.Module]):\n                Defaults to `self.model`.\n        \"\"\"\n\n        if model is not None:\n            model = self.model\n        if self.accelerator is not None:\n            return self.accelerator.unwrap_model(model)\n        if self.distributed:\n            return model.module\n        return model\n\n    @property\n    def mode(self) -&gt; RunnerMode:\n        return self._mode\n\n    @mode.setter\n    def mode(self, mode: str | RunnerMode) -&gt; None:\n        if isinstance(mode, str):\n            mode = RunnerMode(mode)\n        self._mode = mode\n        if self.model is not None:\n            self.model.train(mode == RunnerMode.train)\n\n    @property\n    def batch_size(self) -&gt; int:\n        r\"\"\"\n        Batch size.\n\n        Notes:\n            If `train` is in `dataloaders`, then `batch_size` is the batch size of `train`.\n            Otherwise, `batch_size` is the batch size of the first dataloader.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        batch_size = self.state.get(\"dataloader.batch_size\")\n        if batch_size:\n            return batch_size\n        if self.dataloaders:\n            loader = self.dataloaders.get(\"train\", next(iter(self.dataloaders.values())))\n            if loader.batch_size:\n                return loader.batch_size\n            batch_sampler = loader.batch_sampler if loader.batch_sampler is not None else loader.sampler\n            return batch_sampler.batch_size\n        raise AttributeError(\"batch_size could not be inferred, since no dataloader found.\")\n\n    @property\n    def accum_steps(self) -&gt; int:\n        r\"\"\"\n        Gradient accumulation steps.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        return self.accelerator.gradient_accumulation_steps\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of runner.\n        \"\"\"\n\n        return self.accelerator.device\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n        \"\"\"\n\n        return self.accelerator.num_processes\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n        \"\"\"\n\n        return self.accelerator.process_index\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n        \"\"\"\n\n        return self.accelerator.local_process_index\n\n    def gather(self, tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Gather tensor.\n        \"\"\"\n\n        return self.accelerator.gather(tensor)\n\n    def reduce(self, tensor, reduction: str = \"sum\") -&gt; torch.Tensor:\n        r\"\"\"\n        Reduce tensor.\n        \"\"\"\n\n        return self.accelerator.reduce(tensor, reduction=reduction)\n\n    def __getattr__(self, name: str) -&gt; Any:\n        with suppress(AttributeError):\n            return super().__getattr__(name)\n        if \"accelerator\" in self.__dict__ and hasattr(self.accelerator, name):\n            return getattr(self.accelerator, name)\n        raise super().__getattribute__(name)\n</code></pre>"},{"location":"package/#danling.TorchRunner.accum_steps","title":"<code>accum_steps: int</code>  <code>property</code>","text":"<p>Gradient accumulation steps.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"package/#danling.TorchRunner.batch_size","title":"<code>batch_size: int</code>  <code>property</code>","text":"<p>Batch size.</p> Notes <p>If <code>train</code> is in <code>dataloaders</code>, then <code>batch_size</code> is the batch size of <code>train</code>. Otherwise, <code>batch_size</code> is the batch size of the first dataloader.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"package/#danling.TorchRunner.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Device of runner.</p>"},{"location":"package/#danling.TorchRunner.local_rank","title":"<code>local_rank: int</code>  <code>property</code>","text":"<p>Process index in local processes.</p>"},{"location":"package/#danling.TorchRunner.rank","title":"<code>rank: int</code>  <code>property</code>","text":"<p>Process index in all processes.</p>"},{"location":"package/#danling.TorchRunner.world_size","title":"<code>world_size: int</code>  <code>property</code>","text":"<p>Number of Processes.</p>"},{"location":"package/#danling.TorchRunner.accumulate","title":"<code>accumulate(model=None)</code>","text":"<p>Context manager that enables gradient accumulate.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def accumulate(self, model: nn.Module | None = None):\n    r\"\"\"\n    Context manager that enables gradient accumulate.\n    \"\"\"\n\n    model = model or self.model\n    return self.accelerator.accumulate(model)\n</code></pre>"},{"location":"package/#danling.TorchRunner.autocast","title":"<code>autocast()</code>","text":"<p>Context manager that enables auto-casting for the forward pass (and maybe backward pass).</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def autocast(self):\n    r\"\"\"\n    Context manager that enables auto-casting for the forward pass (and maybe backward pass).\n    \"\"\"\n\n    return self.accelerator.autocast()\n</code></pre>"},{"location":"package/#danling.TorchRunner.backward","title":"<code>backward(loss)</code>","text":"<p>Backward loss to compute gradients.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def backward(self, loss) -&gt; None:\n    r\"\"\"\n    Backward loss to compute gradients.\n    \"\"\"\n\n    return self.accelerator.backward(loss)\n</code></pre>"},{"location":"package/#danling.TorchRunner.evaluate","title":"<code>evaluate(eval_splits=None)</code>","text":"<p>Perform evaluation on <code>eval_splits</code>.</p> <p>Parameters:</p> Name Type Description Default <code>eval_splits</code> <code>list[str]</code> <p>list of split to run evaluate. Defaults to <code>[\"eval\"]</code>.</p> <code>None</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def evaluate(self, eval_splits: list[str] | None = None) -&gt; NestedDict:\n    r\"\"\"\n    Perform evaluation on `eval_splits`.\n\n    Args:\n        eval_splits (list[str]): list of split to run evaluate.\n            Defaults to `[\"eval\"]`.\n\n    Return:\n        NestedDict: evaluation result\n    \"\"\"\n\n    if eval_splits is None:\n        eval_splits = [\"eval\"]\n\n    print(\"Begin evaluation\")\n    print(f\"Evaluation splits: {eval_splits}\")\n    result = NestedDict()\n    result.setattr(\"convert_mapping\", True)\n    for split in eval_splits:\n        result[split] = self.evaluate_epoch(split=split)\n    print(self.format_epoch_result(result))\n    return result\n</code></pre>"},{"location":"package/#danling.TorchRunner.evaluate_epoch","title":"<code>evaluate_epoch(split='val')</code>","text":"<p>Evaluate one epoch on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run evaluate</p> <code>'val'</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>@torch.inference_mode()\ndef evaluate_epoch(self, split: str = \"val\") -&gt; NestedDict:\n    r\"\"\"\n    Evaluate one epoch on `split`.\n\n    Args:\n        split (str): split to run evaluate\n\n    Return:\n        NestedDict: evaluation result\n    \"\"\"\n\n    self.mode = \"eval\"  # type: ignore\n    self.split = split\n    loader = self.dataloaders[split]\n    length = len(loader) - 1\n    last_print_iteration = -1\n    log_interval = self.state.get(\"log_interval\", -1)\n    self.meters.reset()\n    if self.metrics is not None:\n        self.metrics.reset()\n    batch_time = time()\n\n    for iteration, data in enumerate(loader):\n        input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n        target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n        pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n        loss = self.criterion(pred, target)\n        if self.metrics is not None:\n            self.metrics.update(pred.squeeze(-1), target)\n\n        if log_interval &gt; 0 and (iteration &gt; 0 and iteration % log_interval == 0 or iteration == length):\n            interval = iteration - last_print_iteration\n            if self.device == torch.device(\"cuda\"):\n                torch.cuda.synchronize()\n            self.meters.time.update((time() - batch_time) / interval)\n            batch_time = time()\n            reduced_loss = self.reduce(loss).item()\n            self.meters.loss.update(reduced_loss)\n            self.step_log(split, iteration, length)\n            last_print_iteration = iteration\n\n    result = self.meters.average()\n    if self.metrics is not None:\n        result.merge(self.metrics.average())\n    self.write_result(result, split, self.state.epochs)\n    return result\n</code></pre>"},{"location":"package/#danling.TorchRunner.gather","title":"<code>gather(tensor)</code>","text":"<p>Gather tensor.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def gather(self, tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Gather tensor.\n    \"\"\"\n\n    return self.accelerator.gather(tensor)\n</code></pre>"},{"location":"package/#danling.TorchRunner.inference","title":"<code>inference(split='inf')</code>","text":"<p>Perform inference on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run inference</p> <code>'inf'</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>@torch.inference_mode()\ndef inference(self, split: str = \"inf\") -&gt; list:\n    r\"\"\"\n    Perform inference on `split`.\n\n    Args:\n        split (str): split to run inference\n\n    Return:\n        Tensor: inference outputs\n    \"\"\"\n\n    # pylint: disable=E1102, W0622\n    self.mode = \"inf\"  # type: ignore\n    loader = self.dataloaders[split]\n    self.meters.reset()\n    output = []\n    for _, data in tqdm(enumerate(loader), total=len(loader)):\n        input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n        pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n        output.extend(pred.squeeze(-1).tolist())\n\n    if self.distributed:\n        torch.cuda.synchronize()\n        output = self.gather_for_metrics(output)\n    return output\n</code></pre>"},{"location":"package/#danling.TorchRunner.init_distributed","title":"<code>init_distributed()</code>","text":"<p>Set up distributed training.</p> <p>Initialise process group and set up DDP variables.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def init_distributed(self) -&gt; None:\n    r\"\"\"\n    Set up distributed training.\n\n    Initialise process group and set up DDP variables.\n    \"\"\"\n\n    if os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\").lower() == \"true\":\n        deepspeed_config = self.state.get(\"deepspeed\", os.environ.get(\"ACCELERATE_DEEPSPEED_CONFIG_FILE\"))\n        self.accelerate[\"deepspeed_plugin\"] = DeepSpeedPlugin(hf_ds_config=self.init_deepspeed(deepspeed_config))\n    self.accelerator = Accelerator(**self.accelerate)\n    if self.distributed:\n        object_list = [self.id, self.timestamp]\n        dist.broadcast_object_list(object_list)\n        self.id, self.timestamp = object_list\n</code></pre>"},{"location":"package/#danling.TorchRunner.init_tensorboard","title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n    self.writer.add_scalar = catch(OSError, verbose=False)(self.writer.add_scalar)\n</code></pre>"},{"location":"package/#danling.TorchRunner.prepare","title":"<code>prepare(*args, device_placement=None)</code>","text":"<p>Prepare all objects passed in <code>args</code> for distributed training and mixed precision, then return them in the same order.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def prepare(self, *args: list[Any], device_placement: list[bool] | None = None) -&gt; list[Any]:\n    r\"\"\"\n    Prepare all objects passed in `args` for distributed training and mixed precision,\n    then return them in the same order.\n    \"\"\"\n\n    return self.accelerator.prepare(*args, device_placement=device_placement)\n</code></pre>"},{"location":"package/#danling.TorchRunner.reduce","title":"<code>reduce(tensor, reduction='sum')</code>","text":"<p>Reduce tensor.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def reduce(self, tensor, reduction: str = \"sum\") -&gt; torch.Tensor:\n    r\"\"\"\n    Reduce tensor.\n    \"\"\"\n\n    return self.accelerator.reduce(tensor, reduction=reduction)\n</code></pre>"},{"location":"package/#danling.TorchRunner.set_deterministic","title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    if torch.__version__ &gt;= \"1.8.0\":\n        torch.use_deterministic_algorithms(True)\n</code></pre>"},{"location":"package/#danling.TorchRunner.set_seed","title":"<code>set_seed(seed=None, bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Random seed to set. Defaults to <code>self.state.seed</code> (<code>config.seed</code>).</p> <code>None</code> <code>bias</code> <code>int | None</code> <p>Make the seed different for each processes. This is used to ensure the data augmentation are applied differently on every processes. Defaults to <code>self.rank</code>. Set to <code>False</code> to disable this feature.</p> <code>None</code> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Args:\n        seed: Random seed to set.\n            Defaults to `self.state.seed` (`config.seed`).\n\n        bias: Make the seed different for each processes.\n            This is used to ensure the data augmentation are applied differently on every processes.\n            Defaults to `self.rank`.\n            Set to `False` to disable this feature.\n    \"\"\"\n\n    seed = seed or self.state.seed\n    if self.distributed:\n        object_list = [seed]\n        dist.broadcast_object_list(object_list)\n        seed = object_list[0]\n    bias = bias or self.rank\n    if bias:\n        seed += bias\n    self.state.seed = seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    if np_random is not None:\n        np_random.seed(seed)\n    random.seed(seed)\n</code></pre>"},{"location":"package/#danling.TorchRunner.state_dict","title":"<code>state_dict(cls=dict)</code>","text":"<p>Return dict of all attributes for checkpoint.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Return dict of all attributes for checkpoint.\n    \"\"\"\n\n    if self.model is None:\n        raise ValueError(\"Model must be defined when calling state_dict\")\n    model = self.accelerator.unwrap_model(self.model)\n    return cls(\n        runner=self.state.dict(),\n        model=model.state_dict(),\n        optimizer=self.optimizer.state_dict() if self.optimizer else None,\n        scheduler=self.scheduler.state_dict() if self.scheduler else None,\n    )\n</code></pre>"},{"location":"package/#danling.TorchRunner.step","title":"<code>step(loss, batch_size=None, zero_grad=True)</code>","text":"<p>Backward loss and step optimizer &amp; scheduler.</p> <p>This method increment <code>self.state.steps</code>.</p> <p>This method also increment <code>self.state.iters</code> when <code>batch_size</code> is specified.</p> <p>Parameters:</p> Name Type Description Default <code>zero_grad</code> <code>bool</code> <p>Whether to zero the gradients.</p> <code>True</code> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n    r\"\"\"\n    Backward loss and step optimizer &amp; scheduler.\n\n    This method increment `self.state.steps`.\n\n    This method also increment `self.state.iters` when `batch_size` is specified.\n\n    Args:\n        zero_grad: Whether to zero the gradients.\n    \"\"\"\n\n    self.accelerator.backward(loss)\n    if self.sync_gradients:\n        if self.state.get(\"max_grad_value\") is not None:\n            self.clip_grad_value_(self.model.parameters(), self.state.get(\"max_grad_value\"))\n        if self.state.get(\"max_grad_norm\") is not None:\n            self.clip_grad_norm_(self.model.parameters(), self.state.get(\"max_grad_norm\"))\n    if self.optimizer is not None:\n        self.optimizer.step()\n        if zero_grad:\n            self.optimizer.zero_grad()\n    if self.scheduler is not None:\n        self.scheduler.step()\n    self.state.steps += 1\n    if batch_size is None:\n        batch_size = self.batch_size_equivalent\n    self.state.iters += batch_size\n</code></pre>"},{"location":"package/#danling.TorchRunner.train","title":"<code>train(train_splits=None, eval_splits=None)</code>","text":"<p>Perform training on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>train_splits</code> <code>list[str]</code> <p>list of split to run train. Defaults to <code>[\"train\"]</code>.</p> <code>None</code> <code>eval_splits</code> <code>list[str]</code> <p>list of split to run evaluate. Defaults to <code>self.dataloaders</code> except for those in <code>train_splits</code>.</p> <code>None</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def train(self, train_splits: list[str] | None = None, eval_splits: list[str] | None = None) -&gt; NestedDict:\n    r\"\"\"\n    Perform training on `split`.\n\n    Args:\n        train_splits (list[str]): list of split to run train.\n            Defaults to `[\"train\"]`.\n        eval_splits (list[str]): list of split to run evaluate.\n            Defaults to `self.dataloaders` except for those in `train_splits`.\n\n    Return:\n        NestedDict: train results\n    \"\"\"\n\n    early_stop_counter = 0\n    if train_splits is None:\n        train_splits = [\"train\"]\n    if eval_splits is None:\n        eval_splits = [s for s in self.dataloaders if s not in train_splits]\n    self.state.epoch_begin = self.state.epochs\n    print(f\"Begin training from {self.state.epoch_begin} to {self.state.epoch_end}\")\n    print(f\"Training splits: {train_splits}\")\n    print(f\"Evaluation splits: {eval_splits}\")\n    patience = self.state.get(\"patience\", float(\"inf\"))\n    for epochs in range(self.state.epoch_begin, self.state.epoch_end):  # type: ignore\n        self.state.epochs = epochs\n        result = NestedDict()\n        result.setattr(\"convert_mapping\", True)\n        for split in train_splits:\n            result[split] = self.train_epoch(split)\n        for split in eval_splits:\n            result[split] = self.evaluate_epoch(split)\n        self.append_result(result)\n        print(self.format_epoch_result(result))\n        self.save_result()\n        if self.state.save_interval is not None:\n            self.save_checkpoint(epochs)\n        \"\"\"@nni.report_intermediate_result(self.latest_score)\"\"\"\n        early_stop_counter = 0 if self.is_best else early_stop_counter + 1\n        if early_stop_counter &gt; patience:\n            print(\"early stop\")\n            break\n    \"\"\"@nni.report_final_result(self.latest_score)\"\"\"\n    return self.results\n</code></pre>"},{"location":"package/#danling.TorchRunner.train_epoch","title":"<code>train_epoch(split='train')</code>","text":"<p>Train one epoch on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run train</p> <code>'train'</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def train_epoch(self, split: str = \"train\") -&gt; NestedDict:\n    r\"\"\"\n    Train one epoch on `split`.\n\n    Args:\n        split (str): split to run train\n\n    Return:\n        NestedDict: train result\n    \"\"\"\n\n    self.mode = \"train\"  # type: ignore\n    self.split = split\n    loader = self.dataloaders[split]\n    length = len(loader) - 1\n    last_print_iteration = -1\n    log_interval = self.state.get(\"log_interval\", -1)\n    self.meters.reset()\n    if self.metrics is not None:\n        self.metrics.reset()\n    batch_time = time()\n    if hasattr(loader.batch_sampler, \"set_epoch\"):\n        loader.batch_sampler.set_epoch(self.epochs)\n    if hasattr(loader.sampler, \"set_epoch\"):\n        loader.sampler.set_epoch(self.epochs)\n\n    for iteration, data in enumerate(loader):\n        with self.autocast(), self.accumulate():\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            loss = self.criterion(pred, target)\n            if self.metrics is not None:\n                self.metrics.update(pred.squeeze(-1), target)\n            self.step(loss)\n\n        if log_interval &gt; 0 and (iteration &gt; 0 and iteration % log_interval == 0 or iteration == length):\n            interval = iteration - last_print_iteration\n            if self.device == torch.device(\"cuda\"):\n                torch.cuda.synchronize()\n            self.meters.time.update((time() - batch_time) / interval)\n            batch_time = time()\n            reduced_loss = self.reduce(loss).item()\n            self.meters.loss.update(reduced_loss)\n            self.step_log(split, iteration, length)\n            last_print_iteration = iteration\n\n    result = self.meters.average()\n    if self.metrics is not None:\n        result.merge(self.metrics.average())\n    return result\n</code></pre>"},{"location":"package/#danling.TorchRunner.unwrap_model","title":"<code>unwrap_model(model=None)</code>","text":"<p>Unwrap DDP model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[Module]</code> <p>Defaults to <code>self.model</code>.</p> <code>None</code> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def unwrap_model(self, model: nn.Module | None = None) -&gt; nn.Module:\n    r\"\"\"\n    Unwrap DDP model.\n\n    Args:\n        model (Optional[nn.Module]):\n            Defaults to `self.model`.\n    \"\"\"\n\n    if model is not None:\n        model = self.model\n    if self.accelerator is not None:\n        return self.accelerator.unwrap_model(model)\n    if self.distributed:\n        return model.module\n    return model\n</code></pre>"},{"location":"package/#danling.catch","title":"<code>catch(error=Exception, exclude=None, callback=print_exc, *callback_args, **callback_kwargs)</code>","text":"<p>Decorator to catch <code>error</code> except for <code>exclude</code>. Detailed traceback will be printed to <code>stderr</code>.</p> <p><code>catch</code> is extremely useful for unfatal errors. For example, <code>Runner</code> saves checkpoint regularly, however, this might break running if the space is full. Decorating <code>save</code> method with <code>catch</code> will allow you to catch these errors and continue your running.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exceptions</code> <p>Exceptions to be caught.</p> <code>Exception</code> <code>exclude</code> <code>Exceptions | None</code> <p>Exceptions to be excluded.</p> <code>None</code> <code>callback</code> <code>Callable</code> <p>Callback to be called when an error occurs. The first four arguments to <code>callback</code> are <code>exc</code>, <code>func</code>, <code>args</code>, <code>kwargs</code>. Additional arguments should be passed with <code>*callback_args</code> and <code>**callback_kwargs</code>.</p> <code>print_exc</code> <code>callback_args</code> <p>Arguments to be passed to <code>callback</code>.</p> <code>()</code> <code>callback_kwargs</code> <p>Keyword arguments to be passed to <code>callback</code>.</p> <code>{}</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; def file_not_found(*args, **kwargs):\n...     raise FileNotFoundError\n&gt;&gt;&gt; func = file_not_found\n&gt;&gt;&gt; func()\nTraceback (most recent call last):\nFileNotFoundError\n&gt;&gt;&gt; func = catch(OSError)(file_not_found)\n&gt;&gt;&gt; func()\n&gt;&gt;&gt; func = catch(IOError)(file_not_found)\n&gt;&gt;&gt; func()\n&gt;&gt;&gt; func = catch(ZeroDivisionError)(file_not_found)\n&gt;&gt;&gt; func()\nTraceback (most recent call last):\nFileNotFoundError\n</code></pre> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>@flexible_decorator\ndef catch(  # pylint: disable=keyword-arg-before-vararg\n    error: Exceptions = Exception,\n    exclude: Exceptions | None = None,\n    callback: Callable = print_exc,\n    *callback_args,\n    **callback_kwargs,\n):\n    r\"\"\"\n    Decorator to catch `error` except for `exclude`.\n    Detailed traceback will be printed to `stderr`.\n\n    `catch` is extremely useful for unfatal errors.\n    For example, `Runner` saves checkpoint regularly, however, this might break running if the space is full.\n    Decorating `save` method with `catch` will allow you to catch these errors and continue your running.\n\n    Args:\n        error: Exceptions to be caught.\n        exclude: Exceptions to be excluded.\n        callback: Callback to be called when an error occurs.\n            The first four arguments to `callback` are `exc`, `func`, `args`, `kwargs`.\n            Additional arguments should be passed with `*callback_args` and `**callback_kwargs`.\n        callback_args: Arguments to be passed to `callback`.\n        callback_kwargs: Keyword arguments to be passed to `callback`.\n\n    Examples:\n        &gt;&gt;&gt; def file_not_found(*args, **kwargs):\n        ...     raise FileNotFoundError\n        &gt;&gt;&gt; func = file_not_found\n        &gt;&gt;&gt; func()\n        Traceback (most recent call last):\n        FileNotFoundError\n        &gt;&gt;&gt; func = catch(OSError)(file_not_found)\n        &gt;&gt;&gt; func()\n        &gt;&gt;&gt; func = catch(IOError)(file_not_found)\n        &gt;&gt;&gt; func()\n        &gt;&gt;&gt; func = catch(ZeroDivisionError)(file_not_found)\n        &gt;&gt;&gt; func()\n        Traceback (most recent call last):\n        FileNotFoundError\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):  # pylint: disable=inconsistent-return-statements\n            try:\n                return func(*args, **kwargs)\n            except error as exc:  # pylint: disable=broad-exception-caught\n                if exclude is not None and isinstance(exc, exclude):\n                    raise exc\n                callback(exc, func, args, kwargs, *callback_args, **callback_kwargs)\n\n        return wrapper\n\n    decorator.__doc__ = catch.__doc__\n\n    return decorator\n</code></pre>"},{"location":"package/#danling.debug","title":"<code>debug(enable=True, error=Exception, exclude=None)</code>","text":"<p>Contextmanager to enter debug mode on <code>error</code> except for <code>exclude</code>.</p> <p><code>debug</code> is intended to be used to catch the error and enter debug mode. Since it is mainly for development purposed, we include an <code>enable</code> args so that it can be deactivated.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Whether to enable the contextmanager. Defaults to <code>True</code>.</p> <code>True</code> <code>error</code> <code>Exceptions</code> <p>The error to catch. Defaults to <code>Exception</code>.</p> <code>Exception</code> <code>exclude</code> <code>Optional[Exceptions]</code> <p>The error to exclude. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>danling/utils/contextmanagers.py</code> Python<pre><code>@contextmanager\ndef debug(\n    enable: bool = True,\n    error: Exceptions = Exception,\n    exclude: Optional[Exceptions] = None,\n):\n    \"\"\"\n    Contextmanager to enter debug mode on `error` except for `exclude`.\n\n    `debug` is intended to be used to catch the error and enter debug mode.\n    Since it is mainly for development purposed, we include an `enable` args so that it can be deactivated.\n\n    Args:\n        enable: Whether to enable the contextmanager.\n            Defaults to `True`.\n        error: The error to catch.\n            Defaults to `Exception`.\n        exclude: The error to exclude.\n            Defaults to `None`.\n    \"\"\"\n\n    if not enable:\n        yield\n        return\n    try:\n        yield\n    except error as exc:  # pylint: disable=broad-exception-caught\n        if exclude is not None and isinstance(exc, exclude):\n            raise exc\n        _, m, tb = sys.exc_info()\n        print(repr(m), file=sys.stderr)\n        pdb.post_mortem(tb)\n    finally:\n        pass\n</code></pre>"},{"location":"package/#danling.ensure_dir","title":"<code>ensure_dir(func)</code>","text":"<p>Decorator to ensure a directory property exists.</p> Note <p>Please avoid using this with <code>cached_property</code>.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; @property\n... @ensure_dir\n... def dir(self) -&gt; str:\n...     return os.path.join(\"path\", \"to\", \"dir\")\n</code></pre> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>def ensure_dir(func):\n    r\"\"\"\n    Decorator to ensure a directory property exists.\n\n    Note:\n        Please avoid using this with `cached_property`.\n\n    Examples:\n        &gt;&gt;&gt; @property\n        ... @ensure_dir\n        ... def dir(self) -&gt; str:\n        ...     return os.path.join(\"path\", \"to\", \"dir\")\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        path = abspath(func(*args, **kwargs))\n        makedirs(path, exist_ok=True)\n        return path\n\n    return wrapper\n</code></pre>"},{"location":"package/#danling.flexible_decorator","title":"<code>flexible_decorator(maybe_decorator=None)</code>","text":"<p>Meta decorator to allow bracket-less decorator when no arguments are passed.</p> <p>Examples:</p> <p>For decorator defined as follows:</p> Python Console Session<pre><code>&gt;&gt;&gt; @flexible_decorator\n... def decorator(*args, **kwargs):\n...     def wrapper(func, *args, **kwargs):\n...         pass\n...     return wrapper\n</code></pre> <p>The following two are equivalent:</p> Python Console Session<pre><code>&gt;&gt;&gt; @decorator\n... def func(*args, **kwargs):\n...     pass\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; @decorator()\n... def func(*args, **kwargs):\n...     pass\n</code></pre> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>def flexible_decorator(maybe_decorator: Optional[Callable] = None):\n    r\"\"\"\n    Meta decorator to allow bracket-less decorator when no arguments are passed.\n\n    Examples:\n        For decorator defined as follows:\n\n        &gt;&gt;&gt; @flexible_decorator\n        ... def decorator(*args, **kwargs):\n        ...     def wrapper(func, *args, **kwargs):\n        ...         pass\n        ...     return wrapper\n\n        The following two are equivalent:\n\n        &gt;&gt;&gt; @decorator\n        ... def func(*args, **kwargs):\n        ...     pass\n\n        &gt;&gt;&gt; @decorator()\n        ... def func(*args, **kwargs):\n        ...     pass\n    \"\"\"\n\n    def decorator(func: Callable):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if len(args) == 1 and isfunction(args[0]):\n                return func(**kwargs)(args[0])\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    if maybe_decorator is None:\n        return decorator\n    return decorator(maybe_decorator)\n</code></pre>"},{"location":"package/#danling.is_json_serializable","title":"<code>is_json_serializable(obj)</code>","text":"<p>Check if <code>obj</code> is JSON serializable.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def is_json_serializable(obj: Any) -&gt; bool:\n    r\"\"\"\n    Check if `obj` is JSON serializable.\n    \"\"\"\n    try:\n        json.dumps(obj)\n        return True\n    except (TypeError, OverflowError):\n        return False\n</code></pre>"},{"location":"package/#danling.load","title":"<code>load(file, *args, **kwargs)</code>","text":"<p>Load any file with supported extensions.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def load(file: PathStr, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n    r\"\"\"\n    Load any file with supported extensions.\n    \"\"\"\n    if not os.path.isfile(file):\n        raise ValueError(f\"Trying to load {file!r} but it is not a file.\")\n    extension = os.path.splitext(file)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if not TORCH_AVAILABLE:\n            raise ImportError(f\"Trying to load {file!r} but torch is not installed.\")\n        return torch.load(file, *args, **kwargs)\n    if extension in NUMPY:\n        if not NUMPY_AVAILABLE:\n            raise ImportError(f\"Trying to load {file!r} but numpy is not installed.\")\n        return numpy.load(file, *args, **kwargs)\n    if extension in JSON:\n        with open(file) as fp:\n            return json.load(fp, *args, **kwargs)  # type: ignore\n    if extension in YAML:\n        with open(file) as fp:\n            return yaml.load(fp, *args, **kwargs)  # type: ignore\n    if extension in PICKLE:\n        with open(file, \"rb\") as fp:\n            return pickle.load(fp, *args, **kwargs)  # type: ignore\n    if extension in PANDAS_SUPPORTED:\n        return load_pandas(file, *args, **kwargs)\n    raise ValueError(f\"Tying to load {file!r} with unsupported extension={extension!r}\")\n</code></pre>"},{"location":"package/#danling.load_pandas","title":"<code>load_pandas(file, *args, **kwargs)</code>","text":"<p>Load any pandas data file with supported extensions.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def load_pandas(file: PathStr, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n    r\"\"\"\n    Load any pandas data file with supported extensions.\n    \"\"\"\n    if not PANDAS_AVAILABLE:\n        raise ImportError(f\"Trying to load {file!r} but pandas is not installed.\")\n    if not os.path.isfile(file):\n        raise ValueError(f\"Trying to load {file!r} but it is not a file.\")\n    extension = os.path.splitext(file)[-1].lower()[1:]\n    if extension in PANDAS or extension in PICKLE:\n        return pandas.read_pickle(file, *args, **kwargs)\n    if extension in H5:\n        return pandas.read_hdf(file, *args, **kwargs)\n    if extension in CSV:\n        return pandas.read_csv(file, *args, **kwargs)\n    if extension in JSON:\n        return pandas.read_json(file, *args, **kwargs)\n    if extension in EXCEL:\n        return pandas.read_excel(file, *args, **kwargs)\n    if extension in XML:\n        return pandas.read_xml(file, *args, **kwargs)\n    if extension in SQL:\n        return pandas.read_sql(file, *args, **kwargs)\n    raise ValueError(f\"Tying to load {file!r} with unsupported extension={extension!r}\")\n</code></pre>"},{"location":"package/#danling.method_cache","title":"<code>method_cache(maxsize=128, typed=False)</code>","text":"<p>Decorator to cache the result of an instance method.</p> <p><code>functools.lru_cache</code> uses a strong reference to the instance, which will make the instance immortal and break the garbage collection.</p> <p><code>method_cache</code> uses a weak reference to the instance to resolve this issue.</p> See Also Source code in <code>danling/utils/decorators.py</code> Python<pre><code>@flexible_decorator\ndef method_cache(maxsize: int | None = 128, typed: bool = False):\n    r\"\"\"\n    Decorator to cache the result of an instance method.\n\n    `functools.lru_cache` uses a strong reference to the instance,\n    which will make the instance immortal and break the garbage collection.\n\n    `method_cache` uses a weak reference to the instance to resolve this issue.\n\n    See Also:\n        https://rednafi.github.io/reflections/dont-wrap-instance-methods-with-functoolslru_cache-decorator-in-python.html\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            self_ref = ref(self)\n\n            @wraps(func)\n            @lru_cache(maxsize=maxsize, typed=typed)\n            def cached_method(*args, **kwargs):\n                return func(self_ref(), *args, **kwargs)\n\n            setattr(self, func.__name__, cached_method)\n            return cached_method(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"package/#danling.save","title":"<code>save(obj, file, *args, **kwargs)</code>","text":"<p>Save any file with supported extensions.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def save(obj: Any, file: PathStr, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; File:\n    r\"\"\"\n    Save any file with supported extensions.\n    \"\"\"\n    extension = os.path.splitext(file)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if not TORCH_AVAILABLE:\n            raise ImportError(f\"Trying to save {obj} to {file!r} but torch is not installed.\")\n        torch.save(obj, file, *args, **kwargs)\n    elif extension in NUMPY:\n        if not NUMPY_AVAILABLE:\n            raise ImportError(f\"Trying to save {obj} to {file!r} but numpy is not installed.\")\n        numpy.save(file, obj, *args, **kwargs)\n    elif extension in PANDAS:\n        if not PANDAS_AVAILABLE:\n            raise ImportError(f\"Trying to save {obj} to {file!r} but pandas is not installed.\")\n        pandas.to_pickle(obj, file, *args, **kwargs)\n    elif extension in CSV:\n        if isinstance(obj, pandas.DataFrame):\n            obj.to_csv(file, *args, **kwargs)\n        else:\n            raise NotImplementedError(f\"Trying to save {obj} to {file!r} but is not supported\")\n    elif extension in JSON:\n        if isinstance(obj, FlatDict):\n            obj.json(file)\n        else:\n            with open(file, \"w\") as fp:\n                json.dump(obj, fp, *args, **kwargs)  # type: ignore\n    elif extension in YAML:\n        if isinstance(obj, FlatDict):\n            obj.yaml(file)\n        else:\n            with open(file, \"w\") as fp:\n                yaml.dump(obj, fp, *args, **kwargs)  # type: ignore\n    elif extension in PICKLE:\n        with open(file, \"wb\") as fp:\n            pickle.dump(obj, fp, *args, **kwargs)  # type: ignore\n    else:\n        raise ValueError(f\"Tying to save {obj} to {file!r} with unsupported extension={extension!r}\")\n    return file\n</code></pre>"},{"location":"about/license/","title":"GNU AFFERO GENERAL PUBLIC LICENSE","text":"<p>Version 3, 19 November 2007</p> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"about/license/#preamble","title":"Preamble","text":"<p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program\u2013to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users\u2019 freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"about/license/#terms-and-conditions","title":"TERMS AND CONDITIONS","text":""},{"location":"about/license/#0-definitions","title":"0. Definitions.","text":"<p>\u201cThis License\u201d refers to version 3 of the GNU Affero General Public License.</p> <p>\u201cCopyright\u201d also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\u201cThe Program\u201d refers to any copyrightable work licensed under this License. Each licensee is addressed as \u201cyou\u201d. \u201cLicensees\u201d and \u201crecipients\u201d may be individuals or organizations.</p> <p>To \u201cmodify\u201d a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \u201cmodified version\u201d of the earlier work or a work \u201cbased on\u201d the earlier work.</p> <p>A \u201ccovered work\u201d means either the unmodified Program or a work based on the Program.</p> <p>To \u201cpropagate\u201d a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \u201cconvey\u201d a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \u201cAppropriate Legal Notices\u201d to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p>"},{"location":"about/license/#1-source-code","title":"1. Source Code.","text":"<p>The \u201csource code\u201d for a work means the preferred form of the work for making modifications to it. \u201cObject code\u201d means any non-source form of a work.</p> <p>A \u201cStandard Interface\u201d means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \u201cSystem Libraries\u201d of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \u201cMajor Component\u201d, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \u201cCorresponding Source\u201d for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work\u2019s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p>"},{"location":"about/license/#2-basic-permissions","title":"2. Basic Permissions.","text":"<p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.</p>"},{"location":"about/license/#3-protecting-users-legal-rights-from-anti-circumvention-law","title":"3. Protecting Users\u2019 Legal Rights From Anti-Circumvention Law.","text":"<p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work\u2019s users, your or third parties\u2019 legal rights to forbid circumvention of technological measures.</p>"},{"location":"about/license/#4-conveying-verbatim-copies","title":"4. Conveying Verbatim Copies.","text":"<p>You may convey verbatim copies of the Program\u2019s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p>"},{"location":"about/license/#5-conveying-modified-source-versions","title":"5. Conveying Modified Source Versions.","text":"<p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <ul> <li>a) The work must carry prominent notices stating that you modified   it, and giving a relevant date.</li> <li>b) The work must carry prominent notices stating that it is   released under this License and any conditions added under   section 7. This requirement modifies the requirement in section 4   to \u201ckeep intact all notices\u201d.</li> <li>c) You must license the entire work, as a whole, under this   License to anyone who comes into possession of a copy. This   License will therefore apply, along with any applicable section 7   additional terms, to the whole of the work, and all its parts,   regardless of how they are packaged. This License gives no   permission to license the work in any other way, but it does not   invalidate such permission if you have separately received it.</li> <li>d) If the work has interactive user interfaces, each must display   Appropriate Legal Notices; however, if the Program has interactive   interfaces that do not display Appropriate Legal Notices, your   work need not make them do so.</li> </ul> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \u201caggregate\u201d if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation\u2019s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p>"},{"location":"about/license/#6-conveying-non-source-forms","title":"6. Conveying Non-Source Forms.","text":"<p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <ul> <li>a) Convey the object code in, or embodied in, a physical product   (including a physical distribution medium), accompanied by the   Corresponding Source fixed on a durable physical medium   customarily used for software interchange.</li> <li>b) Convey the object code in, or embodied in, a physical product   (including a physical distribution medium), accompanied by a   written offer, valid for at least three years and valid for as   long as you offer spare parts or customer support for that product   model, to give anyone who possesses the object code either (1) a   copy of the Corresponding Source for all the software in the   product that is covered by this License, on a durable physical   medium customarily used for software interchange, for a price no   more than your reasonable cost of physically performing this   conveying of source, or (2) access to copy the Corresponding   Source from a network server at no charge.</li> <li>c) Convey individual copies of the object code with a copy of the   written offer to provide the Corresponding Source. This   alternative is allowed only occasionally and noncommercially, and   only if you received the object code with such an offer, in accord   with subsection 6b.</li> <li>d) Convey the object code by offering access from a designated   place (gratis or for a charge), and offer equivalent access to the   Corresponding Source in the same way through the same place at no   further charge. You need not require recipients to copy the   Corresponding Source along with the object code. If the place to   copy the object code is a network server, the Corresponding Source   may be on a different server (operated by you or a third party)   that supports equivalent copying facilities, provided you maintain   clear directions next to the object code saying where to find the   Corresponding Source. Regardless of what server hosts the   Corresponding Source, you remain obligated to ensure that it is   available for as long as needed to satisfy these requirements.</li> <li>e) Convey the object code using peer-to-peer transmission,   provided you inform other peers where the object code and   Corresponding Source of the work are being offered to the general   public at no charge under subsection 6d.</li> </ul> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \u201cUser Product\u201d is either (1) a \u201cconsumer product\u201d, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \u201cnormally used\u201d refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\u201cInstallation Information\u201d for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p>"},{"location":"about/license/#7-additional-terms","title":"7. Additional Terms.","text":"<p>\u201cAdditional permissions\u201d are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <ul> <li>a) Disclaiming warranty or limiting liability differently from the   terms of sections 15 and 16 of this License; or</li> <li>b) Requiring preservation of specified reasonable legal notices or   author attributions in that material or in the Appropriate Legal   Notices displayed by works containing it; or</li> <li>c) Prohibiting misrepresentation of the origin of that material,   or requiring that modified versions of such material be marked in   reasonable ways as different from the original version; or</li> <li>d) Limiting the use for publicity purposes of names of licensors   or authors of the material; or</li> <li>e) Declining to grant rights under trademark law for use of some   trade names, trademarks, or service marks; or</li> <li>f) Requiring indemnification of licensors and authors of that   material by anyone who conveys the material (or modified versions   of it) with contractual assumptions of liability to the recipient,   for any liability that these contractual assumptions directly   impose on those licensors and authors.</li> </ul> <p>All other non-permissive additional terms are considered \u201cfurther restrictions\u201d within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p>"},{"location":"about/license/#8-termination","title":"8. Termination.","text":"<p>You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p>"},{"location":"about/license/#9-acceptance-not-required-for-having-copies","title":"9. Acceptance Not Required for Having Copies.","text":"<p>You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p>"},{"location":"about/license/#10-automatic-licensing-of-downstream-recipients","title":"10. Automatic Licensing of Downstream Recipients.","text":"<p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \u201centity transaction\u201d is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party\u2019s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p>"},{"location":"about/license/#11-patents","title":"11. Patents.","text":"<p>A \u201ccontributor\u201d is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor\u2019s \u201ccontributor version\u201d.</p> <p>A contributor\u2019s \u201cessential patent claims\u201d are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \u201ccontrol\u201d includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor\u2019s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \u201cpatent license\u201d is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \u201cgrant\u201d such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \u201cKnowingly relying\u201d means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient\u2019s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \u201cdiscriminatory\u201d if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p>"},{"location":"about/license/#12-no-surrender-of-others-freedom","title":"12. No Surrender of Others\u2019 Freedom.","text":"<p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p>"},{"location":"about/license/#13-remote-network-interaction-use-with-the-gnu-general-public-license","title":"13. Remote Network Interaction; Use with the GNU General Public License.","text":"<p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p>"},{"location":"about/license/#14-revised-versions-of-this-license","title":"14. Revised Versions of this License.","text":"<p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License \u201cor any later version\u201d applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy\u2019s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p>"},{"location":"about/license/#15-disclaimer-of-warranty","title":"15. Disclaimer of Warranty.","text":"<p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \u201cAS IS\u201d WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p>"},{"location":"about/license/#16-limitation-of-liability","title":"16. Limitation of Liability.","text":"<p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"about/license/#17-interpretation-of-sections-15-and-16","title":"17. Interpretation of Sections 15 and 16.","text":"<p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"about/license/#how-to-apply-these-terms-to-your-new-programs","title":"How to Apply These Terms to Your New Programs","text":"<p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \u201ccopyright\u201d line and a pointer to where the full notice is found.</p> Text Only<pre><code>    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a \u201cSource\u201d link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \u201ccopyright disclaimer\u201d for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"about/privacy/","title":"Privacy Notice","text":"<p>Last Revised Date</p> <p>This policy was last updated on May 04, 2024.</p>"},{"location":"about/privacy/#privacy-policy","title":"Privacy Policy","text":"<p>This privacy notice for DanLing Team (also known as DanLing) (\u2018we\u2019, \u2018us\u2019, or \u2018our\u2019), describes how and why we might collect, store, use, and/or share (\u2018process\u2019) your information when you use our services (\u2018Services\u2019), such as when you:</p> <ul> <li>Visit our website at danling.org, or any website of ours that links to this privacy notice</li> </ul> <p>You can change your privacy settings at any time by clicking the button below:</p> <p>Privacy Control</p> <p>Questions or concerns? Reading this privacy notice will help you understand your privacy rights and choices. If you do not agree with our policies and practices, please do not use our Services. If you still have any questions or concerns, please contact us at privacy@danling.org.</p>"},{"location":"about/privacy/#0-summary-of-key-points","title":"0. Summary of Key Points","text":"<p>This summary provides key points from our privacy notice, but you can find out more details about any of these topics by clicking the link following each key point or by using our table of contents below to find the section you are looking for.</p> <p>What personal information do we process?</p> <p>When you visit, use, or navigate our Services, we may process personal information depending on how you interact with us and the Services, the choices you make, and the products and features you use.</p> <p>Learn more about what information do we collect.</p> <p>Do we process any sensitive personal information?</p> <p>We do not process any sensitive personal information.</p> <p>Do we collect any information from third parties?</p> <p>We do not collect any information from third parties.</p> <p>How do we process your information?</p> <p>We process your information to provide, improve, and administer our Services, communicate with you, for security and fraud prevention, and to comply with law. We may also process your information for other purposes with your consent. We process your information only when we have a valid legal reason to do so.</p> <p>Learn more about how we process your information.</p> <p>In what situations and with which parties do we share personal information?</p> <p>We may share information in specific situations and with specific third parties.</p> <p>Learn more about when and with whom we share your personal information.</p> <p>How do we keep your information safe?</p> <p>We have organisational and technical processes and procedures in place to protect your personal information. However, no electronic transmission over the internet or information storage technology can be guaranteed to be 100% secure, so we cannot promise or guarantee that hackers, cybercriminals, or other unauthorised third parties will not be able to defeat our security and improperly collect, access, steal, or modify your information.</p> <p>Learn more about how we keep your information safe.</p> <p>What are your rights?</p> <p>Depending on where you are located geographically, the applicable privacy law may mean you have certain rights regarding your personal information.</p> <p>Learn more about your privacy rights.</p> <p>How do you exercise your rights?</p> <p>The easiest way to exercise your rights is by contacting us via privacy@danling.org. We will consider and act upon any request in accordance with applicable data protection laws.</p>"},{"location":"about/privacy/#1-what-information-do-we-collect","title":"1. What information do we collect?","text":""},{"location":"about/privacy/#personal-information-you-disclose-to-us","title":"Personal information you disclose to us","text":"<p>In Short</p> <p>We collect personal information that you provide to us.</p> <p>We collect personal information that you voluntarily provide to us when you express an interest in obtaining information about us or our products and Services, when you participate in activities on the Services, or otherwise when you contact us.</p> <p>Sensitive Personal Information</p> <p>We do not collect any sensitive personal information from you.</p>"},{"location":"about/privacy/#information-automatically-collected","title":"Information automatically collected","text":"<p>In Short</p> <p>Some information \u2014 such as IP address and/or browser and device characteristics \u2014 is collected automatically when you visit our Services.</p> <p>We automatically collect certain information when you visit, use, or navigate our Services. This information does not reveal your specific identity (like your name or contact information) but may include device and usage information, such as your IP address, browser and device characteristics, operating system, language preferences, referring URLs, device name, country, location, information about how and when you use our Services, and other technical information. This information is primarily needed to maintain the security and operation of our Services, and for our internal analytics and reporting purposes.</p> <p>Like many businesses, we also collect information through cookies and similar technologies.</p> <p>The information we collect includes:</p> <ul> <li>Identifiers.   Identifier is a device and browser-specific unique random string that we generate when you use our Service.   This identifier is stored in a cookie on your device, allowing us to identify you across multiple sessions and when you return to our Service.   You can delete this cookie at any time by clearing your browser\u2019s cache.</li> <li>Log and Usage Data.   Log and usage data is service-related, diagnostic, usage, and performance information our servers automatically collect when you access or use our Services and which we record in log files.   Depending on how you interact with us, this log data may include your IP address, device information, browser type, and settings, and information about your activity in the Services (such as the date/time stamps associated with your usage, pages and files viewed, searches and other actions you take such as which features you use), device event information (such as system activity, error reports (sometimes called \u2018crash dumps\u2019) and hardware settings).</li> <li>Device Data.   We collect device data such as information about your computer, phone, tablet, or other devices you use to access the Services.   Depending on the device used, this device data may include information such as your IP address (or proxy server), device and application identification numbers, location, browser type, hardware model, Internet Service Provider and/or mobile carrier, operating system, and system configuration information.</li> <li>Location Data.   We collect location data such as information about your device\u2019s location, which can be either precise or imprecise.   How much information we collect depends on the type and settings of the device you use to access the Services.   For example, we may use GPS and other technologies to collect geolocation data that tells us your current location (based on your IP address).   You can opt out of allowing us to collect this information either by refusing access to the information or by disabling your location settings on your device.</li> </ul>"},{"location":"about/privacy/#categories-of-personal-information-we-collect","title":"Categories of Personal Information We Collect","text":"<p>We have collected the following categories of personal information in the past twelve (12) months:</p> Category Examples Collected A. Identifiers Contact details, such as real name, alias, postal address, telephone or mobile contact number, unique personal identifier, online identifier, Internet Protocol address, email address, and account name YES B. Personal information as defined in the California Customer Records statute Name, contact information, education, employment, employment history, and financial information NO C. Protected classification characteristics under state or federal law Gender, age, date of birth, race and ethnicity, national origin, marital status, and other demographic data NO D. Commercial information Transaction information, purchase history, financial details, and payment information NO E. Biometric information Fingerprints and voiceprints NO F. Internet or other similar network activity Browsing history, search history, online behaviour, interest data, and interactions with our and other websites, applications, systems, and advertisements YES G. Geolocation data Device location YES H. Audio, electronic, sensory, or similar information Images and audio, video or call recordings created in connection with our business activities NO I. Professional or employment-related information Business contact details in order to provide you our Services at a business level or job title, work history, and professional qualifications if you apply for a job with us NO J. Education Information Student records and directory information NO K. Inferences drawn from collected personal information Inferences drawn from any of the collected personal information listed above to create a profile or summary about, for example, an individual\u2019s preferences and characteristics YES L. Sensitive personal Information NO <p>We may also collect other personal information outside of these categories through instances where you interact with us in person, online, or by phone or mail in the context of:</p> <ul> <li>Receiving help through our customer support channels;</li> <li>Participation in customer surveys or contests; and</li> <li>Facilitation in the delivery of our Services and to respond to your inquiries.</li> </ul> <p>We will use and retain the collected personal information as needed to provide you with our Services and as necessary to comply with our legal obligations, resolve disputes, and enforce our agreement for the following period:</p> <ul> <li>Category A: Indefinitely</li> <li>Category F: Indefinitely</li> <li>Category G: Indefinitely</li> <li>Category K: Indefinitely</li> </ul>"},{"location":"about/privacy/#2-how-do-we-process-your-information","title":"2. How do we process your information?","text":"<p>In Short</p> <p>We process your information to provide, improve, and administer our Services, communicate with you, for security and fraud prevention, and to comply with law. We may also process your information for other purposes with your consent.</p> <p>We process your personal information for a variety of reasons, depending on how you interact with our Services, including:</p> <ul> <li>To protect our Services.   We may process your information as part of our efforts to keep our Services safe and secure, including fraud monitoring and prevention.</li> <li>To identify user trends.   We may process information about how you use our Services to better understand how they are being used so we can improve them.</li> <li>To save or protect an individual\u2019s vital interest.   We may process your information when necessary to save or protect an individual\u2019s vital interest, such as to prevent harm.</li> </ul>"},{"location":"about/privacy/#3-what-legal-basis-do-we-have-for-processing-your-information","title":"3. What legal basis do we have for processing your information?","text":"<p>In Short</p> <p>We only process your personal information when we believe it is necessary and we have a valid legal reason (i.e. legal basis) to do so under applicable law, like with your consent, to comply with laws, to provide you with services to enter into or fulfil our contractual obligations, to protect your rights, or to fulfil our legitimate business interests.</p> <p>The General Data Protection Regulation (GDPR) and UK GDPR require us to explain the valid legal bases we rely on in order to process your personal information. As such, we may rely on the following legal bases to process your personal information:</p> <ul> <li>Consent.   We may process your personal information if you have given us specific consent to use your personal information for a specific purpose.   You have the right to withdraw your consent at any time.   Learn more about withdrawing your consents.</li> <li>Legitimate Interests.   We may process your information when we believe it is reasonably necessary to achieve our legitimate business interests and those interests do not outweigh your interests and fundamental rights and freedoms.   For example, we may process your personal information for some of the purposes described in order to:</li> <li>Analyse how our Services are used so we can improve them to engage and retain users</li> <li>Diagnose problems and/or prevent fraudulent activities</li> <li>Legal Obligations.   We may process your information where we believe it is necessary for compliance with our legal obligations, such as to cooperate with a law enforcement body or regulatory agency, exercise or defend our legal rights, or disclose your information as evidence in litigation in which we are involved.</li> <li>Vital Interests.   We may process your information where we believe it is necessary to protect your vital interests or the vital interests of a third party, such as situations involving potential threats to the safety of any person.</li> </ul> <p>Consent to Processing in Canada</p> <p>If you are located in Canada, we may be legally permitted under applicable law to process your information without your consent in some exceptional cases, including, for example:</p> <ul> <li>If collection is clearly in the interests of an individual and consent cannot be obtained in a timely way</li> <li>For investigations and fraud detection and prevention</li> <li>For business transactions provided certain conditions are met</li> <li>If it is contained in a witness statement and the collection is necessary to assess, process, or settle an insurance claim</li> <li>For identifying injured, ill, or deceased persons and communicating with next of kin</li> <li>If we have reasonable grounds to believe an individual has been, is, or may be victim of financial abuse</li> <li>If it is reasonable to expect collection and use with consent would compromise the availability or the accuracy of the information and the collection is reasonable for purposes related to investigating a breach of an agreement or a contravention of the laws of Canada or a province</li> <li>If disclosure is required to comply with a subpoena, warrant, court order, or rules of the court relating to the production of records</li> <li>If it was produced by an individual in the course of their employment, business, or profession and the collection is consistent with the purposes for which the information was produced</li> <li>If the collection is solely for journalistic, artistic, or literary purposes</li> <li>If the information is publicly available and is specified by the regulations</li> </ul>"},{"location":"about/privacy/#4-when-and-with-whom-do-we-share-your-personal-information","title":"4. When and with whom do we share your personal information?","text":"<p>In Short</p> <p>We may share information in specific situations described in this section and/or with the following third parties.</p> <p>We may use your personal information for our business purposes, such as for undertaking internal research for technological development and demonstration. This is not considered to be \u2018selling\u2019 of your personal information.</p> <p>Vendors, Consultants, and Other Third-Party Service Providers. We may share your data with third-party vendors, service providers, contractors, or agents (\u2018third parties\u2019) who perform services for us or on our behalf and require access to such information to do that work. We have contracts in place with our third parties, which are designed to help safeguard your personal information. This means that they cannot do anything with your personal information unless we have instructed them to do it. They will also not share your personal information with any organisation apart from us. They also commit to protect the data they hold on our behalf and to retain it for the period we instruct.</p> <p>The third parties we may share personal information with are as follows:</p> <ul> <li>Advertising, Direct Marketing, and Lead Generation</li> <li>Google AdSense</li> <li>Cloud Computing Services</li> <li>Microsoft Azure</li> <li>Amazon Web Services (AWS)</li> <li>Google Cloud Platform (GCP)</li> <li>Communications and Content Delivery Network (CDN) Services</li> <li>Cloudflare</li> <li>Content Optimisation</li> <li>Google Site Search</li> <li>Google Fonts</li> <li>Functionality and Infrastructure Optimisation   GitHub Pages</li> <li>User Commenting and Forums</li> <li>Disqus</li> <li>GitHub Issues</li> <li>GitHub Discussions</li> <li>Web and Mobile Analytics</li> <li>Google Analytics</li> </ul> <p>We also may need to share your personal information in the following situations:</p> <ul> <li>Business Transfers.   We may share or transfer your information in connection with, or during negotiations of, any merger, sale of company assets, financing, or acquisition of all or a portion of our business to another company.</li> </ul> <p>We have disclosed the following categories of personal information for a business purpose in the past twelve (12) months:</p> <p>Nill</p> <p>The categories of third parties to whom we sold personal information in the past twelve (12) months:</p> <p>Nill</p> <p>The categories of third parties to whom we shared personal information with in the past twelve (12) months:</p> <ul> <li>Advertising, Direct Marketing, and Lead Generation<ul> <li>Google AdSense</li> </ul> </li> <li>Web and Mobile Analytics<ul> <li>Google Analytics</li> </ul> </li> </ul>"},{"location":"about/privacy/#5-do-we-use-cookies-and-other-tracking-technologies","title":"5. Do we use cookies and other tracking technologies?","text":"<p>In Short</p> <p>We may use cookies and other tracking technologies to collect and store your information.</p> <p>We also permit third parties and service providers to use online tracking technologies on our Services for analytics and advertising, including to help manage and display advertisements, to tailor advertisements to your interests, or to send abandoned shopping cart reminders (depending on your communication preferences). The third parties and service providers use their technology to provide advertising about products and services tailored to your interests which may appear either on our Services or on other websites.</p> <p>To the extent these online tracking technologies are deemed to be a \u2018sale\u2019/\u2019sharing\u2019 (which includes targeted advertising, as defined under the applicable laws) under applicable US state laws, you can opt out of these online tracking technologies by clicking the button on the top of this page or the button below:</p> <p>Privacy Control</p>"},{"location":"about/privacy/#google-analytics","title":"Google Analytics","text":"<p>We may share your information with Google Analytics to track and analyse the use of the Services. The Google Analytics Advertising Features that we may use include: Remarketing with Google Analytics, Google Display Network Impressions Reporting and Google Analytics Demographics and Interests Reporting. To opt out of being tracked by Google Analytics across the Services, visit https://tools.google.com/dlpage/gaoptout. You can opt out of Google Analytics Advertising Features through Ads Settings and Ad Settings for mobile apps. Other opt out means include http://optout.networkadvertising.org/ and http://www.networkadvertising.org/mobile-choice. For more information on the privacy practices of Google, please visit the Google Privacy &amp; Terms.</p>"},{"location":"about/privacy/#6-how-long-do-we-keep-your-information","title":"6. How long do we keep your information?","text":"<p>In Short</p> <p>We keep your information for as long as necessary to fulfil the purposes outlined in this privacy notice unless otherwise required by law.</p> <p>We will only keep your personal information for as long as it is necessary for the purposes set out in this privacy notice, unless a longer retention period is required or permitted by law (such as tax, accounting, or other legal requirements).</p> <p>When we have no ongoing legitimate business need to process your personal information, we will either delete or anonymise it, or, if this is not possible (for example, because your personal information has been stored in backup archives), then we will securely store your personal information and isolate it from any further processing until deletion is possible.</p>"},{"location":"about/privacy/#7-how-do-we-keep-your-information-safe","title":"7. How do we keep your information safe?","text":"<p>In Short</p> <p>We aim to protect your personal information through a system of organisational and technical security measures.</p> <p>We have implemented appropriate technical and organisational security measures designed to protect the security of any personal information we process. However, despite our safeguards and efforts to secure your information, no electronic transmission over the internet or information storage technology can be guaranteed to be 100% secure, so we cannot promise or guarantee that hackers, cybercriminals, or other unauthorised third parties will not be able to defeat our security and improperly collect, access, steal, or modify your information. Although we will do our best to protect your personal information, the transmission of personal information to and from our Services is at your own risk. You should only access the Services within a secure environment.</p>"},{"location":"about/privacy/#8-what-are-your-privacy-rights","title":"8. What are your privacy rights?","text":"<p>In Short</p> <p>We strive to protect your privacy rights and choices to the best possible extent under the law.</p> <p>You have rights under certain data protection laws. However, these rights are not absolute, and in certain cases, we may decline your request as permitted by law. These rights include:</p> <ul> <li>Right to know   whether or not we are processing your personal data</li> <li>Right to access   your personal data</li> <li>Right to correct   inaccuracies in your personal data</li> <li>Right to request   the deletion of your personal data</li> <li>Right to obtain a copy   of the personal data you previously shared with us</li> <li>Right to non-discrimination   against you for exercising your rights</li> <li>Right to opt-out</li> <li>of the processing of your personal data if it is used for targeted advertising (or sharing as defined under applicable laws), the sale of personal data, or profiling in furtherance of decisions that produce legal or similarly significant effects (\u2018profiling\u2019) concerning you</li> <li>of the collection of sensitive data and personal data collected through the operation of a voice or facial recognition feature</li> <li>Right to obtain</li> <li>a list of the categories of third parties to which we have disclosed personal data</li> <li>a list of specific third parties to which we have disclosed personal data</li> <li>Right to limit   use and disclosure of sensitive personal data</li> </ul>"},{"location":"about/privacy/#how-to-exercise-your-rights","title":"How to exercise your rights","text":"<p>It is very unlikely that you will be able to exercise the above rights as we do not collect any identifiable personal data from you.</p> <p>We are unable to reply to and act on data subject access request as we do not save any identifiable information about you, and we will not be able to verify your identity.</p> <p>If you believe we are unlawfully processing your personal information, you can contact the relevant data protection regulator, state attorney general, or other competent authority in your jurisdiction.</p> Residency Authority European Economic Area Member State\u2019s data protection supervisory authority United Kingdom Information Commissioner\u2019s Office Australia Office of the Australian Information Commissioner New Zealand Office of New Zealand Privacy Commissioner Canada Office of the Privacy Commissioner of Canada California of the United States California Privacy Protection Agency Switzerland Federal Data Protection and Information Commissioner South Africa Information Regulator"},{"location":"about/privacy/#withdraw-your-consent","title":"Withdraw your consent","text":"<p>If we are relying on your consent to process your personal information, which may be express and/or implied consent depending on the applicable law, you have the right to withdraw your consent at any time. You can withdraw your consent at any time by clicking the button on the top of this page or the button below:</p> <p>Privacy Control</p> <p>However, please note that this will not affect the lawfulness of the processing before its withdrawal nor, when applicable law allows, will it affect the processing of your personal information conducted in reliance on lawful processing grounds other than consent.</p>"},{"location":"about/privacy/#cookies-and-similar-technologies","title":"Cookies and similar technologies","text":"<p>Most web browsers are set to accept cookies by default. If you prefer, you can usually choose to set your browser to remove or reject browser cookies. Please note that if you choose to remove or reject cookies, this will NOT affect the availability and functionality of our Services.</p>"},{"location":"about/privacy/#9-controls-for-do-not-track-features","title":"9. Controls for Do-Not-Track features","text":"<p>Most web browsers and some mobile operating systems and mobile applications include a Do-Not-Track (\u2018DNT\u2019) feature or setting you can activate to signal your privacy preference not to have data about your online browsing activities monitored and collected. At this stage, no uniform technology standard for recognising and implementing DNT signals has been finalised. Although we cannot promise to honour every DNT signal, we strive to honour all such requests where technically feasible.</p> <p>California law requires us to let you know how we respond to web browser DNT signals. Because we cannot guarantee to recognise and houour all DNT signals, we do not respond to them at this time.</p>"},{"location":"about/privacy/#10-do-residents-in-certain-jurisdiction-have-specific-privacy-rights","title":"10. Do residents in certain jurisdiction have specific privacy rights?","text":"<p>NO.</p> <p>All men and women are created equal.</p> <p>We provide the same privacy rights to all individuals, regardless of their location.</p> <p>Be assured that we will treat you with the same respect and dignity as we would want to be treated.</p>"},{"location":"about/privacy/#11-how-can-you-review-update-or-delete-the-data-we-collect-from-you","title":"11. How can you review, update, or delete the data we collect from you?","text":"<p>It is very unlikely that you will be able to review, update, or delete the data we collect from you as we do not collect any identifiable personal data from you, and we will not be able to identify which data belongs to you.</p>"},{"location":"about/privacy/#12-do-we-make-updates-to-this-notice","title":"12. Do we make updates to this notice?","text":"<p>In Short</p> <p>Yes, we will update this notice as necessary to stay compliant with relevant laws.</p> <p>We may update this privacy notice from time to time. The updated version will be indicated by an updated \u2018Last Revised Date\u2019 at the top of this privacy notice. If we make any material changes, we will notify you by posting the new privacy notice on this page. We are unable to notify you directly as we do not collect any contact information from you. We encourage you to review this privacy notice frequently to stay informed of how we are protecting your information.</p>"},{"location":"blog/","title":"DanLing","text":""},{"location":"metrics/average_meter/","title":"AverageMeter","text":""},{"location":"metrics/average_meter/#danling.metrics.average_meter.AverageMeter","title":"<code>AverageMeter</code>","text":"<p>Computes and stores the average and current value.</p> <p>Attributes:</p> Name Type Description <code>val</code> <code>float</code> <p>Results of current batch on current device.</p> <code>bat</code> <p>Results of current batch on all devices.</p> <code>avg</code> <p>Results of all results on all devices.</p> <code>sum</code> <code>float</code> <p>Sum of values.</p> <code>count</code> <code>float</code> <p>Number of values.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\nnan\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>class AverageMeter:\n    r\"\"\"\n    Computes and stores the average and current value.\n\n    Attributes:\n        val: Results of current batch on current device.\n        bat: Results of current batch on all devices.\n        avg: Results of all results on all devices.\n        sum: Sum of values.\n        count: Number of values.\n\n    Examples:\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        nan\n    \"\"\"\n\n    val: float = 0\n    n: float = 1\n    sum: float = 0\n    count: float = 0\n\n    def __init__(self) -&gt; None:\n        self.reset()\n\n    def reset(self) -&gt; None:\n        r\"\"\"\n        Resets the meter.\n\n        Examples:\n            &gt;&gt;&gt; meter = AverageMeter()\n            &gt;&gt;&gt; meter.update(0.7)\n            &gt;&gt;&gt; meter.val\n            0.7\n            &gt;&gt;&gt; meter.avg\n            0.7\n            &gt;&gt;&gt; meter.reset()\n            &gt;&gt;&gt; meter.val\n            0\n            &gt;&gt;&gt; meter.avg\n            nan\n        \"\"\"\n\n        self.val = 0\n        self.n = 1\n        self.sum = 0\n        self.count = 0\n\n    def update(self, value, n: float = 1) -&gt; None:\n        r\"\"\"\n        Updates the average and current value in the meter.\n\n        Args:\n            value: Value to be added to the average.\n            n: Number of values to be added.\n\n        Examples:\n            &gt;&gt;&gt; meter = AverageMeter()\n            &gt;&gt;&gt; meter.update(0.7)\n            &gt;&gt;&gt; meter.val\n            0.7\n            &gt;&gt;&gt; meter.avg\n            0.7\n            &gt;&gt;&gt; meter.update(0.9)\n            &gt;&gt;&gt; meter.val\n            0.9\n            &gt;&gt;&gt; meter.avg\n            0.8\n            &gt;&gt;&gt; meter.sum\n            1.6\n            &gt;&gt;&gt; meter.count\n            2\n        \"\"\"\n\n        self.val = value\n        self.n = n\n        self.sum += value * n\n        self.count += n\n\n    def value(self):\n        return self.val\n\n    def batch(self):\n        world_size = get_world_size()\n        if world_size == 1:\n            return self.val / self.n if self.n != 0 else float(\"nan\")\n        synced_tuple = [None for _ in range(world_size)]\n        dist.all_gather_object(synced_tuple, (self.val * self.n, self.n))\n        val, n = zip(*synced_tuple)\n        count = sum(n)\n        if count == 0:\n            return float(\"nan\")\n        return sum(val) / count\n\n    def average(self):\n        world_size = get_world_size()\n        if world_size == 1:\n            return self.sum / self.count if self.count != 0 else float(\"nan\")\n        synced_tuple = [None for _ in range(world_size)]\n        dist.all_gather_object(synced_tuple, (self.sum, self.count))\n        val, n = zip(*synced_tuple)\n        count = sum(n)\n        if count == 0:\n            return float(\"nan\")\n        return sum(val) / count\n\n    @property\n    def bat(self):\n        return self.batch()\n\n    @property\n    def avg(self):\n        return self.average()\n\n    def __format__(self, format_spec) -&gt; str:\n        return f\"{self.val.__format__(format_spec)} ({self.avg.__format__(format_spec)})\"\n</code></pre>"},{"location":"metrics/average_meter/#danling.metrics.average_meter.AverageMeter.reset","title":"<code>reset()</code>","text":"<p>Resets the meter.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.reset()\n&gt;&gt;&gt; meter.val\n0\n&gt;&gt;&gt; meter.avg\nnan\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def reset(self) -&gt; None:\n    r\"\"\"\n    Resets the meter.\n\n    Examples:\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.reset()\n        &gt;&gt;&gt; meter.val\n        0\n        &gt;&gt;&gt; meter.avg\n        nan\n    \"\"\"\n\n    self.val = 0\n    self.n = 1\n    self.sum = 0\n    self.count = 0\n</code></pre>"},{"location":"metrics/average_meter/#danling.metrics.average_meter.AverageMeter.update","title":"<code>update(value, n=1)</code>","text":"<p>Updates the average and current value in the meter.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>Value to be added to the average.</p> required <code>n</code> <code>float</code> <p>Number of values to be added.</p> <code>1</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meter = AverageMeter()\n&gt;&gt;&gt; meter.update(0.7)\n&gt;&gt;&gt; meter.val\n0.7\n&gt;&gt;&gt; meter.avg\n0.7\n&gt;&gt;&gt; meter.update(0.9)\n&gt;&gt;&gt; meter.val\n0.9\n&gt;&gt;&gt; meter.avg\n0.8\n&gt;&gt;&gt; meter.sum\n1.6\n&gt;&gt;&gt; meter.count\n2\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def update(self, value, n: float = 1) -&gt; None:\n    r\"\"\"\n    Updates the average and current value in the meter.\n\n    Args:\n        value: Value to be added to the average.\n        n: Number of values to be added.\n\n    Examples:\n        &gt;&gt;&gt; meter = AverageMeter()\n        &gt;&gt;&gt; meter.update(0.7)\n        &gt;&gt;&gt; meter.val\n        0.7\n        &gt;&gt;&gt; meter.avg\n        0.7\n        &gt;&gt;&gt; meter.update(0.9)\n        &gt;&gt;&gt; meter.val\n        0.9\n        &gt;&gt;&gt; meter.avg\n        0.8\n        &gt;&gt;&gt; meter.sum\n        1.6\n        &gt;&gt;&gt; meter.count\n        2\n    \"\"\"\n\n    self.val = value\n    self.n = n\n    self.sum += value * n\n    self.count += n\n</code></pre>"},{"location":"metrics/average_meter/#danling.metrics.average_meter.AverageMeters","title":"<code>AverageMeters</code>","text":"<p>               Bases: <code>MetricsDict</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.update({\"loss\": 0.6, \"auroc\": 0.7, \"r2\": 0.8})\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.6000 (0.6000)\nauroc: 0.7000 (0.7000)\nr2: 0.8000 (0.8000)\n&gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.9000 (0.7500)\nauroc: 0.7000 (0.7000)\nr2: 0.8000 (0.8000)\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 1.5, 'auroc': 0.7, 'r2': 0.8}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 2, 'auroc': 1, 'r2': 1}\n&gt;&gt;&gt; meters.reset()\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.0000 (nan)\nauroc: 0.0000 (nan)\nr2: 0.0000 (nan)\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>class AverageMeters(MetricsDict):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; meters = AverageMeters()\n        &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"auroc\": 0.7, \"r2\": 0.8})\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.6000 (0.6000)\n        auroc: 0.7000 (0.7000)\n        r2: 0.8000 (0.8000)\n        &gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.9000 (0.7500)\n        auroc: 0.7000 (0.7000)\n        r2: 0.8000 (0.8000)\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 1.5, 'auroc': 0.7, 'r2': 0.8}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 2, 'auroc': 1, 'r2': 1}\n        &gt;&gt;&gt; meters.reset()\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.0000 (nan)\n        auroc: 0.0000 (nan)\n        r2: 0.0000 (nan)\n    \"\"\"\n\n    def __init__(self, *args, default_factory=AverageMeter, **kwargs) -&gt; None:\n        super().__init__(*args, default_factory=default_factory, **kwargs)\n\n    @property\n    def sum(self) -&gt; FlatDict[str, float]:\n        return FlatDict({key: meter.sum for key, meter in self.all_items()})\n\n    @property\n    def count(self) -&gt; FlatDict[str, int]:\n        return FlatDict({key: meter.count for key, meter in self.all_items()})\n\n    def update(self, values: Dict, *, n: int = 1) -&gt; None:  # pylint: disable=W0237\n        r\"\"\"\n        Updates the average and current value in all meters.\n\n        Args:\n            values: Dict of values to be added to the average.\n            n: Number of values to be added.\n\n        Raises:\n            ValueError: If the value is not an instance of (int, float, Mapping).\n\n        Examples:\n            &gt;&gt;&gt; meters = AverageMeters()\n            &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"auroc\": 0.7, \"r2\": 0.8})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 0.6, 'auroc': 0.7, 'r2': 0.8}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 1, 'auroc': 1, 'r2': 1}\n            &gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 1.5, 'auroc': 0.7, 'r2': 0.8}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 2, 'auroc': 1, 'r2': 1}\n            &gt;&gt;&gt; meters.update({\"loss\": 0.8, \"auroc\": 0.9, \"r2\": 0.8})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 2.3, 'auroc': 1.6, 'r2': 1.6}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 3, 'auroc': 2, 'r2': 2}\n            &gt;&gt;&gt; meters.update({\"auroc\": 0.7, \"r2\": 0.7})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 2.3, 'auroc': 2.3, 'r2': 2.3}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 3, 'auroc': 3, 'r2': 3}\n            &gt;&gt;&gt; meters.update({\"dataset1\": {\"cls.auroc\": 0.9}, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n            Traceback (most recent call last):\n            ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'dict'&gt;\n            This is likely due to nested dictionary in the values.\n            Nested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\n            &gt;&gt;&gt; meters.update(dict(loss=\"\"))\n            Traceback (most recent call last):\n            ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'str'&gt;\n        \"\"\"  # noqa: E501\n\n        for meter, value in values.items():\n            if isinstance(value, (int, float)):\n                self[meter].update(value, n)\n            elif isinstance(value, Dict):\n                value.setdefault(\"n\", n)\n                try:\n                    self[meter].update(**value)\n                except TypeError:\n                    raise ValueError(\n                        f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\\n\"\n                        \"This is likely due to nested dictionary in the values.\\n\"\n                        \"Nested dictionaries cannot be processed due to the method's design, which uses Mapping \"\n                        \"to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\"\n                    ) from None\n            else:\n                raise ValueError(f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\")\n</code></pre>"},{"location":"metrics/average_meter/#danling.metrics.average_meter.AverageMeters.update","title":"<code>update(values, *, n=1)</code>","text":"<p>Updates the average and current value in all meters.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict</code> <p>Dict of values to be added to the average.</p> required <code>n</code> <code>int</code> <p>Number of values to be added.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is not an instance of (int, float, Mapping).</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = AverageMeters()\n&gt;&gt;&gt; meters.update({\"loss\": 0.6, \"auroc\": 0.7, \"r2\": 0.8})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 0.6, 'auroc': 0.7, 'r2': 0.8}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 1, 'auroc': 1, 'r2': 1}\n&gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 1.5, 'auroc': 0.7, 'r2': 0.8}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 2, 'auroc': 1, 'r2': 1}\n&gt;&gt;&gt; meters.update({\"loss\": 0.8, \"auroc\": 0.9, \"r2\": 0.8})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 2.3, 'auroc': 1.6, 'r2': 1.6}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 3, 'auroc': 2, 'r2': 2}\n&gt;&gt;&gt; meters.update({\"auroc\": 0.7, \"r2\": 0.7})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 2.3, 'auroc': 2.3, 'r2': 2.3}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 3, 'auroc': 3, 'r2': 3}\n&gt;&gt;&gt; meters.update({\"dataset1\": {\"cls.auroc\": 0.9}, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\nTraceback (most recent call last):\nValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'dict'&gt;\nThis is likely due to nested dictionary in the values.\nNested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\n&gt;&gt;&gt; meters.update(dict(loss=\"\"))\nTraceback (most recent call last):\nValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'str'&gt;\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def update(self, values: Dict, *, n: int = 1) -&gt; None:  # pylint: disable=W0237\n    r\"\"\"\n    Updates the average and current value in all meters.\n\n    Args:\n        values: Dict of values to be added to the average.\n        n: Number of values to be added.\n\n    Raises:\n        ValueError: If the value is not an instance of (int, float, Mapping).\n\n    Examples:\n        &gt;&gt;&gt; meters = AverageMeters()\n        &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"auroc\": 0.7, \"r2\": 0.8})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 0.6, 'auroc': 0.7, 'r2': 0.8}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 1, 'auroc': 1, 'r2': 1}\n        &gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 1.5, 'auroc': 0.7, 'r2': 0.8}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 2, 'auroc': 1, 'r2': 1}\n        &gt;&gt;&gt; meters.update({\"loss\": 0.8, \"auroc\": 0.9, \"r2\": 0.8})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 2.3, 'auroc': 1.6, 'r2': 1.6}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 3, 'auroc': 2, 'r2': 2}\n        &gt;&gt;&gt; meters.update({\"auroc\": 0.7, \"r2\": 0.7})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 2.3, 'auroc': 2.3, 'r2': 2.3}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 3, 'auroc': 3, 'r2': 3}\n        &gt;&gt;&gt; meters.update({\"dataset1\": {\"cls.auroc\": 0.9}, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n        Traceback (most recent call last):\n        ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'dict'&gt;\n        This is likely due to nested dictionary in the values.\n        Nested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\n        &gt;&gt;&gt; meters.update(dict(loss=\"\"))\n        Traceback (most recent call last):\n        ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'str'&gt;\n    \"\"\"  # noqa: E501\n\n    for meter, value in values.items():\n        if isinstance(value, (int, float)):\n            self[meter].update(value, n)\n        elif isinstance(value, Dict):\n            value.setdefault(\"n\", n)\n            try:\n                self[meter].update(**value)\n            except TypeError:\n                raise ValueError(\n                    f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\\n\"\n                    \"This is likely due to nested dictionary in the values.\\n\"\n                    \"Nested dictionaries cannot be processed due to the method's design, which uses Mapping \"\n                    \"to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\"\n                ) from None\n        else:\n            raise ValueError(f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\")\n</code></pre>"},{"location":"metrics/average_meter/#danling.metrics.average_meter.MultiTaskAverageMeters","title":"<code>MultiTaskAverageMeters</code>","text":"<p>               Bases: <code>MultiTaskDict</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = MultiTaskAverageMeters()\n&gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.6000 (0.6000)\ndataset1.cls.auroc: 0.7000 (0.7000)\ndataset1.reg.r2: 0.8000 (0.8000)\ndataset2.r2: 0.9000 (0.9000)\n&gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.9000 (0.7500)\ndataset1.cls.auroc: 0.7000 (0.7000)\ndataset1.reg.r2: 0.8000 (0.8000)\ndataset2.r2: 0.9000 (0.9000)\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 1.5, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 2, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n&gt;&gt;&gt; meters.reset()\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.0000 (nan)\ndataset1.cls.auroc: 0.0000 (nan)\ndataset1.reg.r2: 0.0000 (nan)\ndataset2.r2: 0.0000 (nan)\n&gt;&gt;&gt; meters = MultiTaskAverageMeters(return_average=True)\n&gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.a.auroc\": 0.7, \"dataset1.b.auroc\": 0.8, \"dataset2.auroc\": 0.9})\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.6000 (0.6000)\ndataset1.a.auroc: 0.7000 (0.7000)\ndataset1.b.auroc: 0.8000 (0.8000)\ndataset2.auroc: 0.9000 (0.9000)\n&gt;&gt;&gt; meters.update({\"loss\": 0.9, \"dataset1.a.auroc\": 0.8, \"dataset1.b.auroc\": 0.9, \"dataset2.auroc\": 1.0})\n&gt;&gt;&gt; print(f\"{meters:.4f}\")\nloss: 0.9000 (0.7500)\ndataset1.a.auroc: 0.8000 (0.7500)\ndataset1.b.auroc: 0.9000 (0.8500)\ndataset2.auroc: 1.0000 (0.9500)\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>class MultiTaskAverageMeters(MultiTaskDict):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; meters = MultiTaskAverageMeters()\n        &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.6000 (0.6000)\n        dataset1.cls.auroc: 0.7000 (0.7000)\n        dataset1.reg.r2: 0.8000 (0.8000)\n        dataset2.r2: 0.9000 (0.9000)\n        &gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.9000 (0.7500)\n        dataset1.cls.auroc: 0.7000 (0.7000)\n        dataset1.reg.r2: 0.8000 (0.8000)\n        dataset2.r2: 0.9000 (0.9000)\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 1.5, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 2, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n        &gt;&gt;&gt; meters.reset()\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.0000 (nan)\n        dataset1.cls.auroc: 0.0000 (nan)\n        dataset1.reg.r2: 0.0000 (nan)\n        dataset2.r2: 0.0000 (nan)\n        &gt;&gt;&gt; meters = MultiTaskAverageMeters(return_average=True)\n        &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.a.auroc\": 0.7, \"dataset1.b.auroc\": 0.8, \"dataset2.auroc\": 0.9})\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.6000 (0.6000)\n        dataset1.a.auroc: 0.7000 (0.7000)\n        dataset1.b.auroc: 0.8000 (0.8000)\n        dataset2.auroc: 0.9000 (0.9000)\n        &gt;&gt;&gt; meters.update({\"loss\": 0.9, \"dataset1.a.auroc\": 0.8, \"dataset1.b.auroc\": 0.9, \"dataset2.auroc\": 1.0})\n        &gt;&gt;&gt; print(f\"{meters:.4f}\")\n        loss: 0.9000 (0.7500)\n        dataset1.a.auroc: 0.8000 (0.7500)\n        dataset1.b.auroc: 0.9000 (0.8500)\n        dataset2.auroc: 1.0000 (0.9500)\n    \"\"\"\n\n    @property\n    def sum(self) -&gt; NestedDict[str, float]:\n        return NestedDict({key: meter.sum for key, meter in self.all_items()})\n\n    @property\n    def count(self) -&gt; NestedDict[str, int]:\n        return NestedDict({key: meter.count for key, meter in self.all_items()})\n\n    def update(self, values: Dict, *, n: int = 1) -&gt; None:  # pylint: disable=W0237\n        r\"\"\"\n        Updates the average and current value in all meters.\n\n        Args:\n            values: Dict of values to be added to the average.\n            n: Number of values to be added.\n\n        Raises:\n            ValueError: If the value is not an instance of (int, float, Mapping).\n\n        Examples:\n            &gt;&gt;&gt; meters = MultiTaskAverageMeters()\n            &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 0.6, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 1, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n            &gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 1.5, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 2, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n            &gt;&gt;&gt; meters.update({\"loss\": 0.8, \"dataset1.cls.auroc\": 0.9, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.7})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 2.3, 'dataset1': {'cls': {'auroc': 1.6}, 'reg': {'r2': 1.6}}, 'dataset2': {'r2': 1.6}}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 3, 'dataset1': {'cls': {'auroc': 2}, 'reg': {'r2': 2}}, 'dataset2': {'r2': 2}}\n            &gt;&gt;&gt; meters.update({\"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.7, \"dataset2.r2\": 0.9})\n            &gt;&gt;&gt; meters.sum.dict()\n            {'loss': 2.3, 'dataset1': {'cls': {'auroc': 2.3}, 'reg': {'r2': 2.3}}, 'dataset2': {'r2': 2.5}}\n            &gt;&gt;&gt; meters.count.dict()\n            {'loss': 3, 'dataset1': {'cls': {'auroc': 3}, 'reg': {'r2': 3}}, 'dataset2': {'r2': 3}}\n            &gt;&gt;&gt; meters.update({\"dataset1\": {\"cls.auroc\": 0.9}, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n            Traceback (most recent call last):\n            ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'dict'&gt;\n            This is likely due to nested dictionary in the values.\n            Nested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\n            &gt;&gt;&gt; meters.update(dict(loss=\"\"))\n            Traceback (most recent call last):\n            ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'str'&gt;\n        \"\"\"  # noqa: E501\n\n        for meter, value in values.items():\n            if isinstance(value, (int, float)):\n                self[meter].update(value, n)\n            elif isinstance(value, Dict):\n                value.setdefault(\"n\", n)\n                try:\n                    self[meter].update(**value)\n                except TypeError:\n                    raise ValueError(\n                        f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\\n\"\n                        \"This is likely due to nested dictionary in the values.\\n\"\n                        \"Nested dictionaries cannot be processed due to the method's design, which uses Mapping \"\n                        \"to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\"\n                    ) from None\n            else:\n                raise ValueError(f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\")\n\n    # evil hack, as the default_factory must not be set to make `NestedDict` happy\n    # this have some side effects, it will break attribute style intermediate nested dict auto creation\n    # but everything has a price\n    def get(self, name: Any, default=None) -&gt; Any:\n        if not name.startswith(\"_\") and not name.endswith(\"_\"):\n            return self.setdefault(name, AverageMeter())\n        return super().get(name, default)\n</code></pre>"},{"location":"metrics/average_meter/#danling.metrics.average_meter.MultiTaskAverageMeters.update","title":"<code>update(values, *, n=1)</code>","text":"<p>Updates the average and current value in all meters.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict</code> <p>Dict of values to be added to the average.</p> required <code>n</code> <code>int</code> <p>Number of values to be added.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is not an instance of (int, float, Mapping).</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; meters = MultiTaskAverageMeters()\n&gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 0.6, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 1, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n&gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 1.5, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 2, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n&gt;&gt;&gt; meters.update({\"loss\": 0.8, \"dataset1.cls.auroc\": 0.9, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.7})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 2.3, 'dataset1': {'cls': {'auroc': 1.6}, 'reg': {'r2': 1.6}}, 'dataset2': {'r2': 1.6}}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 3, 'dataset1': {'cls': {'auroc': 2}, 'reg': {'r2': 2}}, 'dataset2': {'r2': 2}}\n&gt;&gt;&gt; meters.update({\"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.7, \"dataset2.r2\": 0.9})\n&gt;&gt;&gt; meters.sum.dict()\n{'loss': 2.3, 'dataset1': {'cls': {'auroc': 2.3}, 'reg': {'r2': 2.3}}, 'dataset2': {'r2': 2.5}}\n&gt;&gt;&gt; meters.count.dict()\n{'loss': 3, 'dataset1': {'cls': {'auroc': 3}, 'reg': {'r2': 3}}, 'dataset2': {'r2': 3}}\n&gt;&gt;&gt; meters.update({\"dataset1\": {\"cls.auroc\": 0.9}, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\nTraceback (most recent call last):\nValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'dict'&gt;\nThis is likely due to nested dictionary in the values.\nNested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\n&gt;&gt;&gt; meters.update(dict(loss=\"\"))\nTraceback (most recent call last):\nValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'str'&gt;\n</code></pre> Source code in <code>danling/metrics/average_meter.py</code> Python<pre><code>def update(self, values: Dict, *, n: int = 1) -&gt; None:  # pylint: disable=W0237\n    r\"\"\"\n    Updates the average and current value in all meters.\n\n    Args:\n        values: Dict of values to be added to the average.\n        n: Number of values to be added.\n\n    Raises:\n        ValueError: If the value is not an instance of (int, float, Mapping).\n\n    Examples:\n        &gt;&gt;&gt; meters = MultiTaskAverageMeters()\n        &gt;&gt;&gt; meters.update({\"loss\": 0.6, \"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 0.6, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 1, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n        &gt;&gt;&gt; meters.update({\"loss\": {\"value\": 0.9, \"n\": 1}})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 1.5, 'dataset1': {'cls': {'auroc': 0.7}, 'reg': {'r2': 0.8}}, 'dataset2': {'r2': 0.9}}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 2, 'dataset1': {'cls': {'auroc': 1}, 'reg': {'r2': 1}}, 'dataset2': {'r2': 1}}\n        &gt;&gt;&gt; meters.update({\"loss\": 0.8, \"dataset1.cls.auroc\": 0.9, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.7})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 2.3, 'dataset1': {'cls': {'auroc': 1.6}, 'reg': {'r2': 1.6}}, 'dataset2': {'r2': 1.6}}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 3, 'dataset1': {'cls': {'auroc': 2}, 'reg': {'r2': 2}}, 'dataset2': {'r2': 2}}\n        &gt;&gt;&gt; meters.update({\"dataset1.cls.auroc\": 0.7, \"dataset1.reg.r2\": 0.7, \"dataset2.r2\": 0.9})\n        &gt;&gt;&gt; meters.sum.dict()\n        {'loss': 2.3, 'dataset1': {'cls': {'auroc': 2.3}, 'reg': {'r2': 2.3}}, 'dataset2': {'r2': 2.5}}\n        &gt;&gt;&gt; meters.count.dict()\n        {'loss': 3, 'dataset1': {'cls': {'auroc': 3}, 'reg': {'r2': 3}}, 'dataset2': {'r2': 3}}\n        &gt;&gt;&gt; meters.update({\"dataset1\": {\"cls.auroc\": 0.9}, \"dataset1.reg.r2\": 0.8, \"dataset2.r2\": 0.9})\n        Traceback (most recent call last):\n        ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'dict'&gt;\n        This is likely due to nested dictionary in the values.\n        Nested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\n        &gt;&gt;&gt; meters.update(dict(loss=\"\"))\n        Traceback (most recent call last):\n        ValueError: Expected values to be int, float, or a flat dictionary, but got &lt;class 'str'&gt;\n    \"\"\"  # noqa: E501\n\n    for meter, value in values.items():\n        if isinstance(value, (int, float)):\n            self[meter].update(value, n)\n        elif isinstance(value, Dict):\n            value.setdefault(\"n\", n)\n            try:\n                self[meter].update(**value)\n            except TypeError:\n                raise ValueError(\n                    f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\\n\"\n                    \"This is likely due to nested dictionary in the values.\\n\"\n                    \"Nested dictionaries cannot be processed due to the method's design, which uses Mapping \"\n                    \"to pass both value and count ('n'). Ensure your input is a flat dictionary or a single value.\"\n                ) from None\n        else:\n            raise ValueError(f\"Expected values to be int, float, or a flat dictionary, but got {type(value)}\")\n</code></pre>"},{"location":"metrics/metrics/","title":"Metrics","text":""},{"location":"metrics/metrics/#danling.metrics.metrics.Metrics","title":"<code>Metrics</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Metric class wraps around multiple metrics that share the same states.</p> <p>Typically, there are many metrics that we want to compute for a single task. For example, we usually needs to compute <code>pearson</code> and <code>spearman</code> for a regression task. Unlike <code>accuracy</code>, which can uses an average meter to compute the average accuracy, <code>pearson</code> and <code>spearman</code> cannot be computed by averaging the results of multiple batches. They need access to all the data to compute the correct results. And saving all intermediate results for each tasks is quite inefficient.</p> <p><code>Metrics</code> solves this problem by maintaining a shared state for multiple metric functions.</p> <p>Attributes:</p> Name Type Description <code>metrics</code> <code>FlatDict[str, Callable]</code> <p>A dictionary of metrics to be computed.</p> <code>val</code> <code>FlatDict[str, float]</code> <p>Metric results of current batch on current device.</p> <code>bat</code> <code>FlatDict[str, float]</code> <p>Metric results of current batch on all devices.</p> <code>avg</code> <code>FlatDict[str, float]</code> <p>Metric results of all results on all devices.</p> <code>input</code> <p>The input tensor of latest batch.</p> <code>target</code> <p>The target tensor of latest batch.</p> <code>inputs</code> <p>All input tensors.</p> <code>targets</code> <p>All target tensors.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>A single mapping of metrics.</p> <code>()</code> <code>**metrics</code> <code>FlatDict[str, Callable]</code> <p>Metrics.</p> <code>{}</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from danling.metrics.functional import auroc, auprc\n&gt;&gt;&gt; metrics = Metrics(auroc=auroc, auprc=auprc)\n&gt;&gt;&gt; metrics\nMetrics('auroc', 'auprc')\n&gt;&gt;&gt; metrics.update([0.2, 0.3, 0.5, 0.7], [0, 1, 0, 1])\n&gt;&gt;&gt; metrics.input  # predicted values of current batch\ntensor([0.2000, 0.3000, 0.5000, 0.7000])\n&gt;&gt;&gt; metrics.target  # ground truth of current batch\ntensor([0, 1, 0, 1])\n&gt;&gt;&gt; metrics.inputs  # predicted values of all data\ntensor([0.2000, 0.3000, 0.5000, 0.7000])\n&gt;&gt;&gt; metrics.targets  # ground truth of all data\ntensor([0, 1, 0, 1])\n&gt;&gt;&gt; metrics.val  # Metrics of current batch on current device\nFlatDict(\n  ('auroc'): 0.75\n  ('auprc'): 0.8333333730697632\n)\n&gt;&gt;&gt; metrics.bat  # Metrics of current batch on all devices\nFlatDict(\n  ('auroc'): 0.75\n  ('auprc'): 0.8333333730697632\n)\n&gt;&gt;&gt; metrics.avg  # Metrics of all data on all devices\nFlatDict(\n  ('auroc'): 0.75\n  ('auprc'): 0.8333333730697632\n)\n&gt;&gt;&gt; metrics.update([0.1, 0.4, 0.6, 0.8], [0, 0, 1, 0])\n&gt;&gt;&gt; metrics.input  # predicted values of current batch\ntensor([0.1000, 0.4000, 0.6000, 0.8000])\n&gt;&gt;&gt; metrics.target  # ground truth of current batch\ntensor([0, 0, 1, 0])\n&gt;&gt;&gt; metrics.inputs  # predicted values of all data\ntensor([0.2000, 0.3000, 0.5000, 0.7000, 0.1000, 0.4000, 0.6000, 0.8000])\n&gt;&gt;&gt; metrics.targets  # ground truth of all data\ntensor([0, 1, 0, 1, 0, 0, 1, 0])\n&gt;&gt;&gt; metrics.val  # Metrics of current batch on current device\nFlatDict(\n  ('auroc'): 0.6666666666666666\n  ('auprc'): 0.5\n)\n&gt;&gt;&gt; metrics.bat  # Metrics of current batch on all devices\nFlatDict(\n  ('auroc'): 0.6666666666666666\n  ('auprc'): 0.5\n)\n&gt;&gt;&gt; metrics.avg  # Metrics of all data on all devices\nFlatDict(\n  ('auroc'): 0.6666666666666666\n  ('auprc'): 0.5555555820465088\n)\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'auroc: 0.6667 (0.6667)\\tauprc: 0.5000 (0.5556)'\n</code></pre> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>class Metrics(Metric):\n    r\"\"\"\n    Metric class wraps around multiple metrics that share the same states.\n\n    Typically, there are many metrics that we want to compute for a single task.\n    For example, we usually needs to compute `pearson` and `spearman` for a regression task.\n    Unlike `accuracy`, which can uses an average meter to compute the average accuracy,\n    `pearson` and `spearman` cannot be computed by averaging the results of multiple batches.\n    They need access to all the data to compute the correct results.\n    And saving all intermediate results for each tasks is quite inefficient.\n\n    `Metrics` solves this problem by maintaining a shared state for multiple metric functions.\n\n    Attributes:\n        metrics: A dictionary of metrics to be computed.\n        val: Metric results of current batch on current device.\n        bat: Metric results of current batch on all devices.\n        avg: Metric results of all results on all devices.\n        input: The input tensor of latest batch.\n        target: The target tensor of latest batch.\n        inputs: All input tensors.\n        targets: All target tensors.\n\n    Args:\n        *args: A single mapping of metrics.\n        **metrics: Metrics.\n\n    Examples:\n        &gt;&gt;&gt; from danling.metrics.functional import auroc, auprc\n        &gt;&gt;&gt; metrics = Metrics(auroc=auroc, auprc=auprc)\n        &gt;&gt;&gt; metrics\n        Metrics('auroc', 'auprc')\n        &gt;&gt;&gt; metrics.update([0.2, 0.3, 0.5, 0.7], [0, 1, 0, 1])\n        &gt;&gt;&gt; metrics.input  # predicted values of current batch\n        tensor([0.2000, 0.3000, 0.5000, 0.7000])\n        &gt;&gt;&gt; metrics.target  # ground truth of current batch\n        tensor([0, 1, 0, 1])\n        &gt;&gt;&gt; metrics.inputs  # predicted values of all data\n        tensor([0.2000, 0.3000, 0.5000, 0.7000])\n        &gt;&gt;&gt; metrics.targets  # ground truth of all data\n        tensor([0, 1, 0, 1])\n        &gt;&gt;&gt; metrics.val  # Metrics of current batch on current device\n        FlatDict(\n          ('auroc'): 0.75\n          ('auprc'): 0.8333333730697632\n        )\n        &gt;&gt;&gt; metrics.bat  # Metrics of current batch on all devices\n        FlatDict(\n          ('auroc'): 0.75\n          ('auprc'): 0.8333333730697632\n        )\n        &gt;&gt;&gt; metrics.avg  # Metrics of all data on all devices\n        FlatDict(\n          ('auroc'): 0.75\n          ('auprc'): 0.8333333730697632\n        )\n        &gt;&gt;&gt; metrics.update([0.1, 0.4, 0.6, 0.8], [0, 0, 1, 0])\n        &gt;&gt;&gt; metrics.input  # predicted values of current batch\n        tensor([0.1000, 0.4000, 0.6000, 0.8000])\n        &gt;&gt;&gt; metrics.target  # ground truth of current batch\n        tensor([0, 0, 1, 0])\n        &gt;&gt;&gt; metrics.inputs  # predicted values of all data\n        tensor([0.2000, 0.3000, 0.5000, 0.7000, 0.1000, 0.4000, 0.6000, 0.8000])\n        &gt;&gt;&gt; metrics.targets  # ground truth of all data\n        tensor([0, 1, 0, 1, 0, 0, 1, 0])\n        &gt;&gt;&gt; metrics.val  # Metrics of current batch on current device\n        FlatDict(\n          ('auroc'): 0.6666666666666666\n          ('auprc'): 0.5\n        )\n        &gt;&gt;&gt; metrics.bat  # Metrics of current batch on all devices\n        FlatDict(\n          ('auroc'): 0.6666666666666666\n          ('auprc'): 0.5\n        )\n        &gt;&gt;&gt; metrics.avg  # Metrics of all data on all devices\n        FlatDict(\n          ('auroc'): 0.6666666666666666\n          ('auprc'): 0.5555555820465088\n        )\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'auroc: 0.6667 (0.6667)\\tauprc: 0.5000 (0.5556)'\n    \"\"\"\n\n    metrics: FlatDict[str, Callable]\n    _input: Tensor\n    _target: Tensor\n    _inputs: flist\n    _targets: flist\n    _input_buffer: flist\n    _target_buffer: flist\n    score_name: str\n    best_fn: Callable\n    merge_dict: bool = True\n    return_nested: bool = False\n\n    def __init__(\n        self,\n        *args,\n        merge_dict: bool | None = None,\n        return_nested: bool | None = None,\n        device: torch.device | None = None,\n        **metrics: FlatDict[str, Callable],\n    ):\n        super().__init__(device=device)\n        self._add_state(\"_input\", torch.empty(0))\n        self._add_state(\"_target\", torch.empty(0))\n        self._add_state(\"_inputs\", flist())\n        self._add_state(\"_targets\", flist())\n        self._add_state(\"_input_buffer\", flist())\n        self._add_state(\"_target_buffer\", flist())\n        self.metrics = FlatDict(*args, **metrics)\n        if merge_dict is not None:\n            self.merge_dict = merge_dict\n        if return_nested is not None:\n            self.return_nested = return_nested\n\n    @torch.inference_mode()\n    def update(self, input: Any, target: Any) -&gt; None:  # pylint: disable=W0221\n        if isinstance(input, NestedTensor):\n            self._input = input\n            self._input_buffer.extend(input.cpu().storage())\n        else:\n            if not isinstance(input, torch.Tensor):\n                input = torch.tensor(input)\n            self._input = input\n            self._input_buffer.append(input.cpu())\n        if isinstance(target, NestedTensor):\n            self._target = target\n            self._target_buffer.extend(target.cpu().storage())\n        else:\n            if not isinstance(target, torch.Tensor):\n                target = torch.tensor(target)\n            self._target = target\n            self._target_buffer.append(target.cpu())\n\n    def compute(self) -&gt; FlatDict[str, float]:\n        return self.calculate(self.inputs.to(self.device), self.targets.to(self.device))\n\n    def value(self) -&gt; FlatDict[str, float]:\n        return self.calculate(self._input, self._target)\n\n    def batch(self) -&gt; FlatDict[str, float]:\n        return self.calculate(self.input, self.target)\n\n    def average(self) -&gt; FlatDict[str, float]:\n        return self.calculate(self.inputs.to(self.device), self.targets.to(self.device))\n\n    @property\n    def val(self) -&gt; FlatDict[str, float]:\n        return self.value()\n\n    @property\n    def bat(self) -&gt; FlatDict[str, float]:\n        return self.batch()\n\n    @property\n    def avg(self) -&gt; FlatDict[str, float]:\n        return self.average()\n\n    @torch.inference_mode()\n    def calculate(self, input: Tensor, target: Tensor) -&gt; flist | float:\n        if (\n            isinstance(input, (Tensor, NestedTensor))\n            and input.numel() == 0 == target.numel()\n            or isinstance(input, (list, dict))\n            and len(input) == 0 == len(target)\n        ):\n            return FlatDict({name: nan for name in self.metrics.keys()})\n        ret = FlatDict()\n        for name, metric in self.metrics.items():\n            score = self._calculate(metric, input, target)\n            if isinstance(score, Mapping):\n                if self.merge_dict:\n                    ret.merge(score)\n                else:\n                    for n, s in score.items():\n                        ret[f\"{name}.{n}\"] = s\n            else:\n                ret[name] = score\n        return ret\n\n    @torch.inference_mode()\n    def _calculate(self, metric, input: Tensor, target: Tensor) -&gt; flist | float:\n        score = metric(input, target)\n        if isinstance(score, Tensor):\n            return score.item() if score.numel() == 1 else flist(score.tolist())\n        return score\n\n    @torch.inference_mode()\n    def merge_state(self, metrics: Iterable):\n        raise NotImplementedError()\n\n    # Due to an issue with PyTorch, we cannot decorate input/target with @torch.inference_mode()\n    # Otherwise, we will encounter the following error when using \"gloo\" backend:\n    # Inplace update to inference tensor outside InferenceMode is not allowed\n    @property\n    def input(self):\n        world_size = get_world_size()\n        if world_size == 1:\n            if isinstance(self._input, NestedTensor) and not self.return_nested:\n                return torch.cat(self._input.storage(), 0)\n            return self._input\n        if isinstance(self._input, Tensor):\n            synced_tensor = [torch.zeros_like(self._input) for _ in range(world_size)]\n            dist.all_gather(synced_tensor, self._input)\n            return torch.cat(synced_tensor, 0)\n        if isinstance(self._input, NestedTensor):\n            synced_tensors = [None for _ in range(world_size)]\n            dist.all_gather_object(synced_tensors, self._input.storage())\n            synced_tensors = flist(i.to(self.device) for j in synced_tensors for i in j)\n            try:\n                return torch.cat(synced_tensors, 0)\n            except RuntimeError:\n                if self.return_nested:\n                    return NestedTensor(synced_tensor)\n                return synced_tensors\n        raise ValueError(f\"Expected _input to be a Tensor or a NestedTensor, but got {type(self._input)}\")\n\n    @property\n    def target(self):\n        world_size = get_world_size()\n        if world_size == 1:\n            if isinstance(self._target, NestedTensor) and not self.return_nested:\n                return torch.cat(self._target.storage(), 0)\n            return self._target\n        if isinstance(self._target, Tensor):\n            synced_tensor = [torch.zeros_like(self._target) for _ in range(world_size)]\n            dist.all_gather(synced_tensor, self._target)\n            return torch.cat(synced_tensor, 0)\n        if isinstance(self._target, NestedTensor):\n            synced_tensors = [None for _ in range(world_size)]\n            dist.all_gather_object(synced_tensors, self._target.storage())\n            synced_tensors = flist(i.to(self.device) for j in synced_tensors for i in j)\n            try:\n                return torch.cat(synced_tensors, 0)\n            except RuntimeError:\n                if self.return_nested:\n                    return NestedTensor(synced_tensor)\n                return synced_tensors\n        raise ValueError(f\"Expected _target to be a Tensor or a NestedTensor, but got {type(self._target)}\")\n\n    @property\n    def inputs(self):\n        if not self._inputs and not self._input_buffer:\n            return torch.empty(0)\n        if self._input_buffer:\n            world_size = get_world_size()\n            if world_size &gt; 1:\n                synced_tensors = [None for _ in range(world_size)]\n                dist.all_gather_object(synced_tensors, self._input_buffer)\n                self._inputs.extend([i for j in synced_tensors for i in j])\n            else:\n                self._inputs.extend(self._input_buffer)\n            self._input_buffer = flist()\n        try:\n            return torch.cat(self._inputs, 0)\n        except RuntimeError:\n            if self.return_nested:\n                return NestedTensor(self._inputs)\n            return self._inputs\n\n    @property\n    def targets(self):\n        if not self._targets and not self._target_buffer:\n            return torch.empty(0)\n        if self._target_buffer:\n            world_size = get_world_size()\n            if world_size &gt; 1:\n                synced_tensors = [None for _ in range(world_size)]\n                dist.all_gather_object(synced_tensors, self._target_buffer)\n                self._targets.extend([i for j in synced_tensors for i in j])\n            else:\n                self._targets.extend(self._target_buffer)\n            self._target_buffer = flist()\n        try:\n            return torch.cat(self._targets, 0)\n        except RuntimeError:\n            if self.return_nested:\n                return NestedTensor(self._inputs)\n            return self._targets\n\n    def __repr__(self):\n        keys = tuple(i for i in self.metrics.keys())\n        return f\"{self.__class__.__name__}{keys}\"\n\n    def __format__(self, format_spec):\n        val, avg = self.value(), self.average()\n        return \"\\t\".join(\n            [f\"{key}: {val[key].__format__(format_spec)} ({avg[key].__format__(format_spec)})\" for key in self.metrics]\n        )\n\n    def reset(self: Self) -&gt; Self:  # pragma: no cover\n        r\"\"\"\n        Reset the metric state variables to their default value.\n        The tensors in the default values are also moved to the device of\n        the last ``self.to(device)`` call.\n        \"\"\"\n        for state_name, default in self._state_name_to_default.items():\n            if isinstance(default, torch.Tensor):\n                setattr(self, state_name, default.clone().to(self.device))\n            elif isinstance(default, list):\n                setattr(\n                    self,\n                    state_name,\n                    flist(tensor.clone().to(self.device) for tensor in default),\n                )\n            elif isinstance(default, dict):\n                setattr(\n                    self,\n                    state_name,\n                    DefaultDict(\n                        lambda: torch.tensor(0.0, device=self.device),\n                        {key: tensor.clone().to(self.device) for key, tensor in default.items()},\n                    ),\n                )\n            elif isinstance(default, (int, float)):\n                setattr(self, state_name, default)\n            else:\n                raise TypeError(\n                    f\"Invalid type for default value for {state_name}. Received {type(default)},\"\n                    \"but expected ``torch.Tensor``, a list of ``torch.Tensor``,\"\n                    \"a dictionary with ``torch.Tensor``, int, or float.\"\n                )\n        return self\n</code></pre>"},{"location":"metrics/metrics/#danling.metrics.metrics.Metrics.reset","title":"<code>reset()</code>","text":"<p>Reset the metric state variables to their default value. The tensors in the default values are also moved to the device of the last <code>self.to(device)</code> call.</p> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>def reset(self: Self) -&gt; Self:  # pragma: no cover\n    r\"\"\"\n    Reset the metric state variables to their default value.\n    The tensors in the default values are also moved to the device of\n    the last ``self.to(device)`` call.\n    \"\"\"\n    for state_name, default in self._state_name_to_default.items():\n        if isinstance(default, torch.Tensor):\n            setattr(self, state_name, default.clone().to(self.device))\n        elif isinstance(default, list):\n            setattr(\n                self,\n                state_name,\n                flist(tensor.clone().to(self.device) for tensor in default),\n            )\n        elif isinstance(default, dict):\n            setattr(\n                self,\n                state_name,\n                DefaultDict(\n                    lambda: torch.tensor(0.0, device=self.device),\n                    {key: tensor.clone().to(self.device) for key, tensor in default.items()},\n                ),\n            )\n        elif isinstance(default, (int, float)):\n            setattr(self, state_name, default)\n        else:\n            raise TypeError(\n                f\"Invalid type for default value for {state_name}. Received {type(default)},\"\n                \"but expected ``torch.Tensor``, a list of ``torch.Tensor``,\"\n                \"a dictionary with ``torch.Tensor``, int, or float.\"\n            )\n    return self\n</code></pre>"},{"location":"metrics/metrics/#danling.metrics.metrics.MultiTaskMetrics","title":"<code>MultiTaskMetrics</code>","text":"<p>               Bases: <code>MultiTaskDict</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from danling.metrics.functional import auroc, auprc, pearson, spearman, accuracy, matthews_corrcoef\n&gt;&gt;&gt; metrics = MultiTaskMetrics()\n&gt;&gt;&gt; metrics.dataset1.cls = Metrics(auroc=auroc, auprc=auprc)\n&gt;&gt;&gt; metrics.dataset1.reg = Metrics(pearson=pearson, spearman=spearman)\n&gt;&gt;&gt; metrics.dataset2 = Metrics(auroc=auroc, auprc=auprc)\n&gt;&gt;&gt; metrics\nMultiTaskMetrics(&lt;class 'danling.metrics.metrics.MultiTaskMetrics'&gt;,\n  ('dataset1'): MultiTaskMetrics(&lt;class 'danling.metrics.metrics.MultiTaskMetrics'&gt;,\n    ('cls'): Metrics('auroc', 'auprc')\n    ('reg'): Metrics('pearson', 'spearman')\n  )\n  ('dataset2'): Metrics('auroc', 'auprc')\n)\n&gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.2, 0.4, 0.5, 0.7], \"target\": [0, 1, 0, 1]}, \"dataset1.reg\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0.2, 0.3, 0.5, 0.7]}, \"dataset2\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 1, 0, 1]}})\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n&gt;&gt;&gt; metrics.setattr(\"return_average\", True)\n&gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 0, 1, 0]}, \"dataset1.reg\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0.2, 0.4, 0.6, 0.8]}, \"dataset2\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0, 0, 1, 0]}})\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9898 (0.9146)\\tspearman: 1.0000 (0.9222)\\ndataset2: auroc: 0.6667 (0.7333)\\tauprc: 0.5000 (0.7000)'\n</code></pre> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>class MultiTaskMetrics(MultiTaskDict):\n    r\"\"\"\n    Examples:\n        &gt;&gt;&gt; from danling.metrics.functional import auroc, auprc, pearson, spearman, accuracy, matthews_corrcoef\n        &gt;&gt;&gt; metrics = MultiTaskMetrics()\n        &gt;&gt;&gt; metrics.dataset1.cls = Metrics(auroc=auroc, auprc=auprc)\n        &gt;&gt;&gt; metrics.dataset1.reg = Metrics(pearson=pearson, spearman=spearman)\n        &gt;&gt;&gt; metrics.dataset2 = Metrics(auroc=auroc, auprc=auprc)\n        &gt;&gt;&gt; metrics\n        MultiTaskMetrics(&lt;class 'danling.metrics.metrics.MultiTaskMetrics'&gt;,\n          ('dataset1'): MultiTaskMetrics(&lt;class 'danling.metrics.metrics.MultiTaskMetrics'&gt;,\n            ('cls'): Metrics('auroc', 'auprc')\n            ('reg'): Metrics('pearson', 'spearman')\n          )\n          ('dataset2'): Metrics('auroc', 'auprc')\n        )\n        &gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.2, 0.4, 0.5, 0.7], \"target\": [0, 1, 0, 1]}, \"dataset1.reg\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0.2, 0.3, 0.5, 0.7]}, \"dataset2\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 1, 0, 1]}})\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n        &gt;&gt;&gt; metrics.setattr(\"return_average\", True)\n        &gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 0, 1, 0]}, \"dataset1.reg\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0.2, 0.4, 0.6, 0.8]}, \"dataset2\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0, 0, 1, 0]}})\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9898 (0.9146)\\tspearman: 1.0000 (0.9222)\\ndataset2: auroc: 0.6667 (0.7333)\\tauprc: 0.5000 (0.7000)'\n    \"\"\"  # noqa: E501\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, default_factory=MultiTaskMetrics, **kwargs)\n\n    def update(self, values: Mapping[str, Mapping[str, Tensor]]) -&gt; None:  # pylint: disable=W0237\n        r\"\"\"\n        Updates the average and current value in all metrics.\n\n        Args:\n            values: Dict of values to be added to the average.\n\n        Raises:\n            ValueError: If the value is not an instance of (Mapping).\n\n        Examples:\n            &gt;&gt;&gt; from danling.metrics.functional import auroc, auprc, pearson, spearman\n            &gt;&gt;&gt; metrics = MultiTaskMetrics()\n            &gt;&gt;&gt; metrics.dataset1.cls = Metrics(auroc=auroc, auprc=auprc)\n            &gt;&gt;&gt; metrics.dataset1.reg = Metrics(pearson=pearson, spearman=spearman)\n            &gt;&gt;&gt; metrics.dataset2 = Metrics(auroc=auroc, auprc=auprc)\n            &gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.2, 0.4, 0.5, 0.7], \"target\": [0, 1, 0, 1]}, \"dataset1.reg\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0.2, 0.3, 0.5, 0.7]}})\n            &gt;&gt;&gt; f\"{metrics:.4f}\"\n            'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: nan (nan)\\tauprc: nan (nan)'\n            &gt;&gt;&gt; metrics.update({\"dataset2\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 1, 0, 1]}})\n            &gt;&gt;&gt; f\"{metrics:.4f}\"\n            'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n            &gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 0, 1, 0]}})\n            &gt;&gt;&gt; f\"{metrics:.4f}\"\n            'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n            &gt;&gt;&gt; metrics.update({\"dataset1.reg\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0.2, 0.4, 0.6, 0.8]}})\n            &gt;&gt;&gt; f\"{metrics:.4f}\"\n            'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9898 (0.9146)\\tspearman: 1.0000 (0.9222)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n            &gt;&gt;&gt; metrics.update({\"dataset1\": {\"cls\": {\"input\": [0.1, 0.4, 0.6, 0.8]}}})\n            Traceback (most recent call last):\n            ValueError: Expected values to be a flat dictionary, but got &lt;class 'dict'&gt;\n            This is likely due to nested dictionary in the values.\n            Nested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both input and target. Ensure your input is a flat dictionary or a single value.\n            &gt;&gt;&gt; metrics.update(dict(loss=\"\"))\n            Traceback (most recent call last):\n            ValueError: Expected values to be a flat dictionary, but got &lt;class 'str'&gt;\n        \"\"\"  # noqa: E501\n\n        for metric, value in values.items():\n            if isinstance(value, Mapping):\n                if metric not in self:\n                    raise ValueError(f\"Metric {metric} not found in {self}\")\n                try:\n                    self[metric].update(**value)\n                except TypeError:\n                    raise ValueError(\n                        f\"Expected values to be a flat dictionary, but got {type(value)}\\n\"\n                        \"This is likely due to nested dictionary in the values.\\n\"\n                        \"Nested dictionaries cannot be processed due to the method's design, which uses Mapping \"\n                        \"to pass both input and target. Ensure your input is a flat dictionary or a single value.\"\n                    ) from None\n            else:\n                raise ValueError(f\"Expected values to be a flat dictionary, but got {type(value)}\")\n</code></pre>"},{"location":"metrics/metrics/#danling.metrics.metrics.MultiTaskMetrics.update","title":"<code>update(values)</code>","text":"<p>Updates the average and current value in all metrics.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Mapping[str, Mapping[str, Tensor]]</code> <p>Dict of values to be added to the average.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is not an instance of (Mapping).</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from danling.metrics.functional import auroc, auprc, pearson, spearman\n&gt;&gt;&gt; metrics = MultiTaskMetrics()\n&gt;&gt;&gt; metrics.dataset1.cls = Metrics(auroc=auroc, auprc=auprc)\n&gt;&gt;&gt; metrics.dataset1.reg = Metrics(pearson=pearson, spearman=spearman)\n&gt;&gt;&gt; metrics.dataset2 = Metrics(auroc=auroc, auprc=auprc)\n&gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.2, 0.4, 0.5, 0.7], \"target\": [0, 1, 0, 1]}, \"dataset1.reg\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0.2, 0.3, 0.5, 0.7]}})\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: nan (nan)\\tauprc: nan (nan)'\n&gt;&gt;&gt; metrics.update({\"dataset2\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 1, 0, 1]}})\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n&gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 0, 1, 0]}})\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n&gt;&gt;&gt; metrics.update({\"dataset1.reg\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0.2, 0.4, 0.6, 0.8]}})\n&gt;&gt;&gt; f\"{metrics:.4f}\"\n'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9898 (0.9146)\\tspearman: 1.0000 (0.9222)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n&gt;&gt;&gt; metrics.update({\"dataset1\": {\"cls\": {\"input\": [0.1, 0.4, 0.6, 0.8]}}})\nTraceback (most recent call last):\nValueError: Expected values to be a flat dictionary, but got &lt;class 'dict'&gt;\nThis is likely due to nested dictionary in the values.\nNested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both input and target. Ensure your input is a flat dictionary or a single value.\n&gt;&gt;&gt; metrics.update(dict(loss=\"\"))\nTraceback (most recent call last):\nValueError: Expected values to be a flat dictionary, but got &lt;class 'str'&gt;\n</code></pre> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>def update(self, values: Mapping[str, Mapping[str, Tensor]]) -&gt; None:  # pylint: disable=W0237\n    r\"\"\"\n    Updates the average and current value in all metrics.\n\n    Args:\n        values: Dict of values to be added to the average.\n\n    Raises:\n        ValueError: If the value is not an instance of (Mapping).\n\n    Examples:\n        &gt;&gt;&gt; from danling.metrics.functional import auroc, auprc, pearson, spearman\n        &gt;&gt;&gt; metrics = MultiTaskMetrics()\n        &gt;&gt;&gt; metrics.dataset1.cls = Metrics(auroc=auroc, auprc=auprc)\n        &gt;&gt;&gt; metrics.dataset1.reg = Metrics(pearson=pearson, spearman=spearman)\n        &gt;&gt;&gt; metrics.dataset2 = Metrics(auroc=auroc, auprc=auprc)\n        &gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.2, 0.4, 0.5, 0.7], \"target\": [0, 1, 0, 1]}, \"dataset1.reg\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0.2, 0.3, 0.5, 0.7]}})\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: nan (nan)\\tauprc: nan (nan)'\n        &gt;&gt;&gt; metrics.update({\"dataset2\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 1, 0, 1]}})\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'dataset1.cls: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n        &gt;&gt;&gt; metrics.update({\"dataset1.cls\": {\"input\": [0.1, 0.4, 0.6, 0.8], \"target\": [0, 0, 1, 0]}})\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9691 (0.9691)\\tspearman: 1.0000 (1.0000)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n        &gt;&gt;&gt; metrics.update({\"dataset1.reg\": {\"input\": [0.2, 0.3, 0.5, 0.7], \"target\": [0.2, 0.4, 0.6, 0.8]}})\n        &gt;&gt;&gt; f\"{metrics:.4f}\"\n        'dataset1.cls: auroc: 0.6667 (0.7000)\\tauprc: 0.5000 (0.5556)\\ndataset1.reg: pearson: 0.9898 (0.9146)\\tspearman: 1.0000 (0.9222)\\ndataset2: auroc: 0.7500 (0.7500)\\tauprc: 0.8333 (0.8333)'\n        &gt;&gt;&gt; metrics.update({\"dataset1\": {\"cls\": {\"input\": [0.1, 0.4, 0.6, 0.8]}}})\n        Traceback (most recent call last):\n        ValueError: Expected values to be a flat dictionary, but got &lt;class 'dict'&gt;\n        This is likely due to nested dictionary in the values.\n        Nested dictionaries cannot be processed due to the method's design, which uses Mapping to pass both input and target. Ensure your input is a flat dictionary or a single value.\n        &gt;&gt;&gt; metrics.update(dict(loss=\"\"))\n        Traceback (most recent call last):\n        ValueError: Expected values to be a flat dictionary, but got &lt;class 'str'&gt;\n    \"\"\"  # noqa: E501\n\n    for metric, value in values.items():\n        if isinstance(value, Mapping):\n            if metric not in self:\n                raise ValueError(f\"Metric {metric} not found in {self}\")\n            try:\n                self[metric].update(**value)\n            except TypeError:\n                raise ValueError(\n                    f\"Expected values to be a flat dictionary, but got {type(value)}\\n\"\n                    \"This is likely due to nested dictionary in the values.\\n\"\n                    \"Nested dictionaries cannot be processed due to the method's design, which uses Mapping \"\n                    \"to pass both input and target. Ensure your input is a flat dictionary or a single value.\"\n                ) from None\n        else:\n            raise ValueError(f\"Expected values to be a flat dictionary, but got {type(value)}\")\n</code></pre>"},{"location":"metrics/metrics/#danling.metrics.metrics.ScoreMetrics","title":"<code>ScoreMetrics</code>","text":"<p>               Bases: <code>Metrics</code></p> <p><code>ScoreMetrics</code> is a subclass of Metrics that supports scoring.</p> <p>Score is a single value that best represents the performance of the model. It is the core metrics that we use to compare different models. For example, in classification, we usually use auroc as the score.</p> <p><code>ScoreMetrics</code> requires two additional arguments: <code>score_name</code> and <code>best_fn</code>. <code>score_name</code> is the name of the metric that we use to compute the score. <code>best_fn</code> is a function that takes a list of values and returns the best value. <code>best_fn</code> is only not used by <code>ScoreMetrics</code>, it is meant to be accessed by other classes.</p> <p>Attributes:</p> Name Type Description <code>score_name</code> <code>str</code> <p>The name of the metric that we use to compute the score.</p> <code>best_fn</code> <code>Callable</code> <p>A function that takes a list of values and returns the best value.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>A single mapping of metrics.</p> <code>()</code> <code>score_name</code> <code>str | None</code> <p>The name of the metric that we use to compute the score. Defaults to the first metric.</p> <code>None</code> <code>best_fn</code> <code>Callable | None</code> <p>A function that takes a list of values and returns the best value. Defaults to <code>max</code>.</p> <code>max</code> <code>**metrics</code> <code>FlatDict[str, Callable]</code> <p>Metrics.</p> <code>{}</code> Source code in <code>danling/metrics/metrics.py</code> Python<pre><code>class ScoreMetrics(Metrics):  # pylint: disable=abstract-method\n    r\"\"\"\n    `ScoreMetrics` is a subclass of Metrics that supports scoring.\n\n    Score is a single value that best represents the performance of the model.\n    It is the core metrics that we use to compare different models.\n    For example, in classification, we usually use auroc as the score.\n\n    `ScoreMetrics` requires two additional arguments: `score_name` and `best_fn`.\n    `score_name` is the name of the metric that we use to compute the score.\n    `best_fn` is a function that takes a list of values and returns the best value.\n    `best_fn` is only not used by `ScoreMetrics`, it is meant to be accessed by other classes.\n\n    Attributes:\n        score_name: The name of the metric that we use to compute the score.\n        best_fn: A function that takes a list of values and returns the best value.\n\n    Args:\n        *args: A single mapping of metrics.\n        score_name: The name of the metric that we use to compute the score. Defaults to the first metric.\n        best_fn: A function that takes a list of values and returns the best value. Defaults to `max`.\n        **metrics: Metrics.\n    \"\"\"\n\n    _score_name: str\n    best_fn: Callable\n\n    def __init__(\n        self, *args, score_name: str | None = None, best_fn: Callable | None = max, **metrics: FlatDict[str, Callable]\n    ):\n        super().__init__(*args, **metrics)\n        self.score_name = score_name or next(iter(self.metrics.keys()))\n        self.metric = self.metrics[self.score_name]\n        self.best_fn = best_fn or max\n\n    def get_score(self, scope: str) -&gt; float | flist:\n        if scope == \"batch\":\n            return self.batch_score\n        if scope == \"average\":\n            return self.average_score\n        raise ValueError(f\"Unknown scope: {scope}\")\n\n    @property\n    def batch_score(self) -&gt; float | flist:\n        return self._calculate(self.metric, self.input, self.target)\n\n    @property\n    def average_score(self) -&gt; float | flist:\n        return self._calculate(self.metric, self.inputs, self.targets)\n\n    @property\n    def score_name(self) -&gt; str:\n        return self._score_name\n\n    @score_name.setter\n    def score_name(self, name) -&gt; None:\n        if name not in self.metrics:\n            raise ValueError(f\"score_name must be in {self.metrics.keys()}, but got {name}\")\n        self._score_name = name\n</code></pre>"},{"location":"optim/lr_scheduler/","title":"LRScheduler","text":"<p>               Bases: <code>_LRScheduler</code></p> <p>General learning rate scheduler.</p> <p>PyTorch LRScheduler is hard to extend. This class is a wrapper of PyTorch LRScheduler, which provides a more general interface. You only needs to add a new method which calculates a learning rate ratio (range from 0 to 1) with total progress (range from 0 to 1), and everything else will be done automatically.</p> <p>Moreover, this class has warmup and cooldown built-in. By default, the first 5% and last 20% of training steps will be warmup and cooldown respectively. You can alternate by passing <code>warmup_steps</code> and <code>cooldown_steps</code>, or disable them by setting them to 0.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>Wrapped optimizer.</p> required <code>total_steps</code> <code>int</code> <p>Total number of steps.</p> required <code>final_lr_ratio</code> <code>Optional[float]</code> <p>Final learning rate ratio to initial learning rate. Defaults to 1e-3.</p> <code>None</code> <code>final_lr</code> <code>Optional[float]</code> <p>Final learning rate.</p> <code>None</code> <code>min_lr</code> <code>float</code> <p>Minimal learning rate. Defaults to 1e-9.</p> <code>1e-09</code> <code>strategy</code> <code>str</code> <p>Scaling strategy. Defaults to \u201ccosine\u201d.</p> <code>'cosine'</code> <code>warmup_steps</code> <code>Optional[int]</code> <p>Number of warmup steps. Defaults to <code>steps // 20</code>.</p> <code>None</code> <code>cooldown_steps</code> <code>Optional[int]</code> <p>Number of cooldown steps. Defaults to <code>steps // 5</code>.</p> <code>None</code> <code>last_epoch</code> <code>int</code> <p>The index of last epoch. Defaults to -1.</p> <code>-1</code> <code>method</code> <code>str</code> <p>Method to calculate learning rate given ratio, should be one of \u201cpercentile\u201d or \u201cnumerical\u201d. Defaults to \u201cpercentile\u201d.</p> <code>'percentile'</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from danling.optim import LRScheduler\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import optim\n&gt;&gt;&gt; optimizer = optim.SGD([{'params': torch.tensor([0])}], lr=1, momentum=0.9)\n&gt;&gt;&gt; scheduler = LRScheduler(optimizer, total_steps=5, final_lr_ratio=1e-5, strategy='linear')\n&gt;&gt;&gt; lrs = []\n&gt;&gt;&gt; for epoch in range(5):\n...     lrs.append(scheduler.get_lr()[0])\n...     scheduler.step()\n&gt;&gt;&gt; [round(lr, 10) for lr in lrs]\n[0.1, 0.01, 0.001, 0.0001, 1e-09]\n&gt;&gt;&gt; scheduler = LRScheduler(optimizer, total_steps=5, final_lr_ratio=1e-5, strategy='cosine')\n&gt;&gt;&gt; lrs = []\n&gt;&gt;&gt; for epoch in range(5):\n...     lrs.append(scheduler.get_lr()[0])\n...     scheduler.step()\n&gt;&gt;&gt; [round(lr, 10) for lr in lrs]\n[0.3330753446, 0.0187302031, 0.000533897, 3.00232e-05, 1e-09]\n</code></pre> Source code in <code>danling/optim/lr_scheduler/lr_scheduler.py</code> Python<pre><code>class LRScheduler(lr_scheduler._LRScheduler):  # pylint: disable=protected-access\n    r\"\"\"\n    General learning rate scheduler.\n\n    PyTorch LRScheduler is hard to extend.\n    This class is a wrapper of PyTorch LRScheduler, which provides a more general interface.\n    You only needs to add a new method which calculates a learning rate ratio (range from 0 to 1)\n    with total progress (range from 0 to 1), and everything else will be done automatically.\n\n    Moreover, this class has warmup and cooldown built-in.\n    By default, the first 5% and last 20% of training steps will be warmup and cooldown respectively.\n    You can alternate by passing `warmup_steps` and `cooldown_steps`, or disable them by setting them to 0.\n\n    Args:\n        optimizer: Wrapped optimizer.\n        total_steps: Total number of steps.\n        final_lr_ratio: Final learning rate ratio to initial learning rate.\n            Defaults to 1e-3.\n        final_lr: Final learning rate.\n        min_lr: Minimal learning rate.\n            Defaults to 1e-9.\n        strategy: Scaling strategy.\n            Defaults to \"cosine\".\n        warmup_steps: Number of warmup steps.\n            Defaults to `steps // 20`.\n        cooldown_steps: Number of cooldown steps.\n            Defaults to `steps // 5`.\n        last_epoch: The index of last epoch.\n            Defaults to -1.\n        method: Method to calculate learning rate given ratio, should be one of \"percentile\" or \"numerical\".\n            Defaults to \"percentile\".\n\n    Examples:\n        &gt;&gt;&gt; from danling.optim import LRScheduler\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from torch import optim\n        &gt;&gt;&gt; optimizer = optim.SGD([{'params': torch.tensor([0])}], lr=1, momentum=0.9)\n        &gt;&gt;&gt; scheduler = LRScheduler(optimizer, total_steps=5, final_lr_ratio=1e-5, strategy='linear')\n        &gt;&gt;&gt; lrs = []\n        &gt;&gt;&gt; for epoch in range(5):\n        ...     lrs.append(scheduler.get_lr()[0])\n        ...     scheduler.step()\n        &gt;&gt;&gt; [round(lr, 10) for lr in lrs]\n        [0.1, 0.01, 0.001, 0.0001, 1e-09]\n        &gt;&gt;&gt; scheduler = LRScheduler(optimizer, total_steps=5, final_lr_ratio=1e-5, strategy='cosine')\n        &gt;&gt;&gt; lrs = []\n        &gt;&gt;&gt; for epoch in range(5):\n        ...     lrs.append(scheduler.get_lr()[0])\n        ...     scheduler.step()\n        &gt;&gt;&gt; [round(lr, 10) for lr in lrs]\n        [0.3330753446, 0.0187302031, 0.000533897, 3.00232e-05, 1e-09]\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer: Optimizer,\n        total_steps: int,\n        final_lr_ratio: Optional[float] = None,\n        final_lr: Optional[float] = None,\n        min_lr: float = 1e-9,\n        strategy: str = \"cosine\",\n        warmup_steps: Optional[int] = None,\n        cooldown_steps: Optional[int] = None,\n        last_epoch: int = -1,\n        method: str = \"percentile\",\n    ):\n        if total_steps &lt;= 0:\n            raise ValueError(f\"Total steps must be positive, but got {total_steps}\")\n        if warmup_steps is None:\n            warmup_steps = total_steps // 20\n        elif warmup_steps &gt; total_steps:\n            raise ValueError(f\"Warmup steps must be less than total steps, but got {warmup_steps} &gt; {total_steps}\")\n        elif warmup_steps &lt; 0:\n            raise ValueError(f\"Warmup steps must be positive, but got {warmup_steps}\")\n        if cooldown_steps is None:\n            cooldown_steps = total_steps // 5\n        elif cooldown_steps &gt; total_steps:\n            raise ValueError(f\"Cooldown steps must be less than total steps, but got {cooldown_steps} &gt; {total_steps}\")\n        elif cooldown_steps &lt; 0:\n            raise ValueError(f\"Cooldown steps must be positive, but got {cooldown_steps}\")\n        if final_lr_ratio is not None and final_lr is not None:\n            raise ValueError(\"Only one of `final_lr_ratio` and `final_lr` should be set, but not both\")\n        if final_lr_ratio is not None and final_lr_ratio &lt; 0:\n            raise ValueError(f\"`final_lr_ratio` must be positive, but got {final_lr_ratio}\")\n        if final_lr is not None and final_lr &lt; 0:\n            raise ValueError(f\"`final_lr` must be positive, but got {final_lr}\")\n        if min_lr &lt; 0:\n            raise ValueError(f\"`min_lr` must be positive, but got {min_lr}\")\n        self.strategies = {\n            k: v for k, v in self.__class__.__dict__.items() if callable(v) and (not k.startswith(\"_\") or k in \"get_lr\")\n        }\n        if strategy not in self.strategies:\n            raise ValueError(f\"Scaling strategy must be one of {self.strategies.keys()}, but got {strategy}\")\n\n        if final_lr_ratio is None and final_lr is None:\n            final_lr_ratio = 1e-3\n        if final_lr is not None and min_lr &gt; final_lr:\n            min_lr = final_lr\n\n        self.final_lr_ratio = final_lr_ratio\n        self.final_lr = final_lr\n        self.total_steps = total_steps\n        self.min_lr = min_lr\n        self.strategy = strategy\n        self.method = method\n        self.warmup_steps = warmup_steps\n        self.cooldown_steps = cooldown_steps\n        self.cooldown_steps_begin = self.total_steps - self.cooldown_steps\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self) -&gt; List[float]:\n        step_count = self._step_count\n        if step_count &gt; self.total_steps + 1 or step_count &lt; 1:\n            warn(\n                f\"Step count {step_count} is out of range [1, {self.total_steps + 1}]\",\n                category=RuntimeWarning,\n                stacklevel=2,\n            )\n        return [self._get_lr(lr, step_count) for lr in self.base_lrs]\n\n    def _get_lr(\n        self,\n        lr: float,\n        step_count: Optional[int] = None,\n        progress: Optional[float] = None,\n        warmup_ratio: Optional[float] = None,\n        cooldown_ratio: Optional[float] = None,\n        method: Optional[str] = None,\n    ) -&gt; float:\n        method = method or self.method\n        step_count = step_count or self._step_count\n        progress = progress or min(max(step_count / self.total_steps, 0.0), 1.0)\n        final_lr = self.final_lr if self.final_lr is not None else lr * self.final_lr_ratio  # type: ignore[operator]\n        ratio = getattr(self, self.strategy)(progress)\n        if method == \"percentile\":\n            lr *= pow(final_lr / lr, ratio)\n        elif method == \"numerical\":\n            lr = (1 - ratio) * (lr - final_lr) + final_lr\n        else:\n            raise ValueError(f\"Method must be one of ['percentile', 'numerical'], but got {method}\")\n        if self.warmup_steps &gt; step_count &gt; 0:\n            warmup_ratio = warmup_ratio or step_count / self.warmup_steps\n            lr = warmup_ratio * (lr - self.min_lr) + self.min_lr\n        elif self.cooldown_steps &gt; 0 and step_count &gt; self.cooldown_steps_begin:\n            cooldown_ratio = cooldown_ratio or 1 - (step_count - self.cooldown_steps_begin) / self.cooldown_steps\n            lr = cooldown_ratio * (lr - self.min_lr) + self.min_lr\n        return max(self.min_lr, lr)\n\n    def linear(self, progress: float) -&gt; float:\n        return progress\n\n    def cosine(self, progress: float) -&gt; float:\n        return 1 - ((1 + cos(pi * progress)) / 2)\n\n    def constant(self, progress: float) -&gt; float:  # pylint: disable=unused-argument\n        return 0.0\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}({self.strategy}, method={self.method}, \"\n            f\"final_lr_ratio={self.final_lr_ratio}, total_steps={self.total_steps}, \"\n            f\"warmup_steps={self.warmup_steps}, cooldown_steps={self.cooldown_steps})\"\n        )\n</code></pre>"},{"location":"runner/","title":"Runner","text":"<p>The Runner of DanLing sets up the basic environment for running neural networks.</p>"},{"location":"runner/#components","title":"Components","text":"<p>For cross-platform compatibilities, DanLing features a two-level Runner + RunnerState system.</p>"},{"location":"runner/#platformrunner","title":"PlatformRunner","text":"<p>PlatformRunner implements platform-specific features like <code>step</code> and <code>prepare</code>.</p> <p>The Runner contains all runtime information that is irrelevant to the checkpoint (e.g. <code>world_size</code>, <code>rank</code>, etc.). All other information should be saved in <code>RunnerState</code>.</p> <p>Currently, only <code>AccelerateRunner</code> is supported.</p>"},{"location":"runner/#baserunner","title":"<code>BaseRunner</code>","text":"<p><code>BaseRunner</code> defines shared attributes and implements platform-agnostic features, including <code>init_logging</code>, <code>results</code> and <code>scores</code>.</p>"},{"location":"runner/#runnerstate","title":"<code>RunnerState</code>","text":"<p><code>RunnerState</code> stores the state of a run (e.g. <code>epochs</code>, <code>run_id</code>, <code>network</code>, etc.).</p> <p>With <code>RunnerState</code> and corresponding weights, you can resume a run from any point. Therefore, all members in <code>RunnerState</code> will be saved in the checkpoint, and thus should be json serialisable.</p>"},{"location":"runner/#experiments-management","title":"Experiments Management","text":"<p>DanLing Runner is designed for a 3.5-level experiments management system: Project, Group, Experiment, and, Run.</p>"},{"location":"runner/#project","title":"Project","text":"<p>A project corresponds to your project.</p> <p>Generally speaking, there should be only one project for each repository.</p> <p><code>project_root</code> is the root directory of all experiments of a certain project, and should be consistent across the project.</p>"},{"location":"runner/#group","title":"Group","text":"<p>A group groups multiple experiments with similar characteristics.</p> <p>For example, if you run multiple experiments on learning rate, you may want to group them into a group.</p> <p>Note that Group is a virtual level (which is why it only counts 0.5) and does not correspond to anything. There are no attributes/properties for groups.</p>"},{"location":"runner/#experiment","title":"Experiment","text":"<p>An experiment is the basic unit of experiments.</p> <p>Each experiment corresponds to a certain commit, which means the code should be consistent across the experiment.</p> <p>DanLing will automatically generate <code>experiment_id</code> and <code>experiment_uuid</code> based on git revision. They are unique for each commit.</p> <p>You may also set a catchy custom <code>experiment_name</code> to identify each experiment.</p>"},{"location":"runner/#run","title":"Run","text":"<p>A run is the basic unit of runnings.</p> <p>Run corresponds to a certain run of an experiment, each run may have different hyperparameters.</p> <p>DanLing will automatically generate <code>run_id</code> and <code>run_uuid</code> based on <code>experiment_uuid</code> and provided config. They are unique for each commit and config.</p> <p>You may also set a catchy custom <code>run_name</code> to identify each experiment.</p>"},{"location":"runner/#identifiers","title":"Identifiers","text":"<p>DanLing has two properties built-in to help you identify each run.</p> <ul> <li><code>id</code> by default is the join of <code>experiment_id</code>, <code>run_id</code>, and <code>uuid</code>. It is automatically generated hex-strings and is unique for each run.</li> <li><code>name</code> by default is <code>experiment_name-run_name</code>. It is manually specified and easy to read. Note that <code>name</code> is not guaranteed to be unique.</li> </ul>"},{"location":"runner/#directories","title":"Directories","text":"<p>To help you manage your experiments, DanLing will automatically generate directories for you.</p> <p><code>dir</code> is the directory of a certain run, defaults to <code>{dir/name-id}</code>. All run files should be under this directory.</p> <p>In particular, <code>checkpoint_dir</code>, which defaults to <code>dir/checkpoint_dir_name</code> contains all checkpoint files.</p> <p>As a result, your <code>project_root</code> should looks like following:</p> Bash<pre><code>- {project_root}\n-     |- {name}-{id} (equivalents to {experiment_name}-{run_name}-{experiment_id}-{run_id}-{uuid})\n-       |\n-       |- {checkpoint_dir_name}\n-       |    |\n-       |    |- best.pth\n-       |    |- latest.pth\n-       |    |- epoch-10.pth\n-       |\n-       |- run.log\n-       |- runner.yaml\n-       |- results.json\n-       |- latest.json\n-       |- best.json\n</code></pre>"},{"location":"runner/accelerate_runner/","title":"AccelerateRunner","text":"<p>               Bases: <code>BaseRunner</code></p> <p>Set up everything for running a job.</p> <p><code>AccelerateRunner</code> uses <code>accelerate</code> as distributed backend to provide seamless distributed training experience.</p> <p><code>AccelerateRunner</code> will automatically <code>prepare</code> everything, including <code>model</code>, <code>criterion</code>, <code>optimizer</code>, <code>scheduler</code>, and <code>dataloaders</code> for distribute training, mixed precision, and deepspeed (optional).</p> <p>In fact, you don\u2019t even need to create <code>dataloaders</code>, just define <code>datasets</code> and <code>AccelerateRunner</code> will create <code>dataloaders</code> for you. <code>AccelerateRunner</code> will inspect the <code>train</code> flag in corresponding dataset to automatically set <code>shuffle</code>.</p> <p>Attributes:</p> Name Type Description <code>accelerator</code> <code>Accelerator</code> <code>accelerate</code> <code>dict</code> <p>Arguments to pass when building accelerator. Defaults to <code>{}</code>.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>class AccelerateRunner(BaseRunner):  # pylint: disable=too-many-public-methods\n    r\"\"\"\n    Set up everything for running a job.\n\n    `AccelerateRunner` uses [`accelerate`][accelerate] as distributed backend to\n    provide seamless distributed training experience.\n\n    `AccelerateRunner` will automatically [`prepare`][accelerate.Accelerator.prepare] everything,\n    including `model`, `criterion`, `optimizer`, `scheduler`, and `dataloaders` for distribute training,\n    mixed precision, and deepspeed (optional).\n\n    In fact, you don't even need to create `dataloaders`, just define\n    `datasets` and `AccelerateRunner` will create `dataloaders` for you.\n    `AccelerateRunner` will inspect the `train` flag in corresponding dataset to\n    automatically set `shuffle`.\n\n    Attributes:\n        accelerator (Accelerator):\n        accelerate: Arguments to pass when building accelerator. Defaults to `{}`.\n    \"\"\"\n\n    accelerator: Accelerator\n    accelerate: dict\n\n    model: nn.Module\n    criterion: nn.Module\n    optimizer: optim.Optimizer\n    scheduler: optim.lr_scheduler._LRScheduler\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        if len(args) != 1 or kwargs:\n            message = (\n                \"Passing multiple args &amp; kwargs to build Runner is deprecated and will be removed in DanLing v0.3.\\n\"\n                \"Please only pass a config dict instead.\"\n            )\n            warn(message, DeprecationWarning, stacklevel=2)\n            config = NestedDict(*args, **kwargs)\n        else:\n            config = args[0]\n        if \"accelerate\" not in self:  # class attributes\n            self.accelerate = {}\n        self.accelerate.update(config.get(\"accelerate\", {}))\n        super().__init__(config)\n\n    def __post_init__(self) -&gt; None:\n        self.model, self.criterion, self.optimizer = self.prepare(self.model, self.criterion, self.optimizer)\n        self.scheduler = self.prepare(self.scheduler)\n        if self.datasets:\n            datasets = {k: d for k, d in self.datasets.items() if k not in self.dataloaders}\n            default_kwargs = self.state.setdefault(\"dataloader\", NestedDict())\n            dataloader_kwargs = NestedDict({k: default_kwargs.pop(k) for k in self.datasets if k in default_kwargs})\n            for k, d in datasets.items():\n                dataloader_kwargs.setdefault(k, NestedDict())\n                dataloader_kwargs[k].merge(default_kwargs, overwrite=False)\n                dataloader_kwargs[k].setdefault(\"shuffle\", getattr(d, \"train\", True))\n                dataloader_kwargs[k].setdefault(\"drop_last\", not getattr(d, \"train\", True))\n                self.dataloaders[k] = utils.data.DataLoader(d, **dataloader_kwargs[k])\n            default_kwargs.update(dataloader_kwargs)\n        for k, d in self.dataloaders.items():\n            self.dataloaders[k] = self.prepare(d)\n        if self.state.get(\"log_interval\") is None:\n            self.state.log_interval = max(len(d) for d in self.dataloaders.values()) // 10\n\n    @property\n    def deepspeed(self) -&gt; dict | None:\n        if \"accelerator\" not in self:\n            raise ValueError(\"accelerator is not used\")\n        if self.accelerator.state.deepspeed_plugin is not None:\n            return self.accelerator.state.deepspeed_plugin.deepspeed_config\n        return None\n\n    def train(self, train_splits: list[str] | None = None, eval_splits: list[str] | None = None) -&gt; NestedDict:\n        r\"\"\"\n        Perform training on `split`.\n\n        Args:\n            train_splits (list[str]): list of split to run train.\n                Defaults to `[\"train\"]`.\n            eval_splits (list[str]): list of split to run evaluate.\n                Defaults to `self.dataloaders` except for those in `train_splits`.\n\n        Return:\n            NestedDict: train results\n        \"\"\"\n\n        early_stop_counter = 0\n        if train_splits is None:\n            train_splits = [\"train\"]\n        if eval_splits is None:\n            eval_splits = [s for s in self.dataloaders if s not in train_splits]\n        self.state.epoch_begin = self.state.epochs\n        print(f\"Begin training from {self.state.epoch_begin} to {self.state.epoch_end}\")\n        print(f\"Training splits: {train_splits}\")\n        print(f\"Evaluation splits: {eval_splits}\")\n        patience = self.state.get(\"patience\", float(\"inf\"))\n        for epochs in range(self.state.epoch_begin, self.state.epoch_end):  # type: ignore\n            self.state.epochs = epochs\n            result = NestedDict()\n            result.setattr(\"convert_mapping\", True)\n            for split in train_splits:\n                result[split] = self.train_epoch(split)\n            for split in eval_splits:\n                result[split] = self.evaluate_epoch(split)\n            self.append_result(result)\n            print(self.format_epoch_result(result))\n            self.save_result()\n            if self.state.save_interval is not None:\n                self.save_checkpoint(epochs)\n            \"\"\"@nni.report_intermediate_result(self.latest_score)\"\"\"\n            early_stop_counter = 0 if self.is_best else early_stop_counter + 1\n            if early_stop_counter &gt; patience:\n                print(\"early stop\")\n                break\n        \"\"\"@nni.report_final_result(self.latest_score)\"\"\"\n        return self.results\n\n    def train_epoch(self, split: str = \"train\") -&gt; NestedDict:\n        r\"\"\"\n        Train one epoch on `split`.\n\n        Args:\n            split (str): split to run train\n\n        Return:\n            NestedDict: train result\n        \"\"\"\n\n        self.mode = \"train\"  # type: ignore\n        self.split = split\n        loader = self.dataloaders[split]\n        length = len(loader) - 1\n        last_print_iteration = -1\n        log_interval = self.state.get(\"log_interval\", -1)\n        self.meters.reset()\n        if self.metrics is not None:\n            self.metrics.reset()\n        batch_time = time()\n        if hasattr(loader.batch_sampler, \"set_epoch\"):\n            loader.batch_sampler.set_epoch(self.epochs)\n        if hasattr(loader.sampler, \"set_epoch\"):\n            loader.sampler.set_epoch(self.epochs)\n\n        for iteration, data in enumerate(loader):\n            with self.autocast(), self.accumulate():\n                input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n                target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n                pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n                loss = self.criterion(pred, target)\n                if self.metrics is not None:\n                    self.metrics.update(pred.squeeze(-1), target)\n                self.step(loss)\n\n            if log_interval &gt; 0 and (iteration &gt; 0 and iteration % log_interval == 0 or iteration == length):\n                interval = iteration - last_print_iteration\n                if self.device == torch.device(\"cuda\"):\n                    torch.cuda.synchronize()\n                self.meters.time.update((time() - batch_time) / interval)\n                batch_time = time()\n                reduced_loss = self.reduce(loss).item()\n                self.meters.loss.update(reduced_loss)\n                self.step_log(split, iteration, length)\n                last_print_iteration = iteration\n\n        result = self.meters.average()\n        if self.metrics is not None:\n            result.merge(self.metrics.average())\n        return result\n\n    def evaluate(self, eval_splits: list[str] | None = None) -&gt; NestedDict:\n        r\"\"\"\n        Perform evaluation on `eval_splits`.\n\n        Args:\n            eval_splits (list[str]): list of split to run evaluate.\n                Defaults to `[\"eval\"]`.\n\n        Return:\n            NestedDict: evaluation result\n        \"\"\"\n\n        if eval_splits is None:\n            eval_splits = [\"eval\"]\n\n        print(\"Begin evaluation\")\n        print(f\"Evaluation splits: {eval_splits}\")\n        result = NestedDict()\n        result.setattr(\"convert_mapping\", True)\n        for split in eval_splits:\n            result[split] = self.evaluate_epoch(split=split)\n        print(self.format_epoch_result(result))\n        return result\n\n    @torch.inference_mode()\n    def evaluate_epoch(self, split: str = \"val\") -&gt; NestedDict:\n        r\"\"\"\n        Evaluate one epoch on `split`.\n\n        Args:\n            split (str): split to run evaluate\n\n        Return:\n            NestedDict: evaluation result\n        \"\"\"\n\n        self.mode = \"eval\"  # type: ignore\n        self.split = split\n        loader = self.dataloaders[split]\n        length = len(loader) - 1\n        last_print_iteration = -1\n        log_interval = self.state.get(\"log_interval\", -1)\n        self.meters.reset()\n        if self.metrics is not None:\n            self.metrics.reset()\n        batch_time = time()\n\n        for iteration, data in enumerate(loader):\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            loss = self.criterion(pred, target)\n            if self.metrics is not None:\n                self.metrics.update(pred.squeeze(-1), target)\n\n            if log_interval &gt; 0 and (iteration &gt; 0 and iteration % log_interval == 0 or iteration == length):\n                interval = iteration - last_print_iteration\n                if self.device == torch.device(\"cuda\"):\n                    torch.cuda.synchronize()\n                self.meters.time.update((time() - batch_time) / interval)\n                batch_time = time()\n                reduced_loss = self.reduce(loss).item()\n                self.meters.loss.update(reduced_loss)\n                self.step_log(split, iteration, length)\n                last_print_iteration = iteration\n\n        result = self.meters.average()\n        if self.metrics is not None:\n            result.merge(self.metrics.average())\n        self.write_result(result, split, self.state.epochs)\n        return result\n\n    @torch.inference_mode()\n    def inference(self, split: str = \"inf\") -&gt; list:\n        r\"\"\"\n        Perform inference on `split`.\n\n        Args:\n            split (str): split to run inference\n\n        Return:\n            Tensor: inference outputs\n        \"\"\"\n\n        # pylint: disable=E1102, W0622\n        self.mode = \"inf\"  # type: ignore\n        loader = self.dataloaders[split]\n        self.meters.reset()\n        output = []\n        for _, data in tqdm(enumerate(loader), total=len(loader)):\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            output.extend(pred.squeeze(-1).tolist())\n\n        if self.distributed:\n            torch.cuda.synchronize()\n            output = self.gather_for_metrics(output)\n        return output\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Set up distributed training.\n\n        Initialise process group and set up DDP variables.\n        \"\"\"\n\n        if os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\").lower() == \"true\":\n            deepspeed_config = self.state.get(\"deepspeed\", os.environ.get(\"ACCELERATE_DEEPSPEED_CONFIG_FILE\"))\n            self.accelerate[\"deepspeed_plugin\"] = DeepSpeedPlugin(hf_ds_config=self.init_deepspeed(deepspeed_config))\n        self.accelerator = Accelerator(**self.accelerate)\n        if self.distributed:\n            object_list = [self.id, self.timestamp]\n            dist.broadcast_object_list(object_list)\n            self.id, self.timestamp = object_list\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n        if \"log_dir\" not in kwargs:\n            kwargs[\"log_dir\"] = self.dir\n\n        self.writer = SummaryWriter(*args, **kwargs)\n        self.writer.add_scalar = catch(OSError, verbose=False)(self.writer.add_scalar)\n\n    def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Args:\n            seed: Random seed to set.\n                Defaults to `self.state.seed` (`config.seed`).\n\n            bias: Make the seed different for each processes.\n                This is used to ensure the data augmentation are applied differently on every processes.\n                Defaults to `self.rank`.\n                Set to `False` to disable this feature.\n        \"\"\"\n\n        seed = seed or self.state.seed\n        if self.distributed:\n            object_list = [seed]\n            dist.broadcast_object_list(object_list)\n            seed = object_list[0]\n        bias = bias or self.rank\n        if bias:\n            seed += bias\n        self.state.seed = seed\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        if np_random is not None:\n            np_random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n        if torch.__version__ &gt;= \"1.8.0\":\n            torch.use_deterministic_algorithms(True)\n\n    def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n        r\"\"\"\n        Backward loss and step optimizer &amp; scheduler.\n\n        This method increment `self.state.steps`.\n\n        This method also increment `self.state.iters` when `batch_size` is specified.\n\n        Args:\n            zero_grad: Whether to zero the gradients.\n        \"\"\"\n\n        self.accelerator.backward(loss)\n        if self.sync_gradients:\n            if self.state.get(\"max_grad_value\") is not None:\n                self.clip_grad_value_(self.model.parameters(), self.state.get(\"max_grad_value\"))\n            if self.state.get(\"max_grad_norm\") is not None:\n                self.clip_grad_norm_(self.model.parameters(), self.state.get(\"max_grad_norm\"))\n        if self.optimizer is not None:\n            self.optimizer.step()\n            if zero_grad:\n                self.optimizer.zero_grad()\n        if self.scheduler is not None:\n            self.scheduler.step()\n        self.state.steps += 1\n        if batch_size is None:\n            batch_size = self.batch_size_equivalent\n        self.state.iters += batch_size\n        # TODO: Support `drop_last = False`\n        # self.state.iters += self.batch_size_equivalent\n\n    def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Return dict of all attributes for checkpoint.\n        \"\"\"\n\n        if self.model is None:\n            raise ValueError(\"Model must be defined when calling state_dict\")\n        model = self.accelerator.unwrap_model(self.model)\n        return cls(\n            runner=self.state.dict(),\n            model=model.state_dict(),\n            optimizer=self.optimizer.state_dict() if self.optimizer else None,\n            scheduler=self.scheduler.state_dict() if self.scheduler else None,\n        )\n\n    def prepare(self, *args: list[Any], device_placement: list[bool] | None = None) -&gt; list[Any]:\n        r\"\"\"\n        Prepare all objects passed in `args` for distributed training and mixed precision,\n        then return them in the same order.\n        \"\"\"\n\n        return self.accelerator.prepare(*args, device_placement=device_placement)\n\n    def accumulate(self, model: nn.Module | None = None):\n        r\"\"\"\n        Context manager that enables gradient accumulate.\n        \"\"\"\n\n        model = model or self.model\n        return self.accelerator.accumulate(model)\n\n    def autocast(self):\n        r\"\"\"\n        Context manager that enables auto-casting for the forward pass (and maybe backward pass).\n        \"\"\"\n\n        return self.accelerator.autocast()\n\n    def backward(self, loss) -&gt; None:\n        r\"\"\"\n        Backward loss to compute gradients.\n        \"\"\"\n\n        return self.accelerator.backward(loss)\n\n    def unwrap_model(self, model: nn.Module | None = None) -&gt; nn.Module:\n        r\"\"\"\n        Unwrap DDP model.\n\n        Args:\n            model (Optional[nn.Module]):\n                Defaults to `self.model`.\n        \"\"\"\n\n        if model is not None:\n            model = self.model\n        if self.accelerator is not None:\n            return self.accelerator.unwrap_model(model)\n        if self.distributed:\n            return model.module\n        return model\n\n    @property\n    def mode(self) -&gt; RunnerMode:\n        return self._mode\n\n    @mode.setter\n    def mode(self, mode: str | RunnerMode) -&gt; None:\n        if isinstance(mode, str):\n            mode = RunnerMode(mode)\n        self._mode = mode\n        if self.model is not None:\n            self.model.train(mode == RunnerMode.train)\n\n    @property\n    def batch_size(self) -&gt; int:\n        r\"\"\"\n        Batch size.\n\n        Notes:\n            If `train` is in `dataloaders`, then `batch_size` is the batch size of `train`.\n            Otherwise, `batch_size` is the batch size of the first dataloader.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        batch_size = self.state.get(\"dataloader.batch_size\")\n        if batch_size:\n            return batch_size\n        if self.dataloaders:\n            loader = self.dataloaders.get(\"train\", next(iter(self.dataloaders.values())))\n            if loader.batch_size:\n                return loader.batch_size\n            batch_sampler = loader.batch_sampler if loader.batch_sampler is not None else loader.sampler\n            return batch_sampler.batch_size\n        raise AttributeError(\"batch_size could not be inferred, since no dataloader found.\")\n\n    @property\n    def accum_steps(self) -&gt; int:\n        r\"\"\"\n        Gradient accumulation steps.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        return self.accelerator.gradient_accumulation_steps\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of runner.\n        \"\"\"\n\n        return self.accelerator.device\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of Processes.\n        \"\"\"\n\n        return self.accelerator.num_processes\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index in all processes.\n        \"\"\"\n\n        return self.accelerator.process_index\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index in local processes.\n        \"\"\"\n\n        return self.accelerator.local_process_index\n\n    def gather(self, tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Gather tensor.\n        \"\"\"\n\n        return self.accelerator.gather(tensor)\n\n    def reduce(self, tensor, reduction: str = \"sum\") -&gt; torch.Tensor:\n        r\"\"\"\n        Reduce tensor.\n        \"\"\"\n\n        return self.accelerator.reduce(tensor, reduction=reduction)\n\n    def __getattr__(self, name: str) -&gt; Any:\n        with suppress(AttributeError):\n            return super().__getattr__(name)\n        if \"accelerator\" in self.__dict__ and hasattr(self.accelerator, name):\n            return getattr(self.accelerator, name)\n        raise super().__getattribute__(name)\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.accum_steps","title":"<code>accum_steps: int</code>  <code>property</code>","text":"<p>Gradient accumulation steps.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.batch_size","title":"<code>batch_size: int</code>  <code>property</code>","text":"<p>Batch size.</p> Notes <p>If <code>train</code> is in <code>dataloaders</code>, then <code>batch_size</code> is the batch size of <code>train</code>. Otherwise, <code>batch_size</code> is the batch size of the first dataloader.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Device of runner.</p>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.local_rank","title":"<code>local_rank: int</code>  <code>property</code>","text":"<p>Process index in local processes.</p>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.rank","title":"<code>rank: int</code>  <code>property</code>","text":"<p>Process index in all processes.</p>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.world_size","title":"<code>world_size: int</code>  <code>property</code>","text":"<p>Number of Processes.</p>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.accumulate","title":"<code>accumulate(model=None)</code>","text":"<p>Context manager that enables gradient accumulate.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def accumulate(self, model: nn.Module | None = None):\n    r\"\"\"\n    Context manager that enables gradient accumulate.\n    \"\"\"\n\n    model = model or self.model\n    return self.accelerator.accumulate(model)\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.autocast","title":"<code>autocast()</code>","text":"<p>Context manager that enables auto-casting for the forward pass (and maybe backward pass).</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def autocast(self):\n    r\"\"\"\n    Context manager that enables auto-casting for the forward pass (and maybe backward pass).\n    \"\"\"\n\n    return self.accelerator.autocast()\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.backward","title":"<code>backward(loss)</code>","text":"<p>Backward loss to compute gradients.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def backward(self, loss) -&gt; None:\n    r\"\"\"\n    Backward loss to compute gradients.\n    \"\"\"\n\n    return self.accelerator.backward(loss)\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.evaluate","title":"<code>evaluate(eval_splits=None)</code>","text":"<p>Perform evaluation on <code>eval_splits</code>.</p> <p>Parameters:</p> Name Type Description Default <code>eval_splits</code> <code>list[str]</code> <p>list of split to run evaluate. Defaults to <code>[\"eval\"]</code>.</p> <code>None</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def evaluate(self, eval_splits: list[str] | None = None) -&gt; NestedDict:\n    r\"\"\"\n    Perform evaluation on `eval_splits`.\n\n    Args:\n        eval_splits (list[str]): list of split to run evaluate.\n            Defaults to `[\"eval\"]`.\n\n    Return:\n        NestedDict: evaluation result\n    \"\"\"\n\n    if eval_splits is None:\n        eval_splits = [\"eval\"]\n\n    print(\"Begin evaluation\")\n    print(f\"Evaluation splits: {eval_splits}\")\n    result = NestedDict()\n    result.setattr(\"convert_mapping\", True)\n    for split in eval_splits:\n        result[split] = self.evaluate_epoch(split=split)\n    print(self.format_epoch_result(result))\n    return result\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.evaluate_epoch","title":"<code>evaluate_epoch(split='val')</code>","text":"<p>Evaluate one epoch on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run evaluate</p> <code>'val'</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>@torch.inference_mode()\ndef evaluate_epoch(self, split: str = \"val\") -&gt; NestedDict:\n    r\"\"\"\n    Evaluate one epoch on `split`.\n\n    Args:\n        split (str): split to run evaluate\n\n    Return:\n        NestedDict: evaluation result\n    \"\"\"\n\n    self.mode = \"eval\"  # type: ignore\n    self.split = split\n    loader = self.dataloaders[split]\n    length = len(loader) - 1\n    last_print_iteration = -1\n    log_interval = self.state.get(\"log_interval\", -1)\n    self.meters.reset()\n    if self.metrics is not None:\n        self.metrics.reset()\n    batch_time = time()\n\n    for iteration, data in enumerate(loader):\n        input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n        target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n        pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n        loss = self.criterion(pred, target)\n        if self.metrics is not None:\n            self.metrics.update(pred.squeeze(-1), target)\n\n        if log_interval &gt; 0 and (iteration &gt; 0 and iteration % log_interval == 0 or iteration == length):\n            interval = iteration - last_print_iteration\n            if self.device == torch.device(\"cuda\"):\n                torch.cuda.synchronize()\n            self.meters.time.update((time() - batch_time) / interval)\n            batch_time = time()\n            reduced_loss = self.reduce(loss).item()\n            self.meters.loss.update(reduced_loss)\n            self.step_log(split, iteration, length)\n            last_print_iteration = iteration\n\n    result = self.meters.average()\n    if self.metrics is not None:\n        result.merge(self.metrics.average())\n    self.write_result(result, split, self.state.epochs)\n    return result\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.gather","title":"<code>gather(tensor)</code>","text":"<p>Gather tensor.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def gather(self, tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Gather tensor.\n    \"\"\"\n\n    return self.accelerator.gather(tensor)\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.inference","title":"<code>inference(split='inf')</code>","text":"<p>Perform inference on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run inference</p> <code>'inf'</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>@torch.inference_mode()\ndef inference(self, split: str = \"inf\") -&gt; list:\n    r\"\"\"\n    Perform inference on `split`.\n\n    Args:\n        split (str): split to run inference\n\n    Return:\n        Tensor: inference outputs\n    \"\"\"\n\n    # pylint: disable=E1102, W0622\n    self.mode = \"inf\"  # type: ignore\n    loader = self.dataloaders[split]\n    self.meters.reset()\n    output = []\n    for _, data in tqdm(enumerate(loader), total=len(loader)):\n        input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n        pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n        output.extend(pred.squeeze(-1).tolist())\n\n    if self.distributed:\n        torch.cuda.synchronize()\n        output = self.gather_for_metrics(output)\n    return output\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.init_distributed","title":"<code>init_distributed()</code>","text":"<p>Set up distributed training.</p> <p>Initialise process group and set up DDP variables.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def init_distributed(self) -&gt; None:\n    r\"\"\"\n    Set up distributed training.\n\n    Initialise process group and set up DDP variables.\n    \"\"\"\n\n    if os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\").lower() == \"true\":\n        deepspeed_config = self.state.get(\"deepspeed\", os.environ.get(\"ACCELERATE_DEEPSPEED_CONFIG_FILE\"))\n        self.accelerate[\"deepspeed_plugin\"] = DeepSpeedPlugin(hf_ds_config=self.init_deepspeed(deepspeed_config))\n    self.accelerator = Accelerator(**self.accelerate)\n    if self.distributed:\n        object_list = [self.id, self.timestamp]\n        dist.broadcast_object_list(object_list)\n        self.id, self.timestamp = object_list\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.init_tensorboard","title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    from torch.utils.tensorboard.writer import SummaryWriter  # pylint: disable=C0415\n\n    if \"log_dir\" not in kwargs:\n        kwargs[\"log_dir\"] = self.dir\n\n    self.writer = SummaryWriter(*args, **kwargs)\n    self.writer.add_scalar = catch(OSError, verbose=False)(self.writer.add_scalar)\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.prepare","title":"<code>prepare(*args, device_placement=None)</code>","text":"<p>Prepare all objects passed in <code>args</code> for distributed training and mixed precision, then return them in the same order.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def prepare(self, *args: list[Any], device_placement: list[bool] | None = None) -&gt; list[Any]:\n    r\"\"\"\n    Prepare all objects passed in `args` for distributed training and mixed precision,\n    then return them in the same order.\n    \"\"\"\n\n    return self.accelerator.prepare(*args, device_placement=device_placement)\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.reduce","title":"<code>reduce(tensor, reduction='sum')</code>","text":"<p>Reduce tensor.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def reduce(self, tensor, reduction: str = \"sum\") -&gt; torch.Tensor:\n    r\"\"\"\n    Reduce tensor.\n    \"\"\"\n\n    return self.accelerator.reduce(tensor, reduction=reduction)\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.set_deterministic","title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    if torch.__version__ &gt;= \"1.8.0\":\n        torch.use_deterministic_algorithms(True)\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.set_seed","title":"<code>set_seed(seed=None, bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Random seed to set. Defaults to <code>self.state.seed</code> (<code>config.seed</code>).</p> <code>None</code> <code>bias</code> <code>int | None</code> <p>Make the seed different for each processes. This is used to ensure the data augmentation are applied differently on every processes. Defaults to <code>self.rank</code>. Set to <code>False</code> to disable this feature.</p> <code>None</code> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Args:\n        seed: Random seed to set.\n            Defaults to `self.state.seed` (`config.seed`).\n\n        bias: Make the seed different for each processes.\n            This is used to ensure the data augmentation are applied differently on every processes.\n            Defaults to `self.rank`.\n            Set to `False` to disable this feature.\n    \"\"\"\n\n    seed = seed or self.state.seed\n    if self.distributed:\n        object_list = [seed]\n        dist.broadcast_object_list(object_list)\n        seed = object_list[0]\n    bias = bias or self.rank\n    if bias:\n        seed += bias\n    self.state.seed = seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    if np_random is not None:\n        np_random.seed(seed)\n    random.seed(seed)\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.state_dict","title":"<code>state_dict(cls=dict)</code>","text":"<p>Return dict of all attributes for checkpoint.</p> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Return dict of all attributes for checkpoint.\n    \"\"\"\n\n    if self.model is None:\n        raise ValueError(\"Model must be defined when calling state_dict\")\n    model = self.accelerator.unwrap_model(self.model)\n    return cls(\n        runner=self.state.dict(),\n        model=model.state_dict(),\n        optimizer=self.optimizer.state_dict() if self.optimizer else None,\n        scheduler=self.scheduler.state_dict() if self.scheduler else None,\n    )\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.step","title":"<code>step(loss, batch_size=None, zero_grad=True)</code>","text":"<p>Backward loss and step optimizer &amp; scheduler.</p> <p>This method increment <code>self.state.steps</code>.</p> <p>This method also increment <code>self.state.iters</code> when <code>batch_size</code> is specified.</p> <p>Parameters:</p> Name Type Description Default <code>zero_grad</code> <code>bool</code> <p>Whether to zero the gradients.</p> <code>True</code> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n    r\"\"\"\n    Backward loss and step optimizer &amp; scheduler.\n\n    This method increment `self.state.steps`.\n\n    This method also increment `self.state.iters` when `batch_size` is specified.\n\n    Args:\n        zero_grad: Whether to zero the gradients.\n    \"\"\"\n\n    self.accelerator.backward(loss)\n    if self.sync_gradients:\n        if self.state.get(\"max_grad_value\") is not None:\n            self.clip_grad_value_(self.model.parameters(), self.state.get(\"max_grad_value\"))\n        if self.state.get(\"max_grad_norm\") is not None:\n            self.clip_grad_norm_(self.model.parameters(), self.state.get(\"max_grad_norm\"))\n    if self.optimizer is not None:\n        self.optimizer.step()\n        if zero_grad:\n            self.optimizer.zero_grad()\n    if self.scheduler is not None:\n        self.scheduler.step()\n    self.state.steps += 1\n    if batch_size is None:\n        batch_size = self.batch_size_equivalent\n    self.state.iters += batch_size\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.train","title":"<code>train(train_splits=None, eval_splits=None)</code>","text":"<p>Perform training on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>train_splits</code> <code>list[str]</code> <p>list of split to run train. Defaults to <code>[\"train\"]</code>.</p> <code>None</code> <code>eval_splits</code> <code>list[str]</code> <p>list of split to run evaluate. Defaults to <code>self.dataloaders</code> except for those in <code>train_splits</code>.</p> <code>None</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def train(self, train_splits: list[str] | None = None, eval_splits: list[str] | None = None) -&gt; NestedDict:\n    r\"\"\"\n    Perform training on `split`.\n\n    Args:\n        train_splits (list[str]): list of split to run train.\n            Defaults to `[\"train\"]`.\n        eval_splits (list[str]): list of split to run evaluate.\n            Defaults to `self.dataloaders` except for those in `train_splits`.\n\n    Return:\n        NestedDict: train results\n    \"\"\"\n\n    early_stop_counter = 0\n    if train_splits is None:\n        train_splits = [\"train\"]\n    if eval_splits is None:\n        eval_splits = [s for s in self.dataloaders if s not in train_splits]\n    self.state.epoch_begin = self.state.epochs\n    print(f\"Begin training from {self.state.epoch_begin} to {self.state.epoch_end}\")\n    print(f\"Training splits: {train_splits}\")\n    print(f\"Evaluation splits: {eval_splits}\")\n    patience = self.state.get(\"patience\", float(\"inf\"))\n    for epochs in range(self.state.epoch_begin, self.state.epoch_end):  # type: ignore\n        self.state.epochs = epochs\n        result = NestedDict()\n        result.setattr(\"convert_mapping\", True)\n        for split in train_splits:\n            result[split] = self.train_epoch(split)\n        for split in eval_splits:\n            result[split] = self.evaluate_epoch(split)\n        self.append_result(result)\n        print(self.format_epoch_result(result))\n        self.save_result()\n        if self.state.save_interval is not None:\n            self.save_checkpoint(epochs)\n        \"\"\"@nni.report_intermediate_result(self.latest_score)\"\"\"\n        early_stop_counter = 0 if self.is_best else early_stop_counter + 1\n        if early_stop_counter &gt; patience:\n            print(\"early stop\")\n            break\n    \"\"\"@nni.report_final_result(self.latest_score)\"\"\"\n    return self.results\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.train_epoch","title":"<code>train_epoch(split='train')</code>","text":"<p>Train one epoch on <code>split</code>.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>split to run train</p> <code>'train'</code> Return Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def train_epoch(self, split: str = \"train\") -&gt; NestedDict:\n    r\"\"\"\n    Train one epoch on `split`.\n\n    Args:\n        split (str): split to run train\n\n    Return:\n        NestedDict: train result\n    \"\"\"\n\n    self.mode = \"train\"  # type: ignore\n    self.split = split\n    loader = self.dataloaders[split]\n    length = len(loader) - 1\n    last_print_iteration = -1\n    log_interval = self.state.get(\"log_interval\", -1)\n    self.meters.reset()\n    if self.metrics is not None:\n        self.metrics.reset()\n    batch_time = time()\n    if hasattr(loader.batch_sampler, \"set_epoch\"):\n        loader.batch_sampler.set_epoch(self.epochs)\n    if hasattr(loader.sampler, \"set_epoch\"):\n        loader.sampler.set_epoch(self.epochs)\n\n    for iteration, data in enumerate(loader):\n        with self.autocast(), self.accumulate():\n            input = data[\"input\"] if isinstance(data, Mapping) else data[0]\n            target = data[\"target\"] if isinstance(data, Mapping) else data[1]\n            pred = self.model(**input) if isinstance(input, Mapping) else self.model(input)\n            loss = self.criterion(pred, target)\n            if self.metrics is not None:\n                self.metrics.update(pred.squeeze(-1), target)\n            self.step(loss)\n\n        if log_interval &gt; 0 and (iteration &gt; 0 and iteration % log_interval == 0 or iteration == length):\n            interval = iteration - last_print_iteration\n            if self.device == torch.device(\"cuda\"):\n                torch.cuda.synchronize()\n            self.meters.time.update((time() - batch_time) / interval)\n            batch_time = time()\n            reduced_loss = self.reduce(loss).item()\n            self.meters.loss.update(reduced_loss)\n            self.step_log(split, iteration, length)\n            last_print_iteration = iteration\n\n    result = self.meters.average()\n    if self.metrics is not None:\n        result.merge(self.metrics.average())\n    return result\n</code></pre>"},{"location":"runner/accelerate_runner/#danling.runner.AccelerateRunner.unwrap_model","title":"<code>unwrap_model(model=None)</code>","text":"<p>Unwrap DDP model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[Module]</code> <p>Defaults to <code>self.model</code>.</p> <code>None</code> Source code in <code>danling/runner/accelerate_runner.py</code> Python<pre><code>def unwrap_model(self, model: nn.Module | None = None) -&gt; nn.Module:\n    r\"\"\"\n    Unwrap DDP model.\n\n    Args:\n        model (Optional[nn.Module]):\n            Defaults to `self.model`.\n    \"\"\"\n\n    if model is not None:\n        model = self.model\n    if self.accelerator is not None:\n        return self.accelerator.unwrap_model(model)\n    if self.distributed:\n        return model.module\n    return model\n</code></pre>"},{"location":"runner/base_runner/","title":"BaseRunner","text":"<p>Base class for all runners.</p> <p><code>BaseRunner</code> sets up basic running environment, including <code>seed</code>, <code>deterministic</code>, and <code>logging</code>.</p> <p><code>BaseRunner</code> also provides some basic methods, such as, <code>step</code>, <code>state_dict</code>, <code>save_checkpoint</code>, <code>load_checkpoint</code>.</p> <p><code>BaseRunner</code> defines all basic attributes and relevant properties such as <code>scores</code>, <code>progress</code>, etc.</p> <p>ID:</p> Name Type Description <code>timestamp</code> <code>str</code> <p>A time string representing the creation time of run.</p> <code>name</code> <code>str</code> <p><code>f\"{self.state.experiment_name}-{self.state.run_name}\"</code>.</p> <code>id</code> <code>str</code> <p><code>f\"{self.state.experiment_id:.8}{self.state.run_id:.8}\"</code>.</p> <code>uuid</code> <code>(UUID, property)</code> <p><code>uuid5(self.state.run_id, self.id)</code>.</p> <p>Core:</p> Name Type Description <code>mode</code> <code>(RunnerMode, property)</code> <p>Running mode.</p> <code>state</code> <code>RunnerState</code> <p>Running state. See <code>RunnerState</code> for details.</p> <p>Model:</p> Name Type Description <code>model</code> <code>Callable</code> <code>criterion</code> <code>Callable</code> <code>optimizer</code> <code>Any | None</code> <code>scheduler</code> <code>Any | None</code> <p>Data:</p> Name Type Description <code>datasets</code> <code>FlatDict</code> <p>All datasets, should be in the form of <code>{subset: dataset}</code>. Initialised to <code>FlatDict</code> by default.</p> <code>datasamplers</code> <code>FlatDict</code> <p>All datasamplers, should be in the form of <code>{subset: datasampler}</code>. Initialised to <code>FlatDict</code> by default.</p> <code>dataloaders</code> <code>FlatDict</code> <p>All dataloaders, should be in the form of <code>{subset: dataloader}</code>. Initialised to <code>FlatDict</code> by default.</p> <code>split</code> <code>str</code> <p>Current running split.</p> <code>batch_size</code> <code>(int, property)</code> <p>Number of samples per batch in current running split.</p> <code>batch_size_equivalent</code> <code>(int, property)</code> <p>Total batch_size (<code>batch_size * world_size * accum_steps</code>).</p> <p><code>datasets</code>, <code>datasamplers</code>, <code>dataloaders</code> should be a dict with the same keys. Their keys should be <code>split</code> (e.g. <code>train</code>, <code>val</code>, <code>test</code>).</p> <p>Progress:</p> Name Type Description <code>progress</code> <code>(float, property)</code> <p>Running Progress, in <code>range(0, 1)</code>.</p> <p>Results:</p> Name Type Description <code>results</code> <code>NestedDict</code> <p>Results include all metric information of the model. Results should be in the form of <code>{epoch: {subset: {metric: score}}}</code>.</p> <code>latest_result</code> <code>(NestedDict, property)</code> <p>Most recent result, should be in the form of <code>{subset: {metric: score}}</code>.</p> <code>best_result</code> <code>(NestedDict, property)</code> <p>Best result, should be in the form of <code>{subset: {metric: score}}</code>.</p> <code>scores</code> <code>(List[float], property)</code> <p>Score is the core metric that is used to evaluate the performance of the model. Scores should be in the form of <code>{epoch: score}</code>.</p> <code>latest_score</code> <code>(float, property)</code> <p>Most recent score, should be in the form of <code>score</code>.</p> <code>best_score</code> <code>(float, property)</code> <p>Best score, should be in the form of <code>score</code>.</p> <code>score_split</code> <code>Optional[str]</code> <p>The subset to calculate the score. If is <code>None</code>, will use the last set of the result.</p> <code>score_name</code> <code>str</code> <p>The metric name of score. Defaults to <code>\"loss\"</code>.</p> <code>is_best</code> <code>(bool, property)</code> <p>If <code>latest_score == best_score</code>.</p> <p>A <code>result</code> is a dict with the same <code>split</code> as keys, like <code>dataloaders</code>. A typical <code>result</code> shall look like this: Python<pre><code>{\n    \"train\": {\n        \"loss\": 0.1,\n        \"accuracy\": 0.9,\n    },\n    \"val\": {\n        \"loss\": 0.2,\n        \"accuracy\": 0.8,\n    },\n    \"test\": {\n        \"loss\": 0.3,\n        \"accuracy\": 0.7,\n    },\n}\n</code></pre></p> <p><code>scores</code> are dynamically extracted from <code>results</code> by <code>score_split</code> and <code>score_name</code>. They represent the core metric that is used in comparing the performance against different models and settings. For the above <code>results</code>, If <code>score_split = \"val\"</code>, <code>score_name = \"accuracy\"</code>, then <code>scores = 0.9</code>.</p> <p>IO:</p> Name Type Description <code>dir</code> <code>(str, property)</code> <p>Directory of the run. Defaults to <code>${self.project_root}/${self.name}-${self.id}/${self.timestamp})</code>.</p> <code>checkpoint_dir</code> <code>(str, property)</code> <p>Directory of checkpoints.</p> <code>log_path</code> <code>(str, property)</code> <p>Path of log file.</p> <code>checkpoint_dir_name</code> <code>str</code> <p>The name of the directory under <code>runner.dir</code> to save checkpoints. Defaults to <code>\"checkpoints\"</code>.</p> <p>Parallel Training:</p> Name Type Description <code>world_size</code> <code>(int, property)</code> <p>Number of processes.</p> <code>rank</code> <code>(int, property)</code> <p>Process index of all processes.</p> <code>local_rank</code> <code>(int, property)</code> <p>Process index of local processes.</p> <code>distributed</code> <code>(bool, property)</code> <p>If runner is running in distributed mode.</p> <code>is_main_process</code> <code>(bool, property)</code> <p>If current process is the main process of all processes.</p> <code>is_local_main_process</code> <code>(bool, property)</code> <p>If current process is the main process of local processes.</p> <p>logging:</p> Name Type Description <code>meters</code> <code>AverageMeters | MultiTaskAverageMeters</code> <p>Average meters. Initialised to <code>AverageMeters</code> by default.</p> <code>metrics</code> <code>Metrics | MultiTaskMetrics | None</code> <p>Metrics for evaluating.</p> <code>logger</code> <code>Logger | None</code> <code>writer</code> <code>Any | None</code> See Also <p><code>RunnerState</code>: The runeer base that stores runtime information. <code>BaseRunner</code>: The base runner class.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>class BaseRunner(metaclass=RunnerMeta):  # pylint: disable=too-many-public-methods\n    r\"\"\"\n    Base class for all runners.\n\n    `BaseRunner` sets up basic running environment, including `seed`, `deterministic`, and `logging`.\n\n    `BaseRunner` also provides some basic methods, such as, `step`, `state_dict`, `save_checkpoint`, `load_checkpoint`.\n\n    `BaseRunner` defines all basic attributes and relevant properties such as `scores`, `progress`, etc.\n\n    Attributes: ID:\n        timestamp (str): A time string representing the creation time of run.\n        name (str): `f\"{self.state.experiment_name}-{self.state.run_name}\"`.\n        id (str): `f\"{self.state.experiment_id:.8}{self.state.run_id:.8}\"`.\n        uuid (UUID, property): `uuid5(self.state.run_id, self.id)`.\n\n    Attributes: Core:\n        mode (RunnerMode, property): Running mode.\n        state (RunnerState): Running state. See `RunnerState` for details.\n\n    Attributes: Model:\n        model (Callable):\n        criterion (Callable):\n        optimizer:\n        scheduler:\n\n    Attributes: Data:\n        datasets (FlatDict): All datasets, should be in the form of ``{subset: dataset}``.\n            Initialised to `FlatDict` by default.\n        datasamplers (FlatDict): All datasamplers, should be in the form of ``{subset: datasampler}``.\n            Initialised to `FlatDict` by default.\n        dataloaders (FlatDict): All dataloaders, should be in the form of ``{subset: dataloader}``.\n            Initialised to `FlatDict` by default.\n        split (str): Current running split.\n        batch_size (int, property): Number of samples per batch in current running split.\n        batch_size_equivalent (int, property): Total batch_size (`batch_size * world_size * accum_steps`).\n\n    `datasets`, `datasamplers`, `dataloaders` should be a dict with the same keys.\n    Their keys should be `split` (e.g. `train`, `val`, `test`).\n\n    Attributes: Progress:\n        progress (float, property): Running Progress, in `range(0, 1)`.\n\n    Attributes: Results:\n        results (NestedDict): Results include all metric information of the model.\n            Results should be in the form of `{epoch: {subset: {metric: score}}}`.\n        latest_result (NestedDict, property): Most recent result, should be in the form of `{subset: {metric: score}}`.\n        best_result (NestedDict, property): Best result, should be in the form of `{subset: {metric: score}}`.\n        scores (List[float], property): Score is the core metric that is used to evaluate the performance of the model.\n            Scores should be in the form of `{epoch: score}`.\n        latest_score (float, property): Most recent score, should be in the form of `score`.\n        best_score (float, property): Best score, should be in the form of `score`.\n        score_split (Optional[str]): The subset to calculate the score.\n            If is `None`, will use the last set of the result.\n        score_name (str): The metric name of score.\n            Defaults to `\"loss\"`.\n        is_best (bool, property): If `latest_score == best_score`.\n\n    A `result` is a dict with the same `split` as keys, like `dataloaders`.\n    A typical `result` shall look like this:\n    ```python\n    {\n        \"train\": {\n            \"loss\": 0.1,\n            \"accuracy\": 0.9,\n        },\n        \"val\": {\n            \"loss\": 0.2,\n            \"accuracy\": 0.8,\n        },\n        \"test\": {\n            \"loss\": 0.3,\n            \"accuracy\": 0.7,\n        },\n    }\n    ```\n\n    `scores` are dynamically extracted from `results` by `score_split` and `score_name`.\n    They represent the core metric that is used in comparing the performance against different models and settings.\n    For the above `results`, If `score_split = \"val\"`, `score_name = \"accuracy\"`, then `scores = 0.9`.\n\n    Attributes: IO:\n        dir (str, property): Directory of the run.\n            Defaults to `${self.project_root}/${self.name}-${self.id}/${self.timestamp})`.\n        checkpoint_dir (str, property): Directory of checkpoints.\n        log_path (str, property):  Path of log file.\n        checkpoint_dir_name (str): The name of the directory under `runner.dir` to save checkpoints.\n            Defaults to `\"checkpoints\"`.\n\n    Attributes: Parallel Training:\n        world_size (int, property): Number of processes.\n        rank (int, property): Process index of all processes.\n        local_rank (int, property): Process index of local processes.\n        distributed (bool, property): If runner is running in distributed mode.\n        is_main_process (bool, property): If current process is the main process of all processes.\n        is_local_main_process (bool, property): If current process is the main process of local processes.\n\n    Attributes: logging:\n        meters (AverageMeters | MultiTaskAverageMeters): Average meters.\n            Initialised to `AverageMeters` by default.\n        metrics (Metrics | MultiTaskMetrics | None): Metrics for evaluating.\n        logger:\n        writer:\n\n    See Also:\n        [`RunnerState`][danling.runner.runner_state.RunnerState]: The runeer base that stores runtime information.\n        [`BaseRunner`][danling.runner.BaseRunner]: The base runner class.\n    \"\"\"\n\n    # DO NOT set default value in class, as they won't be stored in `__dict__`.\n\n    timestamp: str\n\n    _mode: RunnerMode\n    _state: RunnerState\n    inited: bool = False\n\n    model: Callable | None = None\n    criterion: Callable | None = None\n    optimizer: Any | None = None\n    scheduler: Any | None = None\n\n    datasets: FlatDict\n    datasamplers: FlatDict\n    dataloaders: FlatDict\n    split: str\n\n    results: NestedDict\n    meters: AverageMeters\n    metrics: Metrics | None = None\n    logger: logging.Logger | None = None\n    writer: Any | None = None\n\n    def __init__(self, config: NestedDict) -&gt; None:\n        self.timestamp = get_time_str()\n        if \"datasets\" not in self.__dict__:\n            self.datasets = FlatDict()\n        if \"datasamplers\" not in self.__dict__:\n            self.datasamplers = FlatDict()\n        if \"dataloaders\" not in self.__dict__:\n            self.dataloaders = FlatDict()\n        if \"results\" not in self.__dict__:\n            self.results = NestedDict()\n        self.meters = AverageMeters()\n        self._mode = RunnerMode.train  # type: ignore[assignment]\n        # must init state at last to avoid name conflicts\n        self._state = RunnerState(config)\n        self.inited = True\n        self.init_distributed()\n        if self.state.seed is not None:\n            self.set_seed()\n        if self.state.deterministic:\n            self.set_deterministic()\n        if self.state.log:\n            self.init_logging()\n        self.init_print()\n        if self.state.tensorboard:\n            self.init_tensorboard()\n\n    def init_distributed(self) -&gt; None:\n        r\"\"\"\n        Initialise distributed running environment.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def init_deepspeed(  # pylint: disable=too-many-branches, too-many-statements\n        self, config: Dict = None  # type: ignore\n    ) -&gt; Dict:\n        r\"\"\"\n        Preprocess DeepSpeed config.\n        \"\"\"\n\n        if config is None:\n            config = self.state.get(\"deepspeed\")\n        if config is None:\n            return {}\n        if isinstance(config, str):\n            config = NestedDict.load(config)\n        if config.get(\"steps_per_print\", \"auto\") == \"auto\":\n            config[\"steps_per_print\"] = self.state.log_interval\n        if config.get(\"train_micro_batch_size_per_gpu\", \"auto\") == \"auto\":\n            config[\"train_micro_batch_size_per_gpu\"] = self.batch_size\n        if \"amp\" in config:\n            amp = config[\"amp\"]\n            if amp.get(\"enabled\", \"auto\") == \"auto\":\n                amp[\"enabled\"] = \"true\"\n            if amp.get(\"opt_level\", \"auto\") == \"auto\":\n                amp[\"opt_level\"] = \"O1\"\n        if \"zero_optimization\" in config:\n            zero = config[\"zero_optimization\"]\n            if zero.get(\"allgather_bucket_size\") == \"auto\":\n                zero[\"allgather_bucket_size\"] = 1e6\n            if zero.get(\"reduce_bucket_size\") == \"auto\":\n                zero[\"reduce_bucket_size\"] = 1e6\n            if zero.get(\"stage3_max_live_parameters\") == \"auto\":\n                zero[\"stage3_max_live_parameters\"] = 1e8\n            if zero.get(\"stage3_max_live_gradients\") == \"auto\":\n                zero[\"stage3_max_live_gradients\"] = 1e8\n            if zero.get(\"stage3_max_reuse_distance\") == \"auto\":\n                zero[\"stage3_max_reuse_distance\"] = 1e8\n            if zero.get(\"stage3_prefetch_bucket_size\") == \"auto\":\n                zero[\"stage3_prefetch_bucket_size\"] = 1e6\n            if zero.get(\"stage3_param_persistence_threshold\") == \"auto\":\n                zero[\"stage3_param_persistence_threshold\"] = 1e8\n            if \"amp\" in config:\n                if \"fp16\" not in config:\n                    config[\"fp16\"] = {}\n                if config[\"fp16\"].get(\"enabled\", \"auto\"):\n                    config[\"fp16\"][\"enabled\"] = config[\"amp\"][\"enabled\"]\n                warn(\n                    f\"AMP is not compatible with ZeRO. Automatically set 'fp16' to {config['amp']['enabled']}\",\n                    stacklevel=2,\n                )\n                del config[\"amp\"]\n        if \"optimizer\" in config:\n            if \"params\" not in config[\"optimizer\"]:\n                config[\"optimizer\"][\"params\"] = {}\n            optimizer = config[\"optimizer\"][\"params\"]\n            if optimizer.get(\"lr\", \"auto\") == \"auto\":\n                optimizer[\"lr\"] = self.state.get(\"optim.lr\", 1e-3)\n            if optimizer.get(\"weight_decay\", \"auto\") == \"auto\":\n                optimizer[\"weight_decay\"] = self.state.get(\"optim.weight_decay\", 1e-2)\n            if optimizer.get(\"betas\") == \"auto\":\n                optimizer[\"betas\"] = (0.9, 0.999)\n            if optimizer.get(\"eps\") == \"auto\":\n                optimizer[\"eps\"] = 1e-8\n        if \"scheduler\" in config:\n            if \"params\" not in config[\"scheduler\"]:\n                config[\"scheduler\"][\"params\"] = {}\n            scheduler = config[\"scheduler\"][\"params\"]\n            if scheduler.get(\"total_num_steps\", \"auto\") == \"auto\":\n                scheduler[\"total_num_steps\"] = self.total_steps\n            if scheduler.get(\"warmup_num_steps\", \"auto\") == \"auto\":\n                scheduler[\"warmup_num_steps\"] = scheduler[\"total_num_steps\"] // 20\n            if scheduler.get(\"warmup_max_lr\", \"auto\") == \"auto\":\n                if self.optimizer:\n                    scheduler[\"warmup_max_lr\"] = self.optimizer.param_groups[0][\"lr\"]\n                elif \"optimizer\" in config:\n                    scheduler[\"warmup_max_lr\"] = config[\"optimizer\"][\"params\"][\"lr\"]\n                else:\n                    raise ValueError(\"warmup_max_lr is not defined and cannot be inferred\")\n            if scheduler.get(\"warmup_min_lr\", \"auto\") == \"auto\":\n                scheduler[\"warmup_min_lr\"] = 1e-7\n        return config\n\n    @on_main_process\n    def init_logging(self) -&gt; None:\n        r\"\"\"\n        Set up logging.\n        \"\"\"\n\n        os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n        # Why is setting up proper logging so !@?#! ugly?\n        logging.config.dictConfig(\n            {\n                \"version\": 1,\n                \"disable_existing_loggers\": False,\n                \"formatters\": {\n                    \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n                },\n                \"handlers\": {\n                    \"stdout\": {\n                        \"level\": \"INFO\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.StreamHandler\",\n                        \"stream\": \"ext://sys.stdout\",\n                    },\n                    \"logfile\": {\n                        \"level\": \"DEBUG\",\n                        \"formatter\": \"standard\",\n                        \"class\": \"logging.FileHandler\",\n                        \"filename\": self.log_path,\n                        \"mode\": \"a\",\n                    },\n                },\n                \"loggers\": {\n                    \"\": {\n                        \"handlers\": [\"stdout\", \"logfile\"],\n                        \"level\": \"DEBUG\",\n                        \"propagate\": True,\n                    },\n                },\n            }\n        )\n        logging.captureWarnings(True)\n        self.logger = logging.getLogger(\"runner\")\n        self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n\n    def init_print(self, process: int = 0) -&gt; None:\n        r\"\"\"\n        Set up `print`.\n\n        Only print on a specific `process` or when `force = True`.\n\n        Args:\n            process: The process to `print` on.\n\n        Notes\n        -----\n        If `self.state.log = True`, the default `print` function will be override by `logging.info`.\n        \"\"\"\n\n        logger = logging.getLogger(\"print\")\n        logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n        import builtins as __builtin__  # pylint: disable=C0415\n\n        builtin_print = __builtin__.print\n\n        @catch\n        def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=redefined-builtin\n            if self.rank == process or force:\n                if self.state.log:\n                    logger.info(*args, **kwargs)\n                else:\n                    builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n        __builtin__.print = print\n\n    @on_main_process\n    def init_tensorboard(self, *args, **kwargs) -&gt; None:\n        r\"\"\"\n        Set up Tensoraoard SummaryWriter.\n        \"\"\"\n        raise NotImplementedError\n\n    def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n        r\"\"\"\n        Set up random seed.\n\n        Args:\n            seed: Random seed to set.\n                Defaults to `self.state.seed` (`config.seed`).\n\n            bias: Make the seed different for each processes.\n\n                This avoids same data augmentation are applied on every processes.\n\n                Defaults to `self.rank`.\n\n                Set to `False` to disable this feature.\n        \"\"\"\n\n        seed = seed or self.state.seed\n        bias = bias or self.rank\n        if bias:\n            seed += bias\n        if np_random is not None:\n            np_random.seed(seed)\n        random.seed(seed)\n\n    def set_deterministic(self) -&gt; None:\n        r\"\"\"\n        Set up deterministic.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def scale_lr(\n        self,\n        lr: float,\n        lr_scale_factor: float | None = None,\n        batch_size_base: int | None = None,\n    ) -&gt; float:\n        r\"\"\"\n        Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n        \"\"\"\n\n        if lr_scale_factor in self.state:\n            lr_scale_factor = self.state.lr_scale_factor\n\n        if lr_scale_factor is None:\n            if batch_size_base is None:\n                batch_size_base = getattr(self, \"batch_size_base\", None)\n                if batch_size_base is None:\n                    raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n            lr_scale_factor = self.batch_size_equivalent / batch_size_base\n        elif batch_size_base is not None:\n            warn(\n                \"batch_size_base will be ignored if lr_scale_factor is specified\", category=RuntimeWarning, stacklevel=2\n            )\n        lr = lr * lr_scale_factor\n        self.state.lr_scale_factor = lr_scale_factor\n        return lr\n\n    def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n        r\"\"\"\n        Backward loss and step optimizer &amp; scheduler.\n\n        This method increment `self.state.steps`.\n\n        This method also increment `self.state.iters` when `batch_size` is specified.\n\n        Args:\n            zero_grad: Whether to zero the gradients.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Return dict of all attributes for checkpoint.\n        \"\"\"\n\n        return cls(self.state)\n\n    def dict(self, cls: Callable = dict) -&gt; Mapping:\n        r\"\"\"\n        Convert state to Mapping.\n\n        Args:\n            cls: Target `clc to convert to.\n        \"\"\"\n\n        return self.state.dict(cls)\n\n    @catch\n    def save(self, obj: Any, file: PathStr, main_process_only: bool = True, *args, **kwargs) -&gt; File:\n        r\"\"\"\n        Save any file with supported extensions.\n\n        `Runner.save` internally calls `dl.save`,\n        but with additional arguments to allow it save only on the main process.\n        Moreover, any error raised by `Runner.save` will be caught and logged.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            return save(obj, file, *args, **kwargs)\n        return file\n\n    @staticmethod\n    def load(file: PathStr, *args, **kwargs) -&gt; Any:\n        r\"\"\"\n        Load any file with supported extensions.\n\n        `Runner.load` is identical to `dl.load`.\n        \"\"\"\n\n        return load(file, *args, **kwargs)\n\n    @catch\n    def json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n        r\"\"\"\n        Dump Runner State to json file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            return self.state.json(file, *args, **kwargs)\n\n    @classmethod\n    def from_json(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from json file.\n\n        This function calls `self.from_jsons()` to construct object from json string.\n        You may overwrite `from_jsons` in case something is not json serializable.\n        \"\"\"\n\n        with FlatDict.open(file) as fp:\n            return cls.from_jsons(fp.read(), *args, **kwargs)\n\n    def jsons(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner State to json string.\n        \"\"\"\n\n        return self.state.jsons(*args, **kwargs)\n\n    @classmethod\n    def from_jsons(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from json string.\n        \"\"\"\n\n        return cls(Config.from_jsons(string, *args, **kwargs))\n\n    @catch\n    def yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n        r\"\"\"\n        Dump Runner State to yaml file.\n        \"\"\"\n\n        if main_process_only and self.is_main_process or not main_process_only:\n            return self.state.yaml(file, *args, **kwargs)\n\n    @classmethod\n    def from_yaml(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from yaml file.\n\n        This function calls `self.from_yamls()` to construct object from yaml string.\n        You may overwrite `from_yamls` in case something is not yaml serializable.\n        \"\"\"\n\n        with FlatDict.open(file) as fp:\n            return cls.from_yamls(fp.read(), *args, **kwargs)\n\n    def yamls(self, *args, **kwargs) -&gt; str:\n        r\"\"\"\n        Dump Runner State to yaml string.\n        \"\"\"\n\n        return self.state.yamls(*args, **kwargs)\n\n    @classmethod\n    def from_yamls(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Construct Runner from yaml string.\n        \"\"\"\n\n        return cls(Config.from_yamls(string, *args, **kwargs))\n\n    def check_dir(self, action: str = \"warn\") -&gt; bool:\n        r\"\"\"\n        Check if `self.dir` is not empty.\n\n        Args:\n            action (str): The action to perform if `self.dir` is not empty.\n            Can be one of (\"warn\", \"raise\", \"ignore\"), default is \"warn\".\n        \"\"\"\n\n        if action and action not in (\"warn\", \"raise\", \"ignore\"):\n            raise ValueError(f\"action should be one of warn, raise or ignore, but got {action}\")\n        if os.listdir(self.dir):\n            if action == \"warn\":\n                warn(\n                    f\"Directory `{self.dir}` is not empty\",\n                    category=RuntimeWarning,\n                    stacklevel=2,\n                )\n            if action == \"raise\":\n                raise RuntimeError(f\"Directory `{self.dir}` is not empty\")\n            return False\n        return True\n\n    @catch\n    @on_main_process\n    def save_checkpoint(self, epochs: int | None = None) -&gt; None:\n        r\"\"\"\n        Save checkpoint to `self.checkpoint_dir`.\n\n        The checkpoint will be saved to `self.checkpoint_dir/latest.pth`.\n\n        If `self.state.save_interval` is positive and `self.state.epochs + 1` is a multiple of `save_interval`,\n        the checkpoint will also be copied to `self.checkpoint_dir/epoch-{self.state.epochs}.pth`.\n\n        If `self.is_best` is `True`, the checkpoint will also be copied to `self.checkpoint_dir/best.pth`.\n        \"\"\"\n\n        epochs = epochs or self.state.epochs\n        save_interval = self.state.get(\"save_interval\", -1)\n        latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        self.save(self.state_dict(), latest_path)\n        if save_interval &gt; 0 and (epochs + 1) % save_interval == 0:\n            save_path = os.path.join(self.checkpoint_dir, f\"epoch-{epochs}.pth\")\n            shutil.copy(latest_path, save_path)\n        if self.is_best:\n            best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n            shutil.copy(latest_path, best_path)\n\n    def load_checkpoint(\n        self,\n        checkpoint: Mapping | bytes | str | os.PathLike | None = None,\n        auto_resume: bool | None = None,\n        override_state: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Load info from checkpoint.\n\n        Args:\n            checkpoint: Checkpoint (or its path) to load.\n                Defaults to `self.state.checkpoint`.\n            auto_resume: Automatically resume from latest checkpoint if exists.\n                Defaults to `False`.\n                If is `True` and `checkpoint` is None, will set it to `self.checkpoint_dir/latest.pth`.\n            override_state: If True, override runner state with checkpoint state.\n                Defaults to `False`.\n            *args: Additional arguments to pass to `self.load`.\n            **kwargs: Additional keyword arguments to pass to `self.load`.\n\n        Raises:\n            FileNotFoundError: If `checkpoint` does not exists.\n\n        See Also:\n            [`from_checkpoint`][danling.BaseRunner.from_checkpoint]: Build runner from checkpoint.\n            [`load_pretrained`][danling.BaseRunner.load_pretrained]: Load parameters from pretrained checkpoint.\n        \"\"\"\n\n        checkpoint = checkpoint if checkpoint is not None else self.state.get(\"checkpoint\")\n        auto_resume = auto_resume if auto_resume is not None else self.state.get(\"auto_resume\", False)\n\n        # TODO: Support loading checkpoints in other format\n        if checkpoint is not None:\n            if auto_resume:\n                warn(\n                    \"latest checkpoint is preempted by value specified in checkpoint\",\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n            if isinstance(checkpoint, (bytes, str, os.PathLike)):\n                if not os.path.exists(checkpoint):\n                    raise FileNotFoundError(f\"checkpoint is set to {checkpoint!r} but does not exist\")\n                self.state.checkpoint = checkpoint\n                ckpt = self.load(checkpoint, *args, **kwargs)\n            elif isinstance(checkpoint, Mapping):\n                ckpt = checkpoint\n            else:\n                raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint\")\n        elif auto_resume:\n            checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n            if os.path.exists(checkpoint):\n                self.state.checkpoint = checkpoint\n                ckpt = self.load(checkpoint, *args, **kwargs)\n            else:\n                warn(\"latest checkpoint does not exits\", category=RuntimeWarning, stacklevel=2)\n                return\n        else:\n            raise ValueError(\"checkpoint is not specified and auto_resume is not set to True\")\n\n        # TODO: Wrap state_dict in a dataclass\n        self.state.merge(ckpt[\"runner\"], overwrite=override_state)\n        if self.model is not None and \"model\" in ckpt:\n            model = self.unwrap_model(self.model)\n            model.load_state_dict(ckpt[\"model\"])\n        if self.optimizer is not None and \"optimizer\" in ckpt:\n            self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n        if self.scheduler is not None and \"scheduler\" in ckpt:\n            self.scheduler.load_state_dict(ckpt[\"scheduler\"])\n        self.state.iter_begin = self.state.iters\n        self.state.step_begin = self.state.steps\n        self.state.epoch_begin = self.state.epochs\n\n    @classmethod\n    def from_checkpoint(cls, checkpoint: Mapping | bytes | str | os.PathLike, *args, **kwargs) -&gt; BaseRunner:\n        r\"\"\"\n        Build BaseRunner from checkpoint.\n\n        Args:\n            checkpoint: Checkpoint (or its path) to load.\n            *args: Additional arguments to pass to `cls.load`.\n            **kwargs: Additional keyword arguments to pass to `cls.load`.\n\n        Returns:\n            (BaseRunner):\n        \"\"\"\n\n        if isinstance(checkpoint, (bytes, str, os.PathLike)):\n            ckpt = cls.load(checkpoint, *args, **kwargs)\n        elif isinstance(checkpoint, Mapping):\n            ckpt = checkpoint\n        else:\n            raise ValueError(f\"checkpoint is set to {checkpoint} but is not a valid checkpoint\")\n        runner = cls(**ckpt[\"runner\"])\n        runner.load_checkpoint(ckpt, override_state=False)\n        return runner\n\n    def load_pretrained(self, checkpoint: Mapping | bytes | str | os.PathLike | None = None, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Load parameters from pretrained checkpoint.\n\n        This method only loads the model weights.\n\n        Args:\n            checkpoint: Pretrained checkpoint (or its path) to load.\n                Defaults to `self.state.pretrained`.\n            *args: Additional arguments to pass to `self.load`.\n            **kwargs: Additional keyword arguments to pass to `self.load`.\n\n        Raises:\n            FileNotFoundError: If `checkpoint` does not exists.\n\n        See Also:\n            [`load_checkpoint`][danling.BaseRunner.load_checkpoint]: Load info from checkpoint.\n        \"\"\"\n\n        # TODO: Support loading checkpoints in other format\n        checkpoint = checkpoint if checkpoint is not None else self.state.get(\"pretrained\")\n        if checkpoint is None:\n            raise ValueError(\"pretrained is not specified\")\n        if isinstance(checkpoint, (bytes, str, os.PathLike)):\n            if not os.path.exists(checkpoint):\n                raise FileNotFoundError(f\"pretrained is set to {checkpoint!r} but does not exist\")\n            ckpt = self.load(checkpoint, *args, **kwargs)\n        elif isinstance(checkpoint, Mapping):\n            ckpt = checkpoint\n        else:\n            raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint\")\n        if self.model is not None and \"model\" in ckpt:\n            model = self.unwrap_model(self.model)\n            model.load_state_dict(ckpt[\"model\"])\n        else:\n            raise ValueError(f\"Unable to find model weights in {checkpoint!r}\")\n\n    def append_result(self, result: NestedDict, index: int | None = None) -&gt; None:\n        r\"\"\"\n        Append result to `self.results`.\n\n        Warnings:\n            `self.results` is heavily relied upon for computing metrics.\n\n            Failed to use this method may lead to unexpected behavior.\n        \"\"\"\n\n        if index is None:\n            index = self.state.epochs\n            global __APPEND_RESULT_COUNTER__  # pylint: disable=global-statement\n            __APPEND_RESULT_COUNTER__ += 1\n            if index == 0 and __APPEND_RESULT_COUNTER__ &gt; 1:\n                warn(\n                    \"\"\"\n                    Automatically set index to `self.state.epochs`.\n                    Please ensure `self.state.epochs` updates before calling `append_result`\n                    \"\"\",\n                    category=RuntimeWarning,\n                    stacklevel=2,\n                )\n        if index in self.results:\n            self.results[index].merge(result)\n        else:\n            self.results[index] = result\n\n    def print_result(self) -&gt; None:\n        r\"\"\"\n        Print latest and best result.\n        \"\"\"\n\n        print(f\"latest result: {self.latest_result}\")\n        print(f\"best result: {self.best_result}\")\n\n    def step_log(self, split: str, iteration: int, length: int | None = None):\n        if length is None:\n            length = len(self.dataloaders[split]) - 1\n        result = self.meters.val\n        if self.metrics is not None:\n            result.merge(self.metrics.val)\n        print(self.format_step_result(result, split, iteration, length))\n        if self.mode == \"train\":\n            self.write_result(result, split)\n        return result\n\n    def format_step_result(\n        self, result: NestedDict, split: str, steps: int, length: int, format_spec: str = \".4f\"\n    ) -&gt; str:\n        result = NestedDict(result).clone()\n        repr_str = \"\"\n        if split is not None:\n            if self.mode == \"train\":\n                repr_str = f\"training on {split} \"\n            elif self.mode == \"eval\":\n                repr_str = f\"evaluating on {split} \"\n            else:\n                repr_str = f\"running in {self.mode} mode on {split} \"\n        repr_str += f\"[{steps}/{length}]\\t\"\n        return repr_str + self.format_result(result, format_spec=format_spec)\n\n    def format_epoch_result(\n        self, result: NestedDict, epochs: int | None = None, epoch_end: int | None = None, format_spec: str = \".4f\"\n    ) -&gt; str:\n        result = NestedDict(result).clone()\n        epochs = epochs or self.state.epochs\n        epoch_end = epoch_end or self.state.epoch_end\n        repr_str = f\"epoch [{epochs}/{epoch_end - 1}]\\n\" if epochs is not None and epoch_end else \"\"\n        repr_str += \"\\n\".join([f\"{k}:\\t{self.format_result(v, format_spec=format_spec)}\" for k, v in result.items()])\n        return repr_str\n\n    def format_result(self, result, format_spec: str = \".4f\") -&gt; str:\n        return \"\\t\".join([f\"{k}: {format(v, format_spec)}\" for k, v in result.items()])\n\n    def write_result(self, result: NestedDict, split: str, steps: int | None = None):\n        if steps is None:\n            steps = self.steps\n        for name, score in result.all_items():\n            name = name.replace(\".\", \"/\")\n            if name == \"loss\" and isinstance(score, AverageMeter):\n                score = score.avg\n            if isinstance(score, Sequence):\n                for i, s in enumerate(score):\n                    self.write_score(f\"{name}/{i}\", s, split, steps)\n            elif isinstance(score, Mapping):\n                for k, s in score.items():\n                    self.write_score(f\"{name}/{k}\", s, split, steps)\n            else:\n                self.write_score(name, score, split, steps)\n\n    def write_score(self, name: str, score: float, split: str, steps: int):\n        if self.writer:\n            self.writer.add_scalar(f\"{split}/{name}\", score, steps)\n\n    @catch\n    @on_main_process\n    def save_result(self) -&gt; None:\n        r\"\"\"\n        Save result to `self.dir`.\n\n        This method will save latest and best result to\n        `self.dir/latest.json` and `self.dir/best.json` respectively.\n        \"\"\"\n\n        results_path = os.path.join(self.dir, \"results.json\")\n        self.save(\n            {\n                \"name\": self.name,\n                \"id\": self.id,\n                \"timestamp\": self.timestamp,\n                \"results\": self.results,\n            },\n            results_path,\n            indent=4,\n        )\n        ret = {\"name\": self.name, \"id\": self.id, \"timestamp\": self.timestamp}\n        result = self.latest_result\n        if isinstance(result, FlatDict):\n            result = result.dict()\n        # This is slower but ensure id is the first key\n        if result is not None:\n            ret.update(result)\n        latest_path = os.path.join(self.dir, \"latest.json\")\n        self.save(ret, latest_path, indent=4)\n        if self.is_best:\n            best_path = os.path.join(self.dir, \"best.json\")\n            shutil.copy(latest_path, best_path)\n\n    @cached_property\n    def name(self):\n        if \"name\" in self._state:\n            return self.state[\"name\"]\n        return f\"{self.state.experiment_name}-{self.state.run_name}\"\n\n    @cached_property\n    def id(self):\n        return f\"{self.state.experiment_id:.8}{self.state.run_id:.8}\"\n\n    @cached_property\n    def uuid(self) -&gt; UUID:\n        r\"\"\"\n        UUID of the state.\n        \"\"\"\n\n        return uuid5(self.run_uuid, self.id)\n\n    @property\n    def mode(self) -&gt; RunnerMode:\n        return self._mode\n\n    @mode.setter\n    def mode(self, mode: str | RunnerMode) -&gt; None:\n        if isinstance(mode, str):\n            mode = RunnerMode(mode)\n        self._mode = mode\n\n    @property\n    def state(self) -&gt; RunnerState:\n        return self._state\n\n    @property\n    def batch_size(self) -&gt; int:\n        r\"\"\"\n        Batch size.\n\n        Notes:\n            If `train` is in `dataloaders`, then `batch_size` is the batch size of `train`.\n            Otherwise, `batch_size` is the batch size of the first dataloader.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        if self.dataloaders and self.split:\n            return self.dataloaders[self.split].batch_size\n        batch_size = self.state.get(\"dataloader.batch_size\")\n        if batch_size:\n            return batch_size\n        raise AttributeError(\"batch_size could not be inferred and is not in config\")\n\n    @property\n    def batch_size_equivalent(self) -&gt; int:\n        r\"\"\"\n        Actual batch size.\n\n        Returns:\n            (int): `batch_size` * `world_size` * `accum_steps`\n        \"\"\"\n\n        return self.batch_size * self.world_size * self.accum_steps\n\n    @cached_property\n    def total_epochs(self) -&gt; int:\n        if self.state.epoch_end:\n            return self.state.epoch_end - self.state.epoch_begin + 1\n        raise ValueError(\"epoch_end is not specified\")\n\n    @cached_property\n    def total_steps(self) -&gt; int:\n        if self.state.step_end:\n            return self.state.step_end - self.state.step_begin\n        dataset = self.datasets.get(\"train\", next(iter(self.datasets.values())))\n        return self.total_epochs * ceil(len(dataset) / self.batch_size) + 1\n\n    @cached_property\n    def accum_steps(self) -&gt; int:\n        r\"\"\"\n        Accumulated steps.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        return self.state.get(\"accum_steps\", 1)\n\n    @property\n    def progress(self) -&gt; float:\n        r\"\"\"\n        Training Progress.\n\n        Returns:\n            (float):\n\n        Raises:\n            RuntimeError: If no terminal is defined.\n        \"\"\"\n\n        return self.steps / self.total_steps\n\n    @property\n    def device(self) -&gt; Any:\n        r\"\"\"\n        Device of runner.\n        \"\"\"\n\n        return \"cpu\"\n\n    @property\n    def world_size(self) -&gt; int:\n        r\"\"\"\n        Number of processes.\n        \"\"\"\n\n        return 1\n\n    @property\n    def rank(self) -&gt; int:\n        r\"\"\"\n        Process index of all processes.\n        \"\"\"\n\n        return 0\n\n    @property\n    def local_rank(self) -&gt; int:\n        r\"\"\"\n        Process index of local processes.\n        \"\"\"\n\n        return 0\n\n    @property\n    def distributed(self) -&gt; bool:\n        r\"\"\"\n        If runner is running in distributed mode.\n        \"\"\"\n\n        return self.world_size &gt; 1\n\n    @property\n    def is_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process of all processes.\n        \"\"\"\n\n        return self.rank == 0\n\n    @property\n    def is_local_main_process(self) -&gt; bool:\n        r\"\"\"\n        If current process is the main process of local processes.\n        \"\"\"\n\n        return self.local_rank == 0\n\n    @property\n    def best_fn(self) -&gt; Callable:\n        r\"\"\"\n        Function to determine the best score from a list of scores.\n\n        By default, the `best_fn` returns `min` if `self.state.score_name` is `loss`,\n        otherwise, returns `max`.\n\n        Subclass can override this method to accommodate needs, such as `min`.\n\n        Returns:\n            (callable):\n        \"\"\"\n\n        return max if self.state.score_name != \"loss\" else min\n\n    @property\n    def best_index(self) -&gt; int:\n        r\"\"\"\n        Find the best index from all scores.\n\n        Returns:\n            (int):\n        \"\"\"\n\n        if not self.scores:\n            return 0\n        values = list(self.scores.values())\n        return self.best_fn(range(len(values)), key=values.__getitem__)\n\n    @property\n    def latest_result(self) -&gt; NestedDict | None:\n        r\"\"\"\n        Latest result.\n        \"\"\"\n\n        if not self.results:\n            return None\n        latest_index = next(reversed(self.results if PY38_PLUS else list(self.results)))  # type: ignore\n        ret = self.results[latest_index].clone()\n        ret[\"index\"] = latest_index\n        return ret\n\n    @property\n    def best_result(self) -&gt; NestedDict | None:\n        r\"\"\"\n        Best result.\n        \"\"\"\n\n        if not self.results:\n            return None\n        best_index = self.best_index\n        ret = self.results[best_index].clone()\n        ret[\"index\"] = best_index\n        return ret\n\n    @property\n    def scores(self) -&gt; FlatDict | None:\n        r\"\"\"\n        All scores.\n\n        Scores are extracted from results by `score_split` and `runner.state.score_name`,\n        following `[r[score_split][self.state.score_name] for r in self.results]`.\n\n        Scores are considered as the index of the performance of the model.\n        It is useful to determine the best model and the best hyper-parameters.\n\n        `score_split` is defined in `self.state.score_split`.\n        If it is not set, `DanLing` will use `val` or `validate` if they appear in the `latest_result`.\n        If `DanLing` still could not find, it will fall back to the second key in the `latest_result`\n        if it contains more that one element, or the first key.\n\n        Note that certain keys are ignored when falling back, they are defined in {IGNORED_SET_NAMES}.\n        \"\"\"\n\n        if not self.results:\n            return None\n        subsets = [i for i in self.latest_result.keys() if i not in IGNORED_SET_NAMES]  # type: ignore\n        score_split = self.state.get(\"score_split\")\n        if score_split is None and \"val\" in subsets:\n            score_split = \"val\"\n        if score_split is None and \"validate\" in subsets:\n            score_split = \"validate\"\n        if score_split is None:\n            score_split = subsets[1] if len(subsets) &gt; 1 else subsets[0]\n        return FlatDict({k: v[score_split][self.state.score_name] for k, v in self.results.items()})\n\n    @property\n    def latest_score(self) -&gt; float | None:\n        r\"\"\"\n        Latest score.\n        \"\"\"\n\n        if not self.results:\n            return None\n        if not PY38_PLUS:\n            return next(reversed(list(self.scores.values())))  # type: ignore\n        return next(reversed(self.scores.values()))  # type: ignore\n\n    @property\n    def best_score(self) -&gt; float | None:\n        r\"\"\"\n        Best score.\n        \"\"\"\n\n        if not self.results:\n            return None\n        return self.scores[self.best_index]  # type: ignore\n\n    @property\n    def is_best(self) -&gt; bool:\n        r\"\"\"\n        If current epoch is the best epoch.\n        \"\"\"\n\n        if not self.results:\n            return True\n        try:\n            return abs(self.latest_score - self.best_score) &lt; 1e-7  # type: ignore\n        except TypeError:\n            return True\n\n    @property\n    @ensure_dir\n    def dir(self) -&gt; str:\n        r\"\"\"\n        Directory of the run.\n        \"\"\"\n\n        if \"dir\" in self.state:\n            return self.state.dir\n        return os.path.join(self.project_root, f\"{self.name}-{self.id}\", self.timestamp)\n\n    @cached_property\n    def log_path(self) -&gt; str:\n        r\"\"\"\n        Path of log file.\n        \"\"\"\n\n        if \"log_path\" in self.state:\n            return self.state.log_path\n        return os.path.join(self.dir, \"run.log\")\n\n    @property\n    @ensure_dir\n    def checkpoint_dir(self) -&gt; str:\n        r\"\"\"\n        Directory of checkpoints.\n        \"\"\"\n\n        if \"checkpoint_dir\" in self.state:\n            return self.state.checkpoint_dir\n        return os.path.join(self.dir, self.state.checkpoint_dir_name)\n\n    # def __getattribute__(self, name) -&gt; Any:\n    #     if name in (\"__class__\", \"__dict__\"):\n    #         return super().__getattribute__(name)\n    #     if name in self.__dict__:\n    #         return self.__dict__[name]\n    #     if name in dir(self):\n    #         return super().__getattribute__(name)\n    #     if \"state\" in self and name in self.state:\n    #         return self.state[name]\n    #     return super().__getattribute__(name)\n\n    def __getattr__(self, name) -&gt; Any:\n        if self.inited:\n            if name in self._state:\n                return self.state[name]\n            if name in dir(self.state):\n                return getattr(self.state, name)\n        return super().__getattribute__(name)\n\n    def __setattr__(self, name, value) -&gt; None:\n        if name in self.__dict__:\n            if isinstance(self.__dict__[name], Variable):\n                self.__dict__[name].set(value)\n            else:\n                self.__dict__[name] = value\n            return\n        if name in dir(self):\n            if isinstance(super().__getattribute__(name), Variable):\n                super().__getattribute__(name).set(value)\n            else:\n                object.__setattr__(self, name, value)\n            return\n        if self.inited:\n            if name in self.state:\n                if isinstance(self.state[name], Variable):\n                    self.state[name].set(value)\n                else:\n                    self.state[name] = value\n                return\n            if name in dir(self.state):\n                setattr(self.state, name, value)\n                return\n        object.__setattr__(self, name, value)\n\n    def __contains__(self, name) -&gt; bool:\n        return name in dir(self) or (\"state\" in self.__dict__ and name in dir(self.state))\n\n    def __repr__(self):\n        lines = []\n        for key, value in self.__dict__.items():\n            value_str = repr(value)\n            value_str = self._add_indent(value_str)\n            lines.append(\"(\" + key + \"): \" + value_str)\n\n        main_str = self.__class__.__name__ + \"(\"\n        if lines:\n            main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n        main_str += \")\"\n        return main_str\n\n    def _add_indent(self, text):\n        lines = text.split(\"\\n\")\n        # don't do anything for single-line stuff\n        if len(lines) == 1:\n            return text\n        first = lines.pop(0)\n        # add 2 spaces to each line but the first\n        lines = [(2 * \" \") + line for line in lines]\n        lines = \"\\n\".join(lines)\n        lines = first + \"\\n\" + lines\n        return lines\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.accum_steps","title":"<code>accum_steps: int</code>  <code>cached</code> <code>property</code>","text":"<p>Accumulated steps.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.batch_size","title":"<code>batch_size: int</code>  <code>property</code>","text":"<p>Batch size.</p> Notes <p>If <code>train</code> is in <code>dataloaders</code>, then <code>batch_size</code> is the batch size of <code>train</code>. Otherwise, <code>batch_size</code> is the batch size of the first dataloader.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.batch_size_equivalent","title":"<code>batch_size_equivalent: int</code>  <code>property</code>","text":"<p>Actual batch size.</p> <p>Returns:</p> Type Description <code>int</code> <p><code>batch_size</code> * <code>world_size</code> * <code>accum_steps</code></p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.best_fn","title":"<code>best_fn: Callable</code>  <code>property</code>","text":"<p>Function to determine the best score from a list of scores.</p> <p>By default, the <code>best_fn</code> returns <code>min</code> if <code>self.state.score_name</code> is <code>loss</code>, otherwise, returns <code>max</code>.</p> <p>Subclass can override this method to accommodate needs, such as <code>min</code>.</p> <p>Returns:</p> Type Description <code>callable</code>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.best_index","title":"<code>best_index: int</code>  <code>property</code>","text":"<p>Find the best index from all scores.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.best_result","title":"<code>best_result: NestedDict | None</code>  <code>property</code>","text":"<p>Best result.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.best_score","title":"<code>best_score: float | None</code>  <code>property</code>","text":"<p>Best score.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.checkpoint_dir","title":"<code>checkpoint_dir: str</code>  <code>property</code>","text":"<p>Directory of checkpoints.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.device","title":"<code>device: Any</code>  <code>property</code>","text":"<p>Device of runner.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.dir","title":"<code>dir: str</code>  <code>property</code>","text":"<p>Directory of the run.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.distributed","title":"<code>distributed: bool</code>  <code>property</code>","text":"<p>If runner is running in distributed mode.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.is_best","title":"<code>is_best: bool</code>  <code>property</code>","text":"<p>If current epoch is the best epoch.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.is_local_main_process","title":"<code>is_local_main_process: bool</code>  <code>property</code>","text":"<p>If current process is the main process of local processes.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.is_main_process","title":"<code>is_main_process: bool</code>  <code>property</code>","text":"<p>If current process is the main process of all processes.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.latest_result","title":"<code>latest_result: NestedDict | None</code>  <code>property</code>","text":"<p>Latest result.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.latest_score","title":"<code>latest_score: float | None</code>  <code>property</code>","text":"<p>Latest score.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.local_rank","title":"<code>local_rank: int</code>  <code>property</code>","text":"<p>Process index of local processes.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.log_path","title":"<code>log_path: str</code>  <code>cached</code> <code>property</code>","text":"<p>Path of log file.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.progress","title":"<code>progress: float</code>  <code>property</code>","text":"<p>Training Progress.</p> <p>Returns:</p> Type Description <code>float</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no terminal is defined.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.rank","title":"<code>rank: int</code>  <code>property</code>","text":"<p>Process index of all processes.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.scores","title":"<code>scores: FlatDict | None</code>  <code>property</code>","text":"<p>All scores.</p> <p>Scores are extracted from results by <code>score_split</code> and <code>runner.state.score_name</code>, following <code>[r[score_split][self.state.score_name] for r in self.results]</code>.</p> <p>Scores are considered as the index of the performance of the model. It is useful to determine the best model and the best hyper-parameters.</p> <p><code>score_split</code> is defined in <code>self.state.score_split</code>. If it is not set, <code>DanLing</code> will use <code>val</code> or <code>validate</code> if they appear in the <code>latest_result</code>. If <code>DanLing</code> still could not find, it will fall back to the second key in the <code>latest_result</code> if it contains more that one element, or the first key.</p> <p>Note that certain keys are ignored when falling back, they are defined in {IGNORED_SET_NAMES}.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.uuid","title":"<code>uuid: UUID</code>  <code>cached</code> <code>property</code>","text":"<p>UUID of the state.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.world_size","title":"<code>world_size: int</code>  <code>property</code>","text":"<p>Number of processes.</p>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.append_result","title":"<code>append_result(result, index=None)</code>","text":"<p>Append result to <code>self.results</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def append_result(self, result: NestedDict, index: int | None = None) -&gt; None:\n    r\"\"\"\n    Append result to `self.results`.\n\n    Warnings:\n        `self.results` is heavily relied upon for computing metrics.\n\n        Failed to use this method may lead to unexpected behavior.\n    \"\"\"\n\n    if index is None:\n        index = self.state.epochs\n        global __APPEND_RESULT_COUNTER__  # pylint: disable=global-statement\n        __APPEND_RESULT_COUNTER__ += 1\n        if index == 0 and __APPEND_RESULT_COUNTER__ &gt; 1:\n            warn(\n                \"\"\"\n                Automatically set index to `self.state.epochs`.\n                Please ensure `self.state.epochs` updates before calling `append_result`\n                \"\"\",\n                category=RuntimeWarning,\n                stacklevel=2,\n            )\n    if index in self.results:\n        self.results[index].merge(result)\n    else:\n        self.results[index] = result\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.check_dir","title":"<code>check_dir(action='warn')</code>","text":"<p>Check if <code>self.dir</code> is not empty.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>str</code> <p>The action to perform if <code>self.dir</code> is not empty.</p> <code>'warn'</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def check_dir(self, action: str = \"warn\") -&gt; bool:\n    r\"\"\"\n    Check if `self.dir` is not empty.\n\n    Args:\n        action (str): The action to perform if `self.dir` is not empty.\n        Can be one of (\"warn\", \"raise\", \"ignore\"), default is \"warn\".\n    \"\"\"\n\n    if action and action not in (\"warn\", \"raise\", \"ignore\"):\n        raise ValueError(f\"action should be one of warn, raise or ignore, but got {action}\")\n    if os.listdir(self.dir):\n        if action == \"warn\":\n            warn(\n                f\"Directory `{self.dir}` is not empty\",\n                category=RuntimeWarning,\n                stacklevel=2,\n            )\n        if action == \"raise\":\n            raise RuntimeError(f\"Directory `{self.dir}` is not empty\")\n        return False\n    return True\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.dict","title":"<code>dict(cls=dict)</code>","text":"<p>Convert state to Mapping.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Callable</code> <p>Target `clc to convert to.</p> <code>dict</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Convert state to Mapping.\n\n    Args:\n        cls: Target `clc to convert to.\n    \"\"\"\n\n    return self.state.dict(cls)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.from_checkpoint","title":"<code>from_checkpoint(checkpoint, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Build BaseRunner from checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Mapping | bytes | str | PathLike</code> <p>Checkpoint (or its path) to load.</p> required <code>*args</code> <p>Additional arguments to pass to <code>cls.load</code>.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>cls.load</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseRunner</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_checkpoint(cls, checkpoint: Mapping | bytes | str | os.PathLike, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Build BaseRunner from checkpoint.\n\n    Args:\n        checkpoint: Checkpoint (or its path) to load.\n        *args: Additional arguments to pass to `cls.load`.\n        **kwargs: Additional keyword arguments to pass to `cls.load`.\n\n    Returns:\n        (BaseRunner):\n    \"\"\"\n\n    if isinstance(checkpoint, (bytes, str, os.PathLike)):\n        ckpt = cls.load(checkpoint, *args, **kwargs)\n    elif isinstance(checkpoint, Mapping):\n        ckpt = checkpoint\n    else:\n        raise ValueError(f\"checkpoint is set to {checkpoint} but is not a valid checkpoint\")\n    runner = cls(**ckpt[\"runner\"])\n    runner.load_checkpoint(ckpt, override_state=False)\n    return runner\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.from_json","title":"<code>from_json(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json file.</p> <p>This function calls <code>self.from_jsons()</code> to construct object from json string. You may overwrite <code>from_jsons</code> in case something is not json serializable.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_json(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from json file.\n\n    This function calls `self.from_jsons()` to construct object from json string.\n    You may overwrite `from_jsons` in case something is not json serializable.\n    \"\"\"\n\n    with FlatDict.open(file) as fp:\n        return cls.from_jsons(fp.read(), *args, **kwargs)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.from_jsons","title":"<code>from_jsons(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from json string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_jsons(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from json string.\n    \"\"\"\n\n    return cls(Config.from_jsons(string, *args, **kwargs))\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.from_yaml","title":"<code>from_yaml(file, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml file.</p> <p>This function calls <code>self.from_yamls()</code> to construct object from yaml string. You may overwrite <code>from_yamls</code> in case something is not yaml serializable.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_yaml(cls, file: File, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from yaml file.\n\n    This function calls `self.from_yamls()` to construct object from yaml string.\n    You may overwrite `from_yamls` in case something is not yaml serializable.\n    \"\"\"\n\n    with FlatDict.open(file) as fp:\n        return cls.from_yamls(fp.read(), *args, **kwargs)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.from_yamls","title":"<code>from_yamls(string, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct Runner from yaml string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@classmethod\ndef from_yamls(cls, string: str, *args, **kwargs) -&gt; BaseRunner:\n    r\"\"\"\n    Construct Runner from yaml string.\n    \"\"\"\n\n    return cls(Config.from_yamls(string, *args, **kwargs))\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.init_deepspeed","title":"<code>init_deepspeed(config=None)</code>","text":"<p>Preprocess DeepSpeed config.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_deepspeed(  # pylint: disable=too-many-branches, too-many-statements\n    self, config: Dict = None  # type: ignore\n) -&gt; Dict:\n    r\"\"\"\n    Preprocess DeepSpeed config.\n    \"\"\"\n\n    if config is None:\n        config = self.state.get(\"deepspeed\")\n    if config is None:\n        return {}\n    if isinstance(config, str):\n        config = NestedDict.load(config)\n    if config.get(\"steps_per_print\", \"auto\") == \"auto\":\n        config[\"steps_per_print\"] = self.state.log_interval\n    if config.get(\"train_micro_batch_size_per_gpu\", \"auto\") == \"auto\":\n        config[\"train_micro_batch_size_per_gpu\"] = self.batch_size\n    if \"amp\" in config:\n        amp = config[\"amp\"]\n        if amp.get(\"enabled\", \"auto\") == \"auto\":\n            amp[\"enabled\"] = \"true\"\n        if amp.get(\"opt_level\", \"auto\") == \"auto\":\n            amp[\"opt_level\"] = \"O1\"\n    if \"zero_optimization\" in config:\n        zero = config[\"zero_optimization\"]\n        if zero.get(\"allgather_bucket_size\") == \"auto\":\n            zero[\"allgather_bucket_size\"] = 1e6\n        if zero.get(\"reduce_bucket_size\") == \"auto\":\n            zero[\"reduce_bucket_size\"] = 1e6\n        if zero.get(\"stage3_max_live_parameters\") == \"auto\":\n            zero[\"stage3_max_live_parameters\"] = 1e8\n        if zero.get(\"stage3_max_live_gradients\") == \"auto\":\n            zero[\"stage3_max_live_gradients\"] = 1e8\n        if zero.get(\"stage3_max_reuse_distance\") == \"auto\":\n            zero[\"stage3_max_reuse_distance\"] = 1e8\n        if zero.get(\"stage3_prefetch_bucket_size\") == \"auto\":\n            zero[\"stage3_prefetch_bucket_size\"] = 1e6\n        if zero.get(\"stage3_param_persistence_threshold\") == \"auto\":\n            zero[\"stage3_param_persistence_threshold\"] = 1e8\n        if \"amp\" in config:\n            if \"fp16\" not in config:\n                config[\"fp16\"] = {}\n            if config[\"fp16\"].get(\"enabled\", \"auto\"):\n                config[\"fp16\"][\"enabled\"] = config[\"amp\"][\"enabled\"]\n            warn(\n                f\"AMP is not compatible with ZeRO. Automatically set 'fp16' to {config['amp']['enabled']}\",\n                stacklevel=2,\n            )\n            del config[\"amp\"]\n    if \"optimizer\" in config:\n        if \"params\" not in config[\"optimizer\"]:\n            config[\"optimizer\"][\"params\"] = {}\n        optimizer = config[\"optimizer\"][\"params\"]\n        if optimizer.get(\"lr\", \"auto\") == \"auto\":\n            optimizer[\"lr\"] = self.state.get(\"optim.lr\", 1e-3)\n        if optimizer.get(\"weight_decay\", \"auto\") == \"auto\":\n            optimizer[\"weight_decay\"] = self.state.get(\"optim.weight_decay\", 1e-2)\n        if optimizer.get(\"betas\") == \"auto\":\n            optimizer[\"betas\"] = (0.9, 0.999)\n        if optimizer.get(\"eps\") == \"auto\":\n            optimizer[\"eps\"] = 1e-8\n    if \"scheduler\" in config:\n        if \"params\" not in config[\"scheduler\"]:\n            config[\"scheduler\"][\"params\"] = {}\n        scheduler = config[\"scheduler\"][\"params\"]\n        if scheduler.get(\"total_num_steps\", \"auto\") == \"auto\":\n            scheduler[\"total_num_steps\"] = self.total_steps\n        if scheduler.get(\"warmup_num_steps\", \"auto\") == \"auto\":\n            scheduler[\"warmup_num_steps\"] = scheduler[\"total_num_steps\"] // 20\n        if scheduler.get(\"warmup_max_lr\", \"auto\") == \"auto\":\n            if self.optimizer:\n                scheduler[\"warmup_max_lr\"] = self.optimizer.param_groups[0][\"lr\"]\n            elif \"optimizer\" in config:\n                scheduler[\"warmup_max_lr\"] = config[\"optimizer\"][\"params\"][\"lr\"]\n            else:\n                raise ValueError(\"warmup_max_lr is not defined and cannot be inferred\")\n        if scheduler.get(\"warmup_min_lr\", \"auto\") == \"auto\":\n            scheduler[\"warmup_min_lr\"] = 1e-7\n    return config\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.init_distributed","title":"<code>init_distributed()</code>","text":"<p>Initialise distributed running environment.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_distributed(self) -&gt; None:\n    r\"\"\"\n    Initialise distributed running environment.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.init_logging","title":"<code>init_logging()</code>","text":"<p>Set up logging.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@on_main_process\ndef init_logging(self) -&gt; None:\n    r\"\"\"\n    Set up logging.\n    \"\"\"\n\n    os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n    # Why is setting up proper logging so !@?#! ugly?\n    logging.config.dictConfig(\n        {\n            \"version\": 1,\n            \"disable_existing_loggers\": False,\n            \"formatters\": {\n                \"standard\": {\"format\": \"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"},\n            },\n            \"handlers\": {\n                \"stdout\": {\n                    \"level\": \"INFO\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.StreamHandler\",\n                    \"stream\": \"ext://sys.stdout\",\n                },\n                \"logfile\": {\n                    \"level\": \"DEBUG\",\n                    \"formatter\": \"standard\",\n                    \"class\": \"logging.FileHandler\",\n                    \"filename\": self.log_path,\n                    \"mode\": \"a\",\n                },\n            },\n            \"loggers\": {\n                \"\": {\n                    \"handlers\": [\"stdout\", \"logfile\"],\n                    \"level\": \"DEBUG\",\n                    \"propagate\": True,\n                },\n            },\n        }\n    )\n    logging.captureWarnings(True)\n    self.logger = logging.getLogger(\"runner\")\n    self.logger.flush = lambda: [h.flush() for h in self.logger.handlers]  # type: ignore\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.init_print","title":"<code>init_print(process=0)</code>","text":"<p>Set up <code>print</code>.</p> <p>Only print on a specific <code>process</code> or when <code>force = True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>process</code> <code>int</code> <p>The process to <code>print</code> on.</p> <code>0</code>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.init_print--notes","title":"Notes","text":"<p>If <code>self.state.log = True</code>, the default <code>print</code> function will be override by <code>logging.info</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def init_print(self, process: int = 0) -&gt; None:\n    r\"\"\"\n    Set up `print`.\n\n    Only print on a specific `process` or when `force = True`.\n\n    Args:\n        process: The process to `print` on.\n\n    Notes\n    -----\n    If `self.state.log = True`, the default `print` function will be override by `logging.info`.\n    \"\"\"\n\n    logger = logging.getLogger(\"print\")\n    logger.flush = lambda: [h.flush for h in logger.handlers]  # type: ignore\n    import builtins as __builtin__  # pylint: disable=C0415\n\n    builtin_print = __builtin__.print\n\n    @catch\n    def print(*args, force=False, end=\"\\n\", file=None, flush=False, **kwargs):  # pylint: disable=redefined-builtin\n        if self.rank == process or force:\n            if self.state.log:\n                logger.info(*args, **kwargs)\n            else:\n                builtin_print(*args, end=end, file=file, flush=flush, **kwargs)\n\n    __builtin__.print = print\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.init_tensorboard","title":"<code>init_tensorboard(*args, **kwargs)</code>","text":"<p>Set up Tensoraoard SummaryWriter.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@on_main_process\ndef init_tensorboard(self, *args, **kwargs) -&gt; None:\n    r\"\"\"\n    Set up Tensoraoard SummaryWriter.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.json","title":"<code>json(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner State to json file.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\ndef json(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n    r\"\"\"\n    Dump Runner State to json file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        return self.state.json(file, *args, **kwargs)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.jsons","title":"<code>jsons(*args, **kwargs)</code>","text":"<p>Dump Runner State to json string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def jsons(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner State to json string.\n    \"\"\"\n\n    return self.state.jsons(*args, **kwargs)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.load","title":"<code>load(file, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Load any file with supported extensions.</p> <p><code>Runner.load</code> is identical to <code>dl.load</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@staticmethod\ndef load(file: PathStr, *args, **kwargs) -&gt; Any:\n    r\"\"\"\n    Load any file with supported extensions.\n\n    `Runner.load` is identical to `dl.load`.\n    \"\"\"\n\n    return load(file, *args, **kwargs)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.load_checkpoint","title":"<code>load_checkpoint(checkpoint=None, auto_resume=None, override_state=False, *args, **kwargs)</code>","text":"<p>Load info from checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Mapping | bytes | str | PathLike | None</code> <p>Checkpoint (or its path) to load. Defaults to <code>self.state.checkpoint</code>.</p> <code>None</code> <code>auto_resume</code> <code>bool | None</code> <p>Automatically resume from latest checkpoint if exists. Defaults to <code>False</code>. If is <code>True</code> and <code>checkpoint</code> is None, will set it to <code>self.checkpoint_dir/latest.pth</code>.</p> <code>None</code> <code>override_state</code> <code>bool</code> <p>If True, override runner state with checkpoint state. Defaults to <code>False</code>.</p> <code>False</code> <code>*args</code> <p>Additional arguments to pass to <code>self.load</code>.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>self.load</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>checkpoint</code> does not exists.</p> See Also <p><code>from_checkpoint</code>: Build runner from checkpoint. <code>load_pretrained</code>: Load parameters from pretrained checkpoint.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def load_checkpoint(\n    self,\n    checkpoint: Mapping | bytes | str | os.PathLike | None = None,\n    auto_resume: bool | None = None,\n    override_state: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Load info from checkpoint.\n\n    Args:\n        checkpoint: Checkpoint (or its path) to load.\n            Defaults to `self.state.checkpoint`.\n        auto_resume: Automatically resume from latest checkpoint if exists.\n            Defaults to `False`.\n            If is `True` and `checkpoint` is None, will set it to `self.checkpoint_dir/latest.pth`.\n        override_state: If True, override runner state with checkpoint state.\n            Defaults to `False`.\n        *args: Additional arguments to pass to `self.load`.\n        **kwargs: Additional keyword arguments to pass to `self.load`.\n\n    Raises:\n        FileNotFoundError: If `checkpoint` does not exists.\n\n    See Also:\n        [`from_checkpoint`][danling.BaseRunner.from_checkpoint]: Build runner from checkpoint.\n        [`load_pretrained`][danling.BaseRunner.load_pretrained]: Load parameters from pretrained checkpoint.\n    \"\"\"\n\n    checkpoint = checkpoint if checkpoint is not None else self.state.get(\"checkpoint\")\n    auto_resume = auto_resume if auto_resume is not None else self.state.get(\"auto_resume\", False)\n\n    # TODO: Support loading checkpoints in other format\n    if checkpoint is not None:\n        if auto_resume:\n            warn(\n                \"latest checkpoint is preempted by value specified in checkpoint\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        if isinstance(checkpoint, (bytes, str, os.PathLike)):\n            if not os.path.exists(checkpoint):\n                raise FileNotFoundError(f\"checkpoint is set to {checkpoint!r} but does not exist\")\n            self.state.checkpoint = checkpoint\n            ckpt = self.load(checkpoint, *args, **kwargs)\n        elif isinstance(checkpoint, Mapping):\n            ckpt = checkpoint\n        else:\n            raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint\")\n    elif auto_resume:\n        checkpoint = os.path.join(self.checkpoint_dir, \"latest.pth\")\n        if os.path.exists(checkpoint):\n            self.state.checkpoint = checkpoint\n            ckpt = self.load(checkpoint, *args, **kwargs)\n        else:\n            warn(\"latest checkpoint does not exits\", category=RuntimeWarning, stacklevel=2)\n            return\n    else:\n        raise ValueError(\"checkpoint is not specified and auto_resume is not set to True\")\n\n    # TODO: Wrap state_dict in a dataclass\n    self.state.merge(ckpt[\"runner\"], overwrite=override_state)\n    if self.model is not None and \"model\" in ckpt:\n        model = self.unwrap_model(self.model)\n        model.load_state_dict(ckpt[\"model\"])\n    if self.optimizer is not None and \"optimizer\" in ckpt:\n        self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n    if self.scheduler is not None and \"scheduler\" in ckpt:\n        self.scheduler.load_state_dict(ckpt[\"scheduler\"])\n    self.state.iter_begin = self.state.iters\n    self.state.step_begin = self.state.steps\n    self.state.epoch_begin = self.state.epochs\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.load_pretrained","title":"<code>load_pretrained(checkpoint=None, *args, **kwargs)</code>","text":"<p>Load parameters from pretrained checkpoint.</p> <p>This method only loads the model weights.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Mapping | bytes | str | PathLike | None</code> <p>Pretrained checkpoint (or its path) to load. Defaults to <code>self.state.pretrained</code>.</p> <code>None</code> <code>*args</code> <p>Additional arguments to pass to <code>self.load</code>.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>self.load</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>checkpoint</code> does not exists.</p> See Also <p><code>load_checkpoint</code>: Load info from checkpoint.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def load_pretrained(self, checkpoint: Mapping | bytes | str | os.PathLike | None = None, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Load parameters from pretrained checkpoint.\n\n    This method only loads the model weights.\n\n    Args:\n        checkpoint: Pretrained checkpoint (or its path) to load.\n            Defaults to `self.state.pretrained`.\n        *args: Additional arguments to pass to `self.load`.\n        **kwargs: Additional keyword arguments to pass to `self.load`.\n\n    Raises:\n        FileNotFoundError: If `checkpoint` does not exists.\n\n    See Also:\n        [`load_checkpoint`][danling.BaseRunner.load_checkpoint]: Load info from checkpoint.\n    \"\"\"\n\n    # TODO: Support loading checkpoints in other format\n    checkpoint = checkpoint if checkpoint is not None else self.state.get(\"pretrained\")\n    if checkpoint is None:\n        raise ValueError(\"pretrained is not specified\")\n    if isinstance(checkpoint, (bytes, str, os.PathLike)):\n        if not os.path.exists(checkpoint):\n            raise FileNotFoundError(f\"pretrained is set to {checkpoint!r} but does not exist\")\n        ckpt = self.load(checkpoint, *args, **kwargs)\n    elif isinstance(checkpoint, Mapping):\n        ckpt = checkpoint\n    else:\n        raise ValueError(f\"pretrained is set to {checkpoint!r} but is not a valid checkpoint\")\n    if self.model is not None and \"model\" in ckpt:\n        model = self.unwrap_model(self.model)\n        model.load_state_dict(ckpt[\"model\"])\n    else:\n        raise ValueError(f\"Unable to find model weights in {checkpoint!r}\")\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.print_result","title":"<code>print_result()</code>","text":"<p>Print latest and best result.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def print_result(self) -&gt; None:\n    r\"\"\"\n    Print latest and best result.\n    \"\"\"\n\n    print(f\"latest result: {self.latest_result}\")\n    print(f\"best result: {self.best_result}\")\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.save","title":"<code>save(obj, file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Save any file with supported extensions.</p> <p><code>Runner.save</code> internally calls <code>dl.save</code>, but with additional arguments to allow it save only on the main process. Moreover, any error raised by <code>Runner.save</code> will be caught and logged.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\ndef save(self, obj: Any, file: PathStr, main_process_only: bool = True, *args, **kwargs) -&gt; File:\n    r\"\"\"\n    Save any file with supported extensions.\n\n    `Runner.save` internally calls `dl.save`,\n    but with additional arguments to allow it save only on the main process.\n    Moreover, any error raised by `Runner.save` will be caught and logged.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        return save(obj, file, *args, **kwargs)\n    return file\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.save_checkpoint","title":"<code>save_checkpoint(epochs=None)</code>","text":"<p>Save checkpoint to <code>self.checkpoint_dir</code>.</p> <p>The checkpoint will be saved to <code>self.checkpoint_dir/latest.pth</code>.</p> <p>If <code>self.state.save_interval</code> is positive and <code>self.state.epochs + 1</code> is a multiple of <code>save_interval</code>, the checkpoint will also be copied to <code>self.checkpoint_dir/epoch-{self.state.epochs}.pth</code>.</p> <p>If <code>self.is_best</code> is <code>True</code>, the checkpoint will also be copied to <code>self.checkpoint_dir/best.pth</code>.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_checkpoint(self, epochs: int | None = None) -&gt; None:\n    r\"\"\"\n    Save checkpoint to `self.checkpoint_dir`.\n\n    The checkpoint will be saved to `self.checkpoint_dir/latest.pth`.\n\n    If `self.state.save_interval` is positive and `self.state.epochs + 1` is a multiple of `save_interval`,\n    the checkpoint will also be copied to `self.checkpoint_dir/epoch-{self.state.epochs}.pth`.\n\n    If `self.is_best` is `True`, the checkpoint will also be copied to `self.checkpoint_dir/best.pth`.\n    \"\"\"\n\n    epochs = epochs or self.state.epochs\n    save_interval = self.state.get(\"save_interval\", -1)\n    latest_path = os.path.join(self.checkpoint_dir, \"latest.pth\")\n    self.save(self.state_dict(), latest_path)\n    if save_interval &gt; 0 and (epochs + 1) % save_interval == 0:\n        save_path = os.path.join(self.checkpoint_dir, f\"epoch-{epochs}.pth\")\n        shutil.copy(latest_path, save_path)\n    if self.is_best:\n        best_path = os.path.join(self.checkpoint_dir, \"best.pth\")\n        shutil.copy(latest_path, best_path)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.save_result","title":"<code>save_result()</code>","text":"<p>Save result to <code>self.dir</code>.</p> <p>This method will save latest and best result to <code>self.dir/latest.json</code> and <code>self.dir/best.json</code> respectively.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\n@on_main_process\ndef save_result(self) -&gt; None:\n    r\"\"\"\n    Save result to `self.dir`.\n\n    This method will save latest and best result to\n    `self.dir/latest.json` and `self.dir/best.json` respectively.\n    \"\"\"\n\n    results_path = os.path.join(self.dir, \"results.json\")\n    self.save(\n        {\n            \"name\": self.name,\n            \"id\": self.id,\n            \"timestamp\": self.timestamp,\n            \"results\": self.results,\n        },\n        results_path,\n        indent=4,\n    )\n    ret = {\"name\": self.name, \"id\": self.id, \"timestamp\": self.timestamp}\n    result = self.latest_result\n    if isinstance(result, FlatDict):\n        result = result.dict()\n    # This is slower but ensure id is the first key\n    if result is not None:\n        ret.update(result)\n    latest_path = os.path.join(self.dir, \"latest.json\")\n    self.save(ret, latest_path, indent=4)\n    if self.is_best:\n        best_path = os.path.join(self.dir, \"best.json\")\n        shutil.copy(latest_path, best_path)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.scale_lr","title":"<code>scale_lr(lr, lr_scale_factor=None, batch_size_base=None)</code>","text":"<p>Scale learning rate according to linear scaling rule.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def scale_lr(\n    self,\n    lr: float,\n    lr_scale_factor: float | None = None,\n    batch_size_base: int | None = None,\n) -&gt; float:\n    r\"\"\"\n    Scale learning rate according to [linear scaling rule](https://arxiv.org/abs/1706.02677).\n    \"\"\"\n\n    if lr_scale_factor in self.state:\n        lr_scale_factor = self.state.lr_scale_factor\n\n    if lr_scale_factor is None:\n        if batch_size_base is None:\n            batch_size_base = getattr(self, \"batch_size_base\", None)\n            if batch_size_base is None:\n                raise ValueError(\"batch_size_base must be specified to auto scale lr\")\n        lr_scale_factor = self.batch_size_equivalent / batch_size_base\n    elif batch_size_base is not None:\n        warn(\n            \"batch_size_base will be ignored if lr_scale_factor is specified\", category=RuntimeWarning, stacklevel=2\n        )\n    lr = lr * lr_scale_factor\n    self.state.lr_scale_factor = lr_scale_factor\n    return lr\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.set_deterministic","title":"<code>set_deterministic()</code>","text":"<p>Set up deterministic.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_deterministic(self) -&gt; None:\n    r\"\"\"\n    Set up deterministic.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.set_seed","title":"<code>set_seed(seed=None, bias=None)</code>","text":"<p>Set up random seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Random seed to set. Defaults to <code>self.state.seed</code> (<code>config.seed</code>).</p> <code>None</code> <code>bias</code> <code>int | None</code> <p>Make the seed different for each processes.</p> <p>This avoids same data augmentation are applied on every processes.</p> <p>Defaults to <code>self.rank</code>.</p> <p>Set to <code>False</code> to disable this feature.</p> <code>None</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def set_seed(self, seed: int | None = None, bias: int | None = None) -&gt; None:\n    r\"\"\"\n    Set up random seed.\n\n    Args:\n        seed: Random seed to set.\n            Defaults to `self.state.seed` (`config.seed`).\n\n        bias: Make the seed different for each processes.\n\n            This avoids same data augmentation are applied on every processes.\n\n            Defaults to `self.rank`.\n\n            Set to `False` to disable this feature.\n    \"\"\"\n\n    seed = seed or self.state.seed\n    bias = bias or self.rank\n    if bias:\n        seed += bias\n    if np_random is not None:\n        np_random.seed(seed)\n    random.seed(seed)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.state_dict","title":"<code>state_dict(cls=dict)</code>","text":"<p>Return dict of all attributes for checkpoint.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def state_dict(self, cls: Callable = dict) -&gt; Mapping:\n    r\"\"\"\n    Return dict of all attributes for checkpoint.\n    \"\"\"\n\n    return cls(self.state)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.step","title":"<code>step(loss, batch_size=None, zero_grad=True)</code>","text":"<p>Backward loss and step optimizer &amp; scheduler.</p> <p>This method increment <code>self.state.steps</code>.</p> <p>This method also increment <code>self.state.iters</code> when <code>batch_size</code> is specified.</p> <p>Parameters:</p> Name Type Description Default <code>zero_grad</code> <code>bool</code> <p>Whether to zero the gradients.</p> <code>True</code> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def step(self, loss, batch_size: int | None = None, zero_grad: bool = True) -&gt; None:\n    r\"\"\"\n    Backward loss and step optimizer &amp; scheduler.\n\n    This method increment `self.state.steps`.\n\n    This method also increment `self.state.iters` when `batch_size` is specified.\n\n    Args:\n        zero_grad: Whether to zero the gradients.\n    \"\"\"\n\n    raise NotImplementedError\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.yaml","title":"<code>yaml(file, main_process_only=True, *args, **kwargs)</code>","text":"<p>Dump Runner State to yaml file.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>@catch\ndef yaml(self, file: File, main_process_only: bool = True, *args, **kwargs) -&gt; None:  # pylint: disable=R1710\n    r\"\"\"\n    Dump Runner State to yaml file.\n    \"\"\"\n\n    if main_process_only and self.is_main_process or not main_process_only:\n        return self.state.yaml(file, *args, **kwargs)\n</code></pre>"},{"location":"runner/base_runner/#danling.runner.BaseRunner.yamls","title":"<code>yamls(*args, **kwargs)</code>","text":"<p>Dump Runner State to yaml string.</p> Source code in <code>danling/runner/base_runner.py</code> Python<pre><code>def yamls(self, *args, **kwargs) -&gt; str:\n    r\"\"\"\n    Dump Runner State to yaml string.\n    \"\"\"\n\n    return self.state.yamls(*args, **kwargs)\n</code></pre>"},{"location":"runner/state/","title":"RunnerState","text":""},{"location":"runner/state/#danling.runner.state.RunnerState","title":"<code>RunnerState</code>","text":"<p>               Bases: <code>NestedDict</code></p> <p><code>RunnerState</code> is a <code>NestedDict</code> that contains all states of a <code>Runner</code>.</p> <p><code>RunnerState</code> is designed to store all critical information of a Run so that you can resume a run from a state and corresponding weights or even restart a run from a state.</p> <p><code>RunnerState</code> is also designed to be serialisable and hashable, so that you can save it to a file. <code>RunnerState</code> is saved in checkpoint together with weights by default.</p> <p>Since <code>RunnerState</code> is a <code>NestedDict</code>, you can access its attributes by <code>state[\"key\"]</code> or <code>state.key</code>.</p> <p>General:</p> Name Type Description <code>run_name</code> <code>str</code> <p>Defaults to <code>\"DanLing\"</code>.</p> <code>run_id</code> <code>str</code> <p>hex of <code>self.run_uuid</code>.</p> <code>run_uuid</code> <code>(UUID, property)</code> <p><code>uuid5(self.experiment_id, str(hash(self)))</code>.</p> <code>experiment_name</code> <code>str</code> <p>Defaults to <code>\"DanLing\"</code>.</p> <code>experiment_id</code> <code>str</code> <p>git hash of the current HEAD. Defaults to <code>\"xxxxxxxxxxxxxxxx\"</code> if Runner not under a git repo or git/gitpython not installed.</p> <code>experiment_uuid</code> <code>(UUID, property)</code> <p>UUID of <code>self.experiment_id</code>. Defaults to <code>UUID('78787878-7878-7878-7878-787878787878')</code> if Runner not under a git repo or git/gitpython not installed.</p> <p>Reproducibility:</p> Name Type Description <code>seed</code> <code>int</code> <p>Defaults to <code>randint(0, 2**32 - 1)</code>.</p> <code>deterministic</code> <code>bool</code> <p>Ensure deterministic operations. Defaults to <code>False</code>.</p> <p>Progress:</p> Name Type Description <code>iters</code> <code>int</code> <p>The number of data samples processed. equals to <code>steps</code> when <code>batch_size = 1</code>.</p> <code>steps</code> <code>int</code> <p>The number of <code>step</code> calls.</p> <code>epochs</code> <code>int</code> <p>The number of complete passes over the datasets.</p> <code>iter_end</code> <code>int</code> <p>End running iters. Note that <code>step_end</code> not initialised since this variable may not apply to some Runners.</p> <code>step_end</code> <code>int</code> <p>End running steps. Note that <code>step_end</code> not initialised since this variable may not apply to some Runners.</p> <code>epoch_end</code> <code>int</code> <p>End running epochs. Note that <code>epoch_end</code> not initialised since this variable may not apply to some Runners.</p> <p>In general you should only use one of <code>iter_end</code>, <code>step_end</code>, <code>epoch_end</code> to indicate the length of running.</p> <p>IO:</p> Name Type Description <code>project_root</code> <code>str</code> <p>The root directory for all experiments. Defaults to <code>\"experiments\"</code>.</p> <p><code>project_root</code> is the root directory of all Experiments, and should be consistent across the Project.</p> <p><code>dir</code> is the directory of a certain Run.</p> <p>There is no attributes/properties for Group and Experiment.</p> <p><code>checkpoint_dir_name</code> is relative to <code>dir</code>, and is passed to generate <code>checkpoint_dir</code> (<code>checkpoint_dir = os.path.join(dir, checkpoint_dir_name)</code>). In practice, <code>checkpoint_dir_name</code> is rarely called.</p> <p>logging:</p> Name Type Description <code>log</code> <code>bool</code> <p>Whether to log the outputs. Defaults to <code>True</code>.</p> <code>tensorboard</code> <code>bool</code> <p>Whether to use <code>tensorboard</code>. Defaults to <code>False</code>.</p> <code>log_interval</code> <code>int</code> <p>Interval of printing logs. Defaults to <code>None</code>, print logs every 1/10 of the longest split.</p> <code>save_interval</code> <code>int</code> <p>Interval of saving intermediate checkpoints. Defaults to <code>None</code>, never save checkpoints. If &lt;= 0, save only the latest and the best checkpoints.</p> Notes <p><code>RunnerState</code> is a <code>NestedDict</code>, so you can access its attributes by <code>state[\"name\"]</code> or <code>state.name</code>.</p> See Also <p><code>BaseRunner</code>: The base runner class.</p> Source code in <code>danling/runner/state.py</code> Python<pre><code>class RunnerState(NestedDict):  # pylint: disable=too-many-instance-attributes\n    r\"\"\"\n    `RunnerState` is a `NestedDict` that contains all states of a `Runner`.\n\n    `RunnerState` is designed to store all critical information of a Run so that you can resume a run\n    from a state and corresponding weights or even restart a run from a state.\n\n    `RunnerState` is also designed to be serialisable and hashable, so that you can save it to a file.\n    `RunnerState` is saved in checkpoint together with weights by default.\n\n    Since `RunnerState` is a [`NestedDict`][chanfig.NestedDict], you can access its attributes by\n    `state[\"key\"]` or `state.key`.\n\n    Attributes: General:\n        run_name (str): Defaults to `\"DanLing\"`.\n        run_id (str): hex of `self.run_uuid`.\n        run_uuid (UUID, property): `uuid5(self.experiment_id, str(hash(self)))`.\n        experiment_name (str): Defaults to `\"DanLing\"`.\n        experiment_id (str): git hash of the current HEAD.\n            Defaults to `\"xxxxxxxxxxxxxxxx\"` if Runner not under a git repo or git/gitpython not installed.\n        experiment_uuid (UUID, property): UUID of `self.experiment_id`.\n            Defaults to `UUID('78787878-7878-7878-7878-787878787878')`\n            if Runner not under a git repo or git/gitpython not installed.\n\n    Attributes: Reproducibility:\n        seed (int): Defaults to `randint(0, 2**32 - 1)`.\n        deterministic (bool): Ensure [deterministic](https://pytorch.org/docs/stable/notes/randomness.html) operations.\n            Defaults to `False`.\n\n    Attributes: Progress:\n        iters (int): The number of data samples processed.\n            equals to `steps` when `batch_size = 1`.\n        steps (int): The number of `step` calls.\n        epochs (int): The number of complete passes over the datasets.\n        iter_end (int): End running iters.\n            Note that `step_end` not initialised since this variable may not apply to some Runners.\n        step_end (int): End running steps.\n            Note that `step_end` not initialised since this variable may not apply to some Runners.\n        epoch_end (int): End running epochs.\n            Note that `epoch_end` not initialised since this variable may not apply to some Runners.\n\n    In general you should only use one of `iter_end`, `step_end`, `epoch_end` to indicate the length of running.\n\n    Attributes: IO:\n        project_root (str): The root directory for all experiments.\n            Defaults to `\"experiments\"`.\n\n    `project_root` is the root directory of all **Experiments**, and should be consistent across the **Project**.\n\n    `dir` is the directory of a certain **Run**.\n\n    There is no attributes/properties for **Group** and **Experiment**.\n\n    `checkpoint_dir_name` is relative to `dir`, and is passed to generate `checkpoint_dir`\n    (`checkpoint_dir = os.path.join(dir, checkpoint_dir_name)`).\n    In practice, `checkpoint_dir_name` is rarely called.\n\n    Attributes: logging:\n        log (bool): Whether to log the outputs.\n            Defaults to `True`.\n        tensorboard (bool): Whether to use `tensorboard`.\n            Defaults to `False`.\n        log_interval (int): Interval of printing logs.\n            Defaults to `None`, print logs every 1/10 of the longest split.\n        save_interval (int): Interval of saving intermediate checkpoints.\n            Defaults to `None`, never save checkpoints.\n            If &lt;= 0, save only the latest and the best checkpoints.\n\n    Notes:\n        `RunnerState` is a `NestedDict`, so you can access its attributes by `state[\"name\"]` or `state.name`.\n\n    See Also:\n        [`BaseRunner`][danling.runner.BaseRunner]: The base runner class.\n    \"\"\"\n\n    # DO NOT set default value in class, as they won't be stored in `__dict__`.\n\n    run_name: str = defaults.DEFAULT_RUN_NAME\n    run_id: str\n    experiment_name: str = defaults.DEFAULT_EXPERIMENT_NAME\n    experiment_id: str\n\n    seed: int\n    deterministic: bool = False\n\n    iters: int = 0\n    steps: int = 0\n    epochs: int = 0\n    iter_begin: int = 0\n    step_begin: int = 0\n    epoch_begin: int = 0\n    iter_end: Optional[int] = None\n    step_end: Optional[int] = None\n    epoch_end: Optional[int] = None\n\n    score_split: Optional[str] = None\n    score_name: str = \"loss\"\n\n    project_root: str = \"experiments\"\n    checkpoint_dir_name: str = \"checkpoints\"\n    log: bool = True\n    tensorboard: bool = False\n    log_interval: Optional[int] = None\n    save_interval: Optional[int] = None\n\n    distributed: Optional[bool] = None\n    dist_backend: Optional[str] = None\n    init_method: Optional[str] = None\n    master_addr: Optional[str] = None\n    master_port: Optional[int] = None\n\n    def __init__(self, *args, **kwargs):\n        for k, v in self.__class__.__dict__.items():\n            if not (k.startswith(\"__\") and k.endswith(\"__\")) and (not (isinstance(v, property) or callable(v))):\n                self.set(k, v)\n        self.seed = randint(0, 2**32 - 1)\n        super().__init__(*args, **kwargs)\n        if \"experiment_id\" not in self:\n            self.experiment_id = get_git_hash() or defaults.DEFAULT_EXPERIMENT_ID\n        if \"run_id\" not in self:\n            self.run_id = self.run_uuid.hex\n        self.setattr(\"ignored_keys_in_hash\", defaults.DEFAULT_IGNORED_KEYS_IN_HASH)\n\n    @property\n    def experiment_uuid(self) -&gt; UUID:\n        r\"\"\"\n        UUID of the experiment.\n        \"\"\"\n\n        return UUID(bytes=bytes(self.experiment_id.ljust(16, \"x\")[:16], encoding=\"ascii\"))\n\n    @property\n    def run_uuid(self) -&gt; UUID:\n        r\"\"\"\n        UUID of the run.\n        \"\"\"\n\n        ignored_keys_in_hash = self.getattr(\"ignored_keys_in_hash\", defaults.DEFAULT_IGNORED_KEYS_IN_HASH)\n        state: NestedDict = NestedDict({k: v for k, v in self.dict().items() if k not in ignored_keys_in_hash})\n        return uuid5(self.experiment_uuid, state.yamls())\n\n    def __hash__(self) -&gt; int:\n        return int(self.run_uuid.hex, 16)\n</code></pre>"},{"location":"runner/state/#danling.runner.state.RunnerState.experiment_uuid","title":"<code>experiment_uuid: UUID</code>  <code>property</code>","text":"<p>UUID of the experiment.</p>"},{"location":"runner/state/#danling.runner.state.RunnerState.run_uuid","title":"<code>run_uuid: UUID</code>  <code>property</code>","text":"<p>UUID of the run.</p>"},{"location":"runner/utils/","title":"Utilities","text":""},{"location":"runner/utils/#danling.runner.utils.RunnerMode","title":"<code>RunnerMode</code>","text":"<p>               Bases: <code>LowercaseStrEnum</code></p> <p><code>RunnerMode</code> is an enumeration of running modes.</p> <p>Attributes:</p> Name Type Description <code>train</code> <p>Training mode.</p> <code>eval</code> <p>Evaluation mode.</p> <code>inf</code> <p>Inference mode.</p> Source code in <code>danling/runner/utils.py</code> Python<pre><code>class RunnerMode(StrEnum):  # pylint: disable=too-few-public-methods\n    r\"\"\"\n    `RunnerMode` is an enumeration of running modes.\n\n    Attributes:\n        train: Training mode.\n        eval: Evaluation mode.\n        inf: Inference mode.\n    \"\"\"\n\n    train = auto()\n    eval = auto()\n    inf = auto()\n</code></pre>"},{"location":"runner/utils/#danling.runner.utils.on_local_main_process","title":"<code>on_local_main_process(func)</code>","text":"<p>Decorator to run func only on local main process.</p> Source code in <code>danling/runner/utils.py</code> Python<pre><code>def on_local_main_process(func):\n    \"\"\"\n    Decorator to run func only on local main process.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(self, *args, **kwargs) -&gt; Any | None:\n        if self.is_local_main_process or not self.distributed:\n            return func(self, *args, **kwargs)\n        return None\n\n    return wrapper\n</code></pre>"},{"location":"runner/utils/#danling.runner.utils.on_main_process","title":"<code>on_main_process(func)</code>","text":"<p>Decorator to run func only on main process.</p> Source code in <code>danling/runner/utils.py</code> Python<pre><code>def on_main_process(func):\n    \"\"\"\n    Decorator to run func only on main process.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(self, *args, **kwargs) -&gt; Any | None:\n        if self.is_main_process or not self.distributed:\n            return func(self, *args, **kwargs)\n        return None\n\n    return wrapper\n</code></pre>"},{"location":"tensors/nested_tensor/","title":"NestedTensor","text":"<p>Wrap an iterable of tensors into a single tensor with a mask.</p> <p>In sequence to sequence tasks, elements of a batch are usually not of the same length. This made it tricky to use a single tensor to represent a batch of sequences.</p> <p><code>NestedTensor</code> allows to store a sequence of tensors of different lengths in a single object. It also provides a mask that can be used to retrieve the original sequence of tensors.</p> <p>When calling <code>__getitem__(arg)</code> on a <code>NestedTensor</code>, it has two return type: 1. if arg is <code>int</code> or <code>slice</code>, returns a tuple of two <code>tensor</code>s, representing data and padding mask. 2. if arg is a <code>tuple</code>, return a new <code>NestedTensor</code> with specified shape.</p> <p>Attributes:</p> Name Type Description <code>_storage</code> <p>The sequence of tensors.</p> <code>tensor</code> <code>Tensor</code> <p>padded tensor.</p> <code>mask</code> <code>Tensor</code> <p>mask tensor.</p> <code>concat</code> <code>Tensor</code> <p>concatenated tensor.</p> <code>batch_first</code> <code>bool</code> <p>Whether the first dimension of the tensors is the batch dimension.</p> <p>If <code>True</code>, the first dimension is the batch dimension, i.e., <code>B, N, *</code>.</p> <p>If <code>False</code>, the first dimension is the sequence dimension, i.e., <code>N, B, *</code></p> <code>padding_value</code> <code>SupportsFloat</code> <p>The padding value used to in padded tensor.</p> <code>mask_value</code> <code>bool</code> <p>The mask value used in mask tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>Iterable[Tensor]</code> <code>()</code> <code>batch_first</code> <code>bool</code> <code>True</code> <code>padding_value</code> <code>SupportsFloat</code> <code>0.0</code> <code>mask_value</code> <code>bool</code> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensors</code> is not an iterable.</p> <code>ValueError</code> <p>If <code>tensors</code> is empty.</p> Notes <p>We have rewritten the <code>__getattr__</code> function to support as much native tensor operations as possible. However, not all operations are tested.</p> <p>Please file an issue if you find any bugs.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor(torch.tensor([1, 2, 3]), torch.tensor([4, 5]))\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n&gt;&gt;&gt; nested_tensor.dtype\ntorch.int64\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n&gt;&gt;&gt; nested_tensor.concat\ntensor([1, 2, 3, 4, 5])\n&gt;&gt;&gt; nested_tensor.to(torch.float).tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]])\n&gt;&gt;&gt; nested_tensor.half().tensor\ntensor([[1., 2., 3.],\n        [4., 5., 0.]], dtype=torch.float16)\n&gt;&gt;&gt; nested_tensor[:]\n(tensor([[1, 2, 3],\n        [4, 5, 0]]), tensor([[ True,  True,  True],\n        [ True,  True, False]]))\n&gt;&gt;&gt; nested_tensor[1]\n(tensor([4, 5]), tensor([True, True]))\n&gt;&gt;&gt; nested_tensor[:, 1:]\nNestedTensor([[2, 3],\n        [5, 0]])\n&gt;&gt;&gt; nested_tensor.tolist()\n[[1, 2, 3], [4, 5]]\n&gt;&gt;&gt; NestedTensor(*[[1, 2, 3], [4, 5]])\nNestedTensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class NestedTensor:\n    r\"\"\"\n    Wrap an iterable of tensors into a single tensor with a mask.\n\n    In sequence to sequence tasks, elements of a batch are usually not of the same length.\n    This made it tricky to use a single tensor to represent a batch of sequences.\n\n    `NestedTensor` allows to store a sequence of tensors of different lengths in a single object.\n    It also provides a mask that can be used to retrieve the original sequence of tensors.\n\n    When calling `__getitem__(arg)` on a `NestedTensor`, it has two return type:\n    1. if arg is `int` or `slice`, returns a tuple of two `tensor`s, representing data and padding mask.\n    2. if arg is a `tuple`, return a new `NestedTensor` with specified shape.\n\n    Attributes:\n        _storage: The sequence of tensors.\n        tensor: padded tensor.\n        mask: mask tensor.\n        concat: concatenated tensor.\n        batch_first:  Whether the first dimension of the tensors is the batch dimension.\n\n            If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.\n\n            If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`\n        padding_value: The padding value used to in padded tensor.\n        mask_value: The mask value used in mask tensor.\n\n    Args:\n        tensors:\n        batch_first:\n        padding_value:\n        mask_value:\n\n    Raises:\n        ValueError: If `tensors` is not an iterable.\n        ValueError: If `tensors` is empty.\n\n    Notes:\n        We have rewritten the `__getattr__` function to support as much native tensor operations as possible.\n        However, not all operations are tested.\n\n        Please file an issue if you find any bugs.\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor(torch.tensor([1, 2, 3]), torch.tensor([4, 5]))\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.device\n        device(type='cpu')\n        &gt;&gt;&gt; nested_tensor.dtype\n        torch.int64\n        &gt;&gt;&gt; nested_tensor.tensor\n        tensor([[1, 2, 3],\n                [4, 5, 0]])\n        &gt;&gt;&gt; nested_tensor.mask\n        tensor([[ True,  True,  True],\n                [ True,  True, False]])\n        &gt;&gt;&gt; nested_tensor.concat\n        tensor([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; nested_tensor.to(torch.float).tensor\n        tensor([[1., 2., 3.],\n                [4., 5., 0.]])\n        &gt;&gt;&gt; nested_tensor.half().tensor\n        tensor([[1., 2., 3.],\n                [4., 5., 0.]], dtype=torch.float16)\n        &gt;&gt;&gt; nested_tensor[:]\n        (tensor([[1, 2, 3],\n                [4, 5, 0]]), tensor([[ True,  True,  True],\n                [ True,  True, False]]))\n        &gt;&gt;&gt; nested_tensor[1]\n        (tensor([4, 5]), tensor([True, True]))\n        &gt;&gt;&gt; nested_tensor[:, 1:]\n        NestedTensor([[2, 3],\n                [5, 0]])\n        &gt;&gt;&gt; nested_tensor.tolist()\n        [[1, 2, 3], [4, 5]]\n        &gt;&gt;&gt; NestedTensor(*[[1, 2, 3], [4, 5]])\n        NestedTensor([[1, 2, 3],\n                [4, 5, 0]])\n    \"\"\"\n\n    __storage: Sequence[Tensor]\n    batch_first: bool = True\n    padding_value: SupportsFloat = 0.0\n    mask_value: bool = False\n\n    def __init__(\n        self,\n        *tensors: Iterable[Tensor],\n        batch_first: bool = True,\n        padding_value: SupportsFloat = 0.0,\n        mask_value: bool = False,\n    ) -&gt; None:\n        if len(tensors) == 1 and isinstance(tensors, Sequence):\n            tensors = tensors[0]  # type: ignore\n        self._storage = tensors\n        self.batch_first = batch_first\n        self.padding_value = padding_value\n        self.mask_value = mask_value\n\n    @property\n    def _storage(self):\n        return self.__storage\n\n    @_storage.setter\n    def _storage(self, tensors: Sequence):\n        if not isinstance(tensors, Iterable):\n            raise ValueError(f\"tensors must be an Iterable, bug got {type(tensors)}.\")\n        tensors = list(tensors)\n        if len(tensors) == 0:\n            raise ValueError(\"tensors must be a non-empty Iterable.\")\n        if not isinstance(tensors[0], Tensor):\n            tensors = [tensor(t) for t in tensors]\n        self.__storage = tensors\n\n    def storage(self):\n        return self._storage\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Return a single tensor by padding all the tensors.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.tensor\n            tensor([[1, 2, 3],\n                    [4, 5, 0]])\n        \"\"\"\n\n        return self._tensor(tuple(self._storage), self.batch_first, self.padding_value)\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Padding mask of `tensor`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.mask\n            tensor([[ True,  True,  True],\n                    [ True,  True, False]])\n        \"\"\"\n\n        return self._mask(tuple(self._storage), self.batch_first, self.mask_value)\n\n    @property\n    def concat(self) -&gt; Tensor:\n        r\"\"\"\n        Concat `tensor` in padding dim.\n\n        This is particularly useful when calculating loss or passing `Linear` to avoid unnecessary computation.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 8), torch.randn(11, 8)])\n            &gt;&gt;&gt; nested_tensor.concat.shape\n            torch.Size([20, 8])\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 9, 8), torch.randn(11, 11, 8)])\n            &gt;&gt;&gt; nested_tensor.concat.shape\n            torch.Size([202, 8])\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 9, 8, 6), torch.randn(11, 11, 8, 6)])\n            &gt;&gt;&gt; nested_tensor.concat.shape\n            torch.Size([202, 8, 6])\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 9, 8, 7), torch.randn(11, 11, 8, 6)])\n            &gt;&gt;&gt; nested_tensor.concat.shape\n            torch.Size([1293, 8])\n        \"\"\"\n        shape = list(self.size())  # type: ignore[arg-type]\n        shape = shape[1:] if self.batch_first else shape[0] + shape[2:]\n        elem = self._storage[0]\n        if elem.shape == shape:\n            return torch.cat(self._storage, dim=1 if self.batch_first else 0)\n\n        static_dims = set(range(len(shape)))\n        for i, s in enumerate(shape):\n            if not all(t.shape[i] == s for t in self._storage):\n                shape[i] = -1\n                static_dims.remove(i)\n        target_shape = [-1] + [s for s in shape if s != -1]\n        storage = [i.view(target_shape) for i in self._storage]\n        return torch.cat(storage, dim=0 if self.batch_first else 1)\n\n    @classmethod\n    def from_tensor_mask(cls, tensor: Tensor, mask: Tensor):\n        r\"\"\"\n        Build a `NestedTensor` object from a padded `Tensor` and corresponding mask `Tensor`.\n\n        Args:\n            tensor: Padded Tensor.\n            mask: Tensor Mask.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; padded_tensor = torch.tensor([[1, 2, 3, 0, 0],\n            ...                                [4, 5, 0, 0, 0],\n            ...                                [6, 7, 8, 9, 0]])\n            &gt;&gt;&gt; mask_tensor = torch.tensor([[1, 1, 1, 0, 0],\n            ...                             [1, 1, 0, 0, 0],\n            ...                             [1, 1, 1, 1, 0]])\n            &gt;&gt;&gt; nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)\n            &gt;&gt;&gt; nested_tensor\n            NestedTensor([[1, 2, 3, 0],\n                    [4, 5, 0, 0],\n                    [6, 7, 8, 9]])\n        \"\"\"\n\n        if mask.ndim == 2:\n            return cls(t[slice(0, m.sum())] for t, m in zip(tensor, mask))\n        return cls(\n            t[[slice(0, (m.sum(dim=dim) &gt; 0).sum().item()) for dim in reversed(range(m.dim()))]]\n            for t, m in zip(tensor, mask)\n        )\n\n    def nested_like(self, tensor: Tensor, strict: bool = True) -&gt; NestedTensor:\n        r\"\"\"\n        Create a new `NestedTensor` from a `Tensor`.\n        The newly created `NestedTensor` will have the same shape as current `NestedTensor`.\n\n        Args:\n            tensor: The tensor to be converted to `NestedTensor`.\n            strict: Check if the shape of `tensor` is the same as the current `NestedTensor`.\n\n        Returns:\n            (NestedTensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; (nested_tensor == nested_tensor.nested_like(nested_tensor)).all()\n            tensor(True)\n            &gt;&gt;&gt; tensor = nested_tensor.tensor\n            &gt;&gt;&gt; (nested_tensor == nested_tensor.nested_like(tensor)).all()\n            tensor(True)\n            &gt;&gt;&gt; f = nested_tensor.nested_like(torch.randn(2, 2))\n            Traceback (most recent call last):\n            ValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])\n            &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(2, 2), False)\n            &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(3, 3), False)\n            Traceback (most recent call last):\n            ValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\n        \"\"\"  # noqa: E501\n\n        if isinstance(tensor, NestedTensor):\n            return tensor.clone()\n\n        if strict and self.shape != tensor.shape:\n            raise ValueError(\n                f\"The shape of NestedTensor and input tensor does not match, {self.shape} != {tensor.shape}\"\n            )\n        if self.size(0) != tensor.size(0):\n            raise ValueError(\n                f\"The batch size of NestedTensor and input tensor does not match, {self.size(0)} != {tensor.size(0)}\"\n            )\n        return NestedTensor([o[tuple(slice(0, dim) for dim in t.shape)] for t, o in zip(self._storage, tensor)])\n\n    @property\n    def device(self) -&gt; torch.device:\n        r\"\"\"\n        Device of the NestedTensor.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.device\n            device(type='cpu')\n        \"\"\"\n\n        return self._device(tuple(self._storage))\n\n    @property\n    def shape(self) -&gt; torch.Size | int:\n        r\"\"\"\n        Alias for `size()`.\n        \"\"\"\n\n        return self.size()\n\n    @property\n    def ndim(self) -&gt; int:\n        r\"\"\"\n        Alias for `dim()`.\n        \"\"\"\n\n        return self.dim()\n\n    def size(self, dim: int | None = None) -&gt; torch.Size | int:\n        r\"\"\"\n        Returns the size of the self `NestedTensor`.\n\n        Args:\n            dim: If not specified, the returned value is a `torch.Size`, a subclass of `tuple`.\n                If specified, returns an `int` holding the size of that dimension.\n                Defaults to `None`.\n\n        Returns:\n            (torch.Size | int):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.size()\n            torch.Size([2, 3])\n            &gt;&gt;&gt; nested_tensor.size(0)\n            2\n            &gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n            &gt;&gt;&gt; nested_tensor.shape\n            torch.Size([2, 4])\n            &gt;&gt;&gt; nested_tensor.size(1)\n            4\n        \"\"\"\n\n        return self._size(tuple(self._storage), dim, self.batch_first)\n\n    def dim(self) -&gt; int:\n        r\"\"\"\n        Number of dimension of the NestedTensor.\n\n        Returns:\n            (int):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.dim()\n            2\n            &gt;&gt;&gt; nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))\n            &gt;&gt;&gt; nested_tensor.ndim\n            2\n        \"\"\"\n\n        return self._dim(tuple(self._storage))\n\n    def tolist(self) -&gt; list:\n        r\"\"\"\n        Convert a NestedTensor to a list of lists of values.\n\n        Returns:\n            (list):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.tolist()\n            [[1, 2, 3], [4, 5]]\n        \"\"\"\n\n        return [t.tolist() for t in self._storage]\n\n    def all(self, dim: int | None = None, keepdim: bool = False) -&gt; bool | Tensor | NestedTensor:\n        r\"\"\"\n        Tests if all elements in NestedTensor evaluate to True.\n\n        Returns:\n            (bool | Tensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.ones(2, 4, dtype=torch.bool), torch.ones(3, 5, dtype=torch.bool)])\n            &gt;&gt;&gt; nested_tensor.all()\n            tensor(True)\n            &gt;&gt;&gt; nested_tensor.all(dim=0)\n            tensor([True, True])\n            &gt;&gt;&gt; nested_tensor.all(dim=0, keepdim=True)\n            tensor([[True, True]])\n            &gt;&gt;&gt; nested_tensor.all(dim=1)\n            NestedTensor([[ True,  True,  True,  True, False],\n                    [ True,  True,  True,  True,  True]])\n            &gt;&gt;&gt; nested_tensor.all(dim=1, keepdim=True)\n            NestedTensor([[[ True,  True,  True,  True, False]],\n            &lt;BLANKLINE&gt;\n                    [[ True,  True,  True,  True,  True]]])\n            &gt;&gt;&gt; nested_tensor.batch_first = False\n            &gt;&gt;&gt; nested_tensor.all(dim=1)\n            tensor([True, True])\n            &gt;&gt;&gt; nested_tensor.batch_first = False\n            &gt;&gt;&gt; nested_tensor.all(dim=0)\n            NestedTensor([[ True,  True,  True,  True, False],\n                    [ True,  True,  True,  True,  True]])\n            &gt;&gt;&gt; nested_tensor.all(dim=1)\n            tensor([True, True])\n        \"\"\"\n\n        if dim is None:\n            return torch.tensor(all(i.all() for i in self._storage))\n        if (self.batch_first and dim == 0) or (not self.batch_first and dim == 1):\n            if keepdim:\n                return torch.tensor([i.all() for i in self._storage]).unsqueeze(0 if self.batch_first else 1)\n            return torch.tensor([i.all() for i in self._storage])\n        if self.batch_first or dim != 0:\n            dim -= 1\n        return NestedTensor([i.all(dim=dim, keepdim=keepdim) for i in self._storage])\n\n    def where(self, condition: Tensor | NestedTensor, other: Tensor | NestedTensor | SupportsFloat) -&gt; NestedTensor:\n        r\"\"\"\n        Return a NestedTensor of elements selected from either self or other, depending on condition.\n\n        Returns:\n            (NestedTensor):\n\n        Examples:\n            &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n            &gt;&gt;&gt; nested_tensor.where(nested_tensor &gt; 2, torch.tensor([[6, 5, 4], [3, 2, 1]]))\n            NestedTensor([[6, 5, 3],\n                    [4, 5, 0]])\n            &gt;&gt;&gt; nested_tensor.where(nested_tensor &gt; 2, NestedTensor([[6, 5, 4], [3, 2]]))\n            NestedTensor([[6, 5, 3],\n                    [4, 5, 0]])\n            &gt;&gt;&gt; nested_tensor.where(torch.tensor(True), NestedTensor([[6, 5, 4], [3, 2]]))\n            NestedTensor([[1, 2, 3],\n                    [4, 5, 0]])\n        \"\"\"\n\n        if isinstance(condition, Tensor) and self.shape == condition.shape:\n            condition = self.nested_like(condition)\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(condition, NestedTensor) and isinstance(other, NestedTensor):\n            return NestedTensor(\n                [x.where(c, y) for x, c, y in zip(self._storage, condition._storage, other._storage)], **self._state\n            )\n        if isinstance(condition, NestedTensor):\n            return NestedTensor([x.where(c, other) for x, c in zip(self._storage, condition._storage)], **self._state)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x.where(condition, y) for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor(x.where(condition, other) for x in self._storage)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        if func not in NestedTensorFunc or not all(issubclass(t, (torch.Tensor, NestedTensor)) for t in types):\n            args = [a.tensor if hasattr(a, \"tensor\") else a for a in args]\n            return func(*args, **kwargs)\n        return NestedTensorFunc[func](*args, **kwargs)\n\n    def __getitem__(self, index: int | slice | tuple) -&gt; tuple[Tensor, Tensor] | NestedTensor:\n        if isinstance(index, tuple):\n            return NestedTensor([t[index[0]][index[1:]] for t in self._storage])\n        if isinstance(index, (int, slice)):\n            ret = self._storage[index]\n            if isinstance(ret, Tensor):\n                return ret, torch.ones_like(ret, dtype=torch.bool)\n            return self.tensor, self.mask\n        raise ValueError(f\"Unsupported index type {type(index)}\")\n\n    def __getattr__(self, name: str) -&gt; Any:\n        if not self._storage:\n            raise ValueError(f\"Unable to get {name} from an empty {self.__class__.__name__}\")\n        ret = [getattr(i, name) for i in self._storage]\n        elem = ret[0]\n        if isinstance(elem, Tensor):\n            return NestedTensor(ret, **self._state)\n        if callable(elem):\n            return NestedTensorFuncWrapper(ret, state=self._state)\n        if elem.__hash__ is not None and len(set(ret)) == 1:\n            return elem\n        return ret\n\n    @property\n    def _state(self) -&gt; Mapping:\n        return {k: v for k, v in self.__dict__.items() if not (k.startswith(\"_\") or k.endswith(\"_\"))}\n\n    def __state__(self) -&gt; Mapping:\n        return self.__dict__\n\n    def __setstate__(self, state: Mapping) -&gt; None:\n        self.__dict__.update(state)\n\n    def __len__(self) -&gt; int:\n        return len(self._storage)\n\n    def __repr__(self):\n        return self.__class__.__name__ + repr(self.tensor)[len(self.tensor.__class__.__name__) :]  # noqa: E203\n\n    def __bool__(self) -&gt; int:\n        return all(bool(x) for x in self._storage)\n\n    def __gt__(  # type: ignore[override]\n        self, other: Tensor | NestedTensor | SupportsFloat\n    ) -&gt; bool | Tensor | NestedTensor:\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor(i &gt; j for i, j in zip(self._storage, other._storage))\n        if isinstance(other, (int, float, Tensor)):\n            return NestedTensor([x &gt; other for x in self._storage], **self._state)\n        raise TypeError(f\"&gt; not supported between instances of '{type(self)}' and '{type(other)}'\")\n\n    def __ge__(  # type: ignore[override]\n        self, other: Tensor | NestedTensor | SupportsFloat\n    ) -&gt; bool | Tensor | NestedTensor:\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor(i &gt;= j for i, j in zip(self._storage, other._storage))\n        if isinstance(other, (int, float, Tensor)):\n            return NestedTensor([x &gt;= other for x in self._storage], **self._state)\n        raise TypeError(f\"&gt;= not supported between instances of '{type(self)}' and '{type(other)}'\")\n\n    def __eq__(  # type: ignore[override]\n        self, other: Tensor | NestedTensor | SupportsFloat\n    ) -&gt; bool | Tensor | NestedTensor:\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor(i == j for i, j in zip(self._storage, other._storage))\n        if isinstance(other, (int, float, Tensor)):\n            return NestedTensor([x == other for x in self._storage], **self._state)\n        return False\n\n    def __le__(  # type: ignore[override]\n        self, other: Tensor | NestedTensor | SupportsFloat\n    ) -&gt; bool | Tensor | NestedTensor:\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor(i &lt;= j for i, j in zip(self._storage, other._storage))\n        if isinstance(other, (int, float, Tensor)):\n            return NestedTensor([x &lt;= other for x in self._storage], **self._state)\n        raise TypeError(f\"&lt;= not supported between instances of '{type(self)}' and '{type(other)}'\")\n\n    def __lt__(  # type: ignore[override]\n        self, other: Tensor | NestedTensor | SupportsFloat\n    ) -&gt; bool | Tensor | NestedTensor:\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor(i &lt; j for i, j in zip(self._storage, other._storage))\n        if isinstance(other, (int, float, Tensor)):\n            return NestedTensor([x &lt; other for x in self._storage], **self._state)\n        raise TypeError(f\"&lt; not supported between instances of '{type(self)}' and '{type(other)}'\")\n\n    def __abs__(self):\n        return NestedTensor([abs(value) for value in self._storage], **self._state)\n\n    def __add__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x + y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value + other for value in self._storage], **self._state)\n\n    def __radd__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y + x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other + value for value in self._storage], **self._state)\n\n    def __iadd__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x += y\n        else:\n            for value in self._storage:\n                value += other\n        return self\n\n    def __sub__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x - y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value - other for value in self._storage], **self._state)\n\n    def __rsub__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y - x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other - value for value in self._storage], **self._state)\n\n    def __isub__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x -= y\n        else:\n            for value in self._storage:\n                value -= other\n        return self\n\n    def __pos__(self):\n        return NestedTensor([+x for x in self._storage])\n\n    def __neg__(self):\n        return NestedTensor([-x for x in self._storage])\n\n    def __invert__(self):\n        return NestedTensor([~x for x in self._storage])\n\n    def __mul__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x * y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value * other for value in self._storage], **self._state)\n\n    def __rmul__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y * x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other * value for value in self._storage], **self._state)\n\n    def __imul__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x *= y\n        else:\n            for value in self._storage:\n                value *= other\n        return self\n\n    def __pow__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x**y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value**other for value in self._storage], **self._state)\n\n    def __rpow__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y**x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other**value for value in self._storage], **self._state)\n\n    def __ipow__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x **= y\n        else:\n            for value in self._storage:\n                value **= other\n        return self\n\n    def __matmul__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x @ y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value @ other for value in self._storage], **self._state)\n\n    def __rmatmul__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y @ x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other @ value for value in self._storage], **self._state)\n\n    def __imatmul__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x @= y\n        else:\n            for value in self._storage:\n                value @= other\n        return self\n\n    def __truediv__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x / y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value / other for value in self._storage], **self._state)\n\n    def __rtruediv__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y / x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other / value for value in self._storage], **self._state)\n\n    def __itruediv__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x /= y\n        else:\n            for value in self._storage:\n                value /= other\n        return self\n\n    def __floordiv__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x // y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value // other for value in self._storage], **self._state)\n\n    def __rfloordiv__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y // x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other // value for value in self._storage], **self._state)\n\n    def __ifloordiv__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x //= y\n        else:\n            for value in self._storage:\n                value //= other\n        return self\n\n    def __mod__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x % y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value % other for value in self._storage], **self._state)\n\n    def __rmod__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y % x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other % value for value in self._storage], **self._state)\n\n    def __imod__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x %= y\n        else:\n            for value in self._storage:\n                value %= other\n        return self\n\n    def __and__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x &amp; y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value &amp; other for value in self._storage], **self._state)\n\n    def __rand__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y &amp; x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other &amp; value for value in self._storage], **self._state)\n\n    def __iand__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x &amp;= y\n        else:\n            for value in self._storage:\n                value &amp;= other\n        return self\n\n    def __or__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x | y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value | other for value in self._storage], **self._state)\n\n    def __ror__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y | x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other | value for value in self._storage], **self._state)\n\n    def __ior__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x |= y\n        else:\n            for value in self._storage:\n                value |= other\n        return self\n\n    def __xor__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x ^ y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value ^ other for value in self._storage], **self._state)\n\n    def __rxor__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y ^ x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other ^ value for value in self._storage], **self._state)\n\n    def __ixor__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x ^= y\n        else:\n            for value in self._storage:\n                value ^= other\n        return self\n\n    def __lshift__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x &lt;&lt; y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value &lt;&lt; other for value in self._storage], **self._state)\n\n    def __rlshift__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y &lt;&lt; x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other &lt;&lt; value for value in self._storage], **self._state)\n\n    def __ilshift__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x &lt;&lt;= y\n        else:\n            for value in self._storage:\n                value &lt;&lt;= other\n        return self\n\n    def __rshift__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([x &gt;&gt; y for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([value &gt;&gt; other for value in self._storage], **self._state)\n\n    def __rrshift__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if isinstance(other, NestedTensor):\n            return NestedTensor([y &gt;&gt; x for x, y in zip(self._storage, other._storage)], **self._state)\n        return NestedTensor([other &gt;&gt; value for value in self._storage], **self._state)\n\n    def __irshift__(self, other: Tensor | NestedTensor | SupportsFloat):\n        if isinstance(other, Tensor) and self.shape == other.shape:\n            other = self.nested_like(other)\n        if hasattr(other, \"to\"):\n            other = other.to(self.dtype)\n        if isinstance(other, NestedTensor):\n            for x, y in zip(self._storage, other._storage):\n                x &gt;&gt;= y\n        else:\n            for value in self._storage:\n                value &gt;&gt;= other\n        return self\n\n    @method_cache(maxsize=1)\n    def _tensor(self, storage: tuple, batch_first: bool, padding_value: SupportsFloat) -&gt; Tensor:\n        if storage[0].dim() == 0:\n            return torch.stack(storage, dim=0)\n        return pad_tensor(storage, size=self.size(), batch_first=batch_first, padding_value=float(padding_value))\n\n    @method_cache(maxsize=1)\n    def _mask(self, storage: tuple, batch_first: bool, mask_value: bool) -&gt; Tensor:\n        if storage[0].dim() == 0:\n            return torch.full((len(storage),), not mask_value, dtype=torch.bool, device=self.device)\n        size = self.size()\n        # ignore channel dimension\n        if storage[0].dim() &gt; 1 and len({t.size(-1) for t in storage}) == 1:\n            size = size[:-1]  # type: ignore\n        return mask_tensor(storage, size=size, batch_first=batch_first, mask_value=mask_value)\n\n    @method_cache(maxsize=1)\n    def _device(self, storage) -&gt; torch.device:\n        return storage[0].device\n\n    @method_cache(maxsize=1)\n    def _size(self, storage, dim: int | None = None, batch_first: bool = True) -&gt; torch.Size | int:\n        if dim is not None:\n            if dim == 0:\n                return len(storage)\n            return max(t.size(dim - 1) for t in storage)\n        if max(t.dim() for t in storage) == 0:\n            return torch.Size((len(storage),))\n        ndim = max(t.dim() for t in storage)\n        size = [max(t.shape[i] if i &lt; len(t.shape) else 0 for t in storage) for i in range(ndim)]\n        size.insert(0 if batch_first else 1, len(storage))\n        return torch.Size(size)\n\n    @method_cache(maxsize=1)\n    def _dim(self, storage) -&gt; torch.Size:\n        return max(t.dim() for t in storage) + 1\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.concat","title":"<code>concat: Tensor</code>  <code>property</code>","text":"<p>Concat <code>tensor</code> in padding dim.</p> <p>This is particularly useful when calculating loss or passing <code>Linear</code> to avoid unnecessary computation.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 8), torch.randn(11, 8)])\n&gt;&gt;&gt; nested_tensor.concat.shape\ntorch.Size([20, 8])\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 9, 8), torch.randn(11, 11, 8)])\n&gt;&gt;&gt; nested_tensor.concat.shape\ntorch.Size([202, 8])\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 9, 8, 6), torch.randn(11, 11, 8, 6)])\n&gt;&gt;&gt; nested_tensor.concat.shape\ntorch.Size([202, 8, 6])\n&gt;&gt;&gt; nested_tensor = NestedTensor([torch.randn(9, 9, 8, 7), torch.randn(11, 11, 8, 6)])\n&gt;&gt;&gt; nested_tensor.concat.shape\ntorch.Size([1293, 8])\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Device of the NestedTensor.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.device\ndevice(type='cpu')\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.mask","title":"<code>mask: Tensor</code>  <code>property</code>","text":"<p>Padding mask of <code>tensor</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.mask\ntensor([[ True,  True,  True],\n        [ True,  True, False]])\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.ndim","title":"<code>ndim: int</code>  <code>property</code>","text":"<p>Alias for <code>dim()</code>.</p>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.shape","title":"<code>shape: torch.Size | int</code>  <code>property</code>","text":"<p>Alias for <code>size()</code>.</p>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.tensor","title":"<code>tensor: Tensor</code>  <code>property</code>","text":"<p>Return a single tensor by padding all the tensors.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.tensor\ntensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.all","title":"<code>all(dim=None, keepdim=False)</code>","text":"<p>Tests if all elements in NestedTensor evaluate to True.</p> <p>Returns:</p> Type Description <code>bool | Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.ones(2, 4, dtype=torch.bool), torch.ones(3, 5, dtype=torch.bool)])\n&gt;&gt;&gt; nested_tensor.all()\ntensor(True)\n&gt;&gt;&gt; nested_tensor.all(dim=0)\ntensor([True, True])\n&gt;&gt;&gt; nested_tensor.all(dim=0, keepdim=True)\ntensor([[True, True]])\n&gt;&gt;&gt; nested_tensor.all(dim=1)\nNestedTensor([[ True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True]])\n&gt;&gt;&gt; nested_tensor.all(dim=1, keepdim=True)\nNestedTensor([[[ True,  True,  True,  True, False]],\n\n        [[ True,  True,  True,  True,  True]]])\n&gt;&gt;&gt; nested_tensor.batch_first = False\n&gt;&gt;&gt; nested_tensor.all(dim=1)\ntensor([True, True])\n&gt;&gt;&gt; nested_tensor.batch_first = False\n&gt;&gt;&gt; nested_tensor.all(dim=0)\nNestedTensor([[ True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True]])\n&gt;&gt;&gt; nested_tensor.all(dim=1)\ntensor([True, True])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def all(self, dim: int | None = None, keepdim: bool = False) -&gt; bool | Tensor | NestedTensor:\n    r\"\"\"\n    Tests if all elements in NestedTensor evaluate to True.\n\n    Returns:\n        (bool | Tensor):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.ones(2, 4, dtype=torch.bool), torch.ones(3, 5, dtype=torch.bool)])\n        &gt;&gt;&gt; nested_tensor.all()\n        tensor(True)\n        &gt;&gt;&gt; nested_tensor.all(dim=0)\n        tensor([True, True])\n        &gt;&gt;&gt; nested_tensor.all(dim=0, keepdim=True)\n        tensor([[True, True]])\n        &gt;&gt;&gt; nested_tensor.all(dim=1)\n        NestedTensor([[ True,  True,  True,  True, False],\n                [ True,  True,  True,  True,  True]])\n        &gt;&gt;&gt; nested_tensor.all(dim=1, keepdim=True)\n        NestedTensor([[[ True,  True,  True,  True, False]],\n        &lt;BLANKLINE&gt;\n                [[ True,  True,  True,  True,  True]]])\n        &gt;&gt;&gt; nested_tensor.batch_first = False\n        &gt;&gt;&gt; nested_tensor.all(dim=1)\n        tensor([True, True])\n        &gt;&gt;&gt; nested_tensor.batch_first = False\n        &gt;&gt;&gt; nested_tensor.all(dim=0)\n        NestedTensor([[ True,  True,  True,  True, False],\n                [ True,  True,  True,  True,  True]])\n        &gt;&gt;&gt; nested_tensor.all(dim=1)\n        tensor([True, True])\n    \"\"\"\n\n    if dim is None:\n        return torch.tensor(all(i.all() for i in self._storage))\n    if (self.batch_first and dim == 0) or (not self.batch_first and dim == 1):\n        if keepdim:\n            return torch.tensor([i.all() for i in self._storage]).unsqueeze(0 if self.batch_first else 1)\n        return torch.tensor([i.all() for i in self._storage])\n    if self.batch_first or dim != 0:\n        dim -= 1\n    return NestedTensor([i.all(dim=dim, keepdim=keepdim) for i in self._storage])\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.dim","title":"<code>dim()</code>","text":"<p>Number of dimension of the NestedTensor.</p> <p>Returns:</p> Type Description <code>int</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.dim()\n2\n&gt;&gt;&gt; nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))\n&gt;&gt;&gt; nested_tensor.ndim\n2\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def dim(self) -&gt; int:\n    r\"\"\"\n    Number of dimension of the NestedTensor.\n\n    Returns:\n        (int):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.dim()\n        2\n        &gt;&gt;&gt; nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))\n        &gt;&gt;&gt; nested_tensor.ndim\n        2\n    \"\"\"\n\n    return self._dim(tuple(self._storage))\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.from_tensor_mask","title":"<code>from_tensor_mask(tensor, mask)</code>  <code>classmethod</code>","text":"<p>Build a <code>NestedTensor</code> object from a padded <code>Tensor</code> and corresponding mask <code>Tensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Padded Tensor.</p> required <code>mask</code> <code>Tensor</code> <p>Tensor Mask.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; padded_tensor = torch.tensor([[1, 2, 3, 0, 0],\n...                                [4, 5, 0, 0, 0],\n...                                [6, 7, 8, 9, 0]])\n&gt;&gt;&gt; mask_tensor = torch.tensor([[1, 1, 1, 0, 0],\n...                             [1, 1, 0, 0, 0],\n...                             [1, 1, 1, 1, 0]])\n&gt;&gt;&gt; nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)\n&gt;&gt;&gt; nested_tensor\nNestedTensor([[1, 2, 3, 0],\n        [4, 5, 0, 0],\n        [6, 7, 8, 9]])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>@classmethod\ndef from_tensor_mask(cls, tensor: Tensor, mask: Tensor):\n    r\"\"\"\n    Build a `NestedTensor` object from a padded `Tensor` and corresponding mask `Tensor`.\n\n    Args:\n        tensor: Padded Tensor.\n        mask: Tensor Mask.\n\n    Returns:\n        (torch.Tensor):\n\n    Examples:\n        &gt;&gt;&gt; padded_tensor = torch.tensor([[1, 2, 3, 0, 0],\n        ...                                [4, 5, 0, 0, 0],\n        ...                                [6, 7, 8, 9, 0]])\n        &gt;&gt;&gt; mask_tensor = torch.tensor([[1, 1, 1, 0, 0],\n        ...                             [1, 1, 0, 0, 0],\n        ...                             [1, 1, 1, 1, 0]])\n        &gt;&gt;&gt; nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)\n        &gt;&gt;&gt; nested_tensor\n        NestedTensor([[1, 2, 3, 0],\n                [4, 5, 0, 0],\n                [6, 7, 8, 9]])\n    \"\"\"\n\n    if mask.ndim == 2:\n        return cls(t[slice(0, m.sum())] for t, m in zip(tensor, mask))\n    return cls(\n        t[[slice(0, (m.sum(dim=dim) &gt; 0).sum().item()) for dim in reversed(range(m.dim()))]]\n        for t, m in zip(tensor, mask)\n    )\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.nested_like","title":"<code>nested_like(tensor, strict=True)</code>","text":"<p>Create a new <code>NestedTensor</code> from a <code>Tensor</code>. The newly created <code>NestedTensor</code> will have the same shape as current <code>NestedTensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to be converted to <code>NestedTensor</code>.</p> required <code>strict</code> <code>bool</code> <p>Check if the shape of <code>tensor</code> is the same as the current <code>NestedTensor</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>NestedTensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; (nested_tensor == nested_tensor.nested_like(nested_tensor)).all()\ntensor(True)\n&gt;&gt;&gt; tensor = nested_tensor.tensor\n&gt;&gt;&gt; (nested_tensor == nested_tensor.nested_like(tensor)).all()\ntensor(True)\n&gt;&gt;&gt; f = nested_tensor.nested_like(torch.randn(2, 2))\nTraceback (most recent call last):\nValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])\n&gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(2, 2), False)\n&gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(3, 3), False)\nTraceback (most recent call last):\nValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def nested_like(self, tensor: Tensor, strict: bool = True) -&gt; NestedTensor:\n    r\"\"\"\n    Create a new `NestedTensor` from a `Tensor`.\n    The newly created `NestedTensor` will have the same shape as current `NestedTensor`.\n\n    Args:\n        tensor: The tensor to be converted to `NestedTensor`.\n        strict: Check if the shape of `tensor` is the same as the current `NestedTensor`.\n\n    Returns:\n        (NestedTensor):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; (nested_tensor == nested_tensor.nested_like(nested_tensor)).all()\n        tensor(True)\n        &gt;&gt;&gt; tensor = nested_tensor.tensor\n        &gt;&gt;&gt; (nested_tensor == nested_tensor.nested_like(tensor)).all()\n        tensor(True)\n        &gt;&gt;&gt; f = nested_tensor.nested_like(torch.randn(2, 2))\n        Traceback (most recent call last):\n        ValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])\n        &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(2, 2), False)\n        &gt;&gt;&gt; p = nested_tensor.nested_like(torch.randn(3, 3), False)\n        Traceback (most recent call last):\n        ValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\n    \"\"\"  # noqa: E501\n\n    if isinstance(tensor, NestedTensor):\n        return tensor.clone()\n\n    if strict and self.shape != tensor.shape:\n        raise ValueError(\n            f\"The shape of NestedTensor and input tensor does not match, {self.shape} != {tensor.shape}\"\n        )\n    if self.size(0) != tensor.size(0):\n        raise ValueError(\n            f\"The batch size of NestedTensor and input tensor does not match, {self.size(0)} != {tensor.size(0)}\"\n        )\n    return NestedTensor([o[tuple(slice(0, dim) for dim in t.shape)] for t, o in zip(self._storage, tensor)])\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.size","title":"<code>size(dim=None)</code>","text":"<p>Returns the size of the self <code>NestedTensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int | None</code> <p>If not specified, the returned value is a <code>torch.Size</code>, a subclass of <code>tuple</code>. If specified, returns an <code>int</code> holding the size of that dimension. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Size | int</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; nested_tensor.size(0)\n2\n&gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n&gt;&gt;&gt; nested_tensor.shape\ntorch.Size([2, 4])\n&gt;&gt;&gt; nested_tensor.size(1)\n4\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def size(self, dim: int | None = None) -&gt; torch.Size | int:\n    r\"\"\"\n    Returns the size of the self `NestedTensor`.\n\n    Args:\n        dim: If not specified, the returned value is a `torch.Size`, a subclass of `tuple`.\n            If specified, returns an `int` holding the size of that dimension.\n            Defaults to `None`.\n\n    Returns:\n        (torch.Size | int):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.size()\n        torch.Size([2, 3])\n        &gt;&gt;&gt; nested_tensor.size(0)\n        2\n        &gt;&gt;&gt; nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])\n        &gt;&gt;&gt; nested_tensor.shape\n        torch.Size([2, 4])\n        &gt;&gt;&gt; nested_tensor.size(1)\n        4\n    \"\"\"\n\n    return self._size(tuple(self._storage), dim, self.batch_first)\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.tolist","title":"<code>tolist()</code>","text":"<p>Convert a NestedTensor to a list of lists of values.</p> <p>Returns:</p> Type Description <code>list</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.tolist()\n[[1, 2, 3], [4, 5]]\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def tolist(self) -&gt; list:\n    r\"\"\"\n    Convert a NestedTensor to a list of lists of values.\n\n    Returns:\n        (list):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.tolist()\n        [[1, 2, 3], [4, 5]]\n    \"\"\"\n\n    return [t.tolist() for t in self._storage]\n</code></pre>"},{"location":"tensors/nested_tensor/#danling.tensors.NestedTensor.where","title":"<code>where(condition, other)</code>","text":"<p>Return a NestedTensor of elements selected from either self or other, depending on condition.</p> <p>Returns:</p> Type Description <code>NestedTensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n&gt;&gt;&gt; nested_tensor.where(nested_tensor &gt; 2, torch.tensor([[6, 5, 4], [3, 2, 1]]))\nNestedTensor([[6, 5, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.where(nested_tensor &gt; 2, NestedTensor([[6, 5, 4], [3, 2]]))\nNestedTensor([[6, 5, 3],\n        [4, 5, 0]])\n&gt;&gt;&gt; nested_tensor.where(torch.tensor(True), NestedTensor([[6, 5, 4], [3, 2]]))\nNestedTensor([[1, 2, 3],\n        [4, 5, 0]])\n</code></pre> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>def where(self, condition: Tensor | NestedTensor, other: Tensor | NestedTensor | SupportsFloat) -&gt; NestedTensor:\n    r\"\"\"\n    Return a NestedTensor of elements selected from either self or other, depending on condition.\n\n    Returns:\n        (NestedTensor):\n\n    Examples:\n        &gt;&gt;&gt; nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])\n        &gt;&gt;&gt; nested_tensor.where(nested_tensor &gt; 2, torch.tensor([[6, 5, 4], [3, 2, 1]]))\n        NestedTensor([[6, 5, 3],\n                [4, 5, 0]])\n        &gt;&gt;&gt; nested_tensor.where(nested_tensor &gt; 2, NestedTensor([[6, 5, 4], [3, 2]]))\n        NestedTensor([[6, 5, 3],\n                [4, 5, 0]])\n        &gt;&gt;&gt; nested_tensor.where(torch.tensor(True), NestedTensor([[6, 5, 4], [3, 2]]))\n        NestedTensor([[1, 2, 3],\n                [4, 5, 0]])\n    \"\"\"\n\n    if isinstance(condition, Tensor) and self.shape == condition.shape:\n        condition = self.nested_like(condition)\n    if isinstance(other, Tensor) and self.shape == other.shape:\n        other = self.nested_like(other)\n    if isinstance(condition, NestedTensor) and isinstance(other, NestedTensor):\n        return NestedTensor(\n            [x.where(c, y) for x, c, y in zip(self._storage, condition._storage, other._storage)], **self._state\n        )\n    if isinstance(condition, NestedTensor):\n        return NestedTensor([x.where(c, other) for x, c in zip(self._storage, condition._storage)], **self._state)\n    if isinstance(other, NestedTensor):\n        return NestedTensor([x.where(condition, y) for x, y in zip(self._storage, other._storage)], **self._state)\n    return NestedTensor(x.where(condition, other) for x in self._storage)\n</code></pre>"},{"location":"tensors/pn_tensor/","title":"PNTensor","text":"<p>               Bases: <code>Tensor</code></p> <p>Wrapper for tensors to be converted to <code>NestedTensor</code>.</p> <p><code>PNTensor</code> is a subclass of <code>torch.Tensor</code>. It implements three additional property as <code>NestedTensor</code>: <code>tensor</code>, <code>mask</code>, and <code>concat</code>.</p> <p>Although it is possible to directly construct <code>NestedTensor</code> in dataset, the best practice is to do so is in <code>collate_fn</code>. <code>PNTensor</code> is introduced to smoothen the process.</p> <p>Convert tensors that will be converted to <code>NestedTensor</code> to a <code>PNTensor</code>, and PyTorch Dataloader will automatically collate <code>PNTensor</code> to <code>NestedTensor</code>.</p> Source code in <code>danling/tensors/nested_tensor.py</code> Python<pre><code>class PNTensor(Tensor):\n    r\"\"\"\n    Wrapper for tensors to be converted to `NestedTensor`.\n\n    `PNTensor` is a subclass of `torch.Tensor`.\n    It implements three additional property as `NestedTensor`: `tensor`, `mask`, and `concat`.\n\n    Although it is possible to directly construct `NestedTensor` in dataset,\n    the best practice is to do so is in `collate_fn`.\n    `PNTensor` is introduced to smoothen the process.\n\n    Convert tensors that will be converted to `NestedTensor` to a `PNTensor`,\n    and PyTorch Dataloader will automatically collate `PNTensor` to `NestedTensor`.\n    \"\"\"\n\n    @property\n    def tensor(self) -&gt; Tensor:\n        r\"\"\"\n        Identical to `self`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n            &gt;&gt;&gt; pn_tensor = PNTensor([1, 2, 3])\n            &gt;&gt;&gt; bool((tensor == pn_tensor).all())\n            True\n            &gt;&gt;&gt; bool((tensor == pn_tensor.tensor).all())\n            True\n        \"\"\"\n\n        return self\n\n    @property\n    def mask(self) -&gt; Tensor:\n        r\"\"\"\n        Identical to `torch.ones_like(self)`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n            &gt;&gt;&gt; pn_tensor = PNTensor([1, 2, 3])\n            &gt;&gt;&gt; bool((pn_tensor.mask == torch.ones_like(pn_tensor)).all().item())\n            True\n        \"\"\"\n\n        return torch.ones_like(self)\n\n    @property\n    def contact(self) -&gt; Tensor:\n        r\"\"\"\n        Identical to `self`.\n\n        Returns:\n            (torch.Tensor):\n\n        Examples:\n            &gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n            &gt;&gt;&gt; pn_tensor = PNTensor([1, 2, 3])\n            &gt;&gt;&gt; bool((tensor == pn_tensor).all())\n            True\n            &gt;&gt;&gt; bool((tensor == pn_tensor.contact).all())\n            True\n        \"\"\"\n\n        return self\n\n    def new_empty(self, *args, **kwargs):\n        return PNTensor(super().new_empty(*args, **kwargs))\n</code></pre>"},{"location":"tensors/pn_tensor/#danling.tensors.PNTensor.contact","title":"<code>contact: Tensor</code>  <code>property</code>","text":"<p>Identical to <code>self</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; pn_tensor = PNTensor([1, 2, 3])\n&gt;&gt;&gt; bool((tensor == pn_tensor).all())\nTrue\n&gt;&gt;&gt; bool((tensor == pn_tensor.contact).all())\nTrue\n</code></pre>"},{"location":"tensors/pn_tensor/#danling.tensors.PNTensor.mask","title":"<code>mask: Tensor</code>  <code>property</code>","text":"<p>Identical to <code>torch.ones_like(self)</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; pn_tensor = PNTensor([1, 2, 3])\n&gt;&gt;&gt; bool((pn_tensor.mask == torch.ones_like(pn_tensor)).all().item())\nTrue\n</code></pre>"},{"location":"tensors/pn_tensor/#danling.tensors.PNTensor.tensor","title":"<code>tensor: Tensor</code>  <code>property</code>","text":"<p>Identical to <code>self</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; pn_tensor = PNTensor([1, 2, 3])\n&gt;&gt;&gt; bool((tensor == pn_tensor).all())\nTrue\n&gt;&gt;&gt; bool((tensor == pn_tensor.tensor).all())\nTrue\n</code></pre>"},{"location":"tensors/torch_func_registry/","title":"TorchFuncRegistry","text":"<p>               Bases: <code>Registry</code></p> <p><code>TorchFuncRegistry</code> for extending PyTorch Tensor.</p> Source code in <code>danling/tensors/utils.py</code> Python<pre><code>class TorchFuncRegistry(Registry):  # pylint: disable=too-few-public-methods\n    \"\"\"\n    `TorchFuncRegistry` for extending PyTorch Tensor.\n    \"\"\"\n\n    def implement(self, torch_function: Callable) -&gt; Callable:\n        r\"\"\"\n        Implement an implementation for a torch function.\n\n        Args:\n            function: The torch function to implement.\n\n        Returns:\n            function: The registered function.\n\n        Raises:\n            ValueError: If the function with the same name already registered and `TorchFuncRegistry.override=False`.\n\n        Examples:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; registry = TorchFuncRegistry(\"test\")\n            &gt;&gt;&gt; @registry.implement(torch.mean)\n            ... def mean(input):\n            ...     raise input.mean()\n            &gt;&gt;&gt; registry  # doctest: +ELLIPSIS\n            TorchFuncRegistry(\n              (&lt;built-in method mean of type object at ...&gt;): &lt;function mean at ...&gt;\n            )\n        \"\"\"\n\n        if torch_function in self and not self.override:\n            raise ValueError(f\"Torch function {torch_function.__name__} already registered.\")\n\n        @wraps(self.register)\n        def register(function):\n            self.set(torch_function, function)\n            return function\n\n        return register\n</code></pre>"},{"location":"tensors/torch_func_registry/#danling.tensors.TorchFuncRegistry.implement","title":"<code>implement(torch_function)</code>","text":"<p>Implement an implementation for a torch function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <p>The torch function to implement.</p> required <p>Returns:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The registered function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the function with the same name already registered and <code>TorchFuncRegistry.override=False</code>.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; registry = TorchFuncRegistry(\"test\")\n&gt;&gt;&gt; @registry.implement(torch.mean)\n... def mean(input):\n...     raise input.mean()\n&gt;&gt;&gt; registry\nTorchFuncRegistry(\n  (&lt;built-in method mean of type object at ...&gt;): &lt;function mean at ...&gt;\n)\n</code></pre> Source code in <code>danling/tensors/utils.py</code> Python<pre><code>def implement(self, torch_function: Callable) -&gt; Callable:\n    r\"\"\"\n    Implement an implementation for a torch function.\n\n    Args:\n        function: The torch function to implement.\n\n    Returns:\n        function: The registered function.\n\n    Raises:\n        ValueError: If the function with the same name already registered and `TorchFuncRegistry.override=False`.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; registry = TorchFuncRegistry(\"test\")\n        &gt;&gt;&gt; @registry.implement(torch.mean)\n        ... def mean(input):\n        ...     raise input.mean()\n        &gt;&gt;&gt; registry  # doctest: +ELLIPSIS\n        TorchFuncRegistry(\n          (&lt;built-in method mean of type object at ...&gt;): &lt;function mean at ...&gt;\n        )\n    \"\"\"\n\n    if torch_function in self and not self.override:\n        raise ValueError(f\"Torch function {torch_function.__name__} already registered.\")\n\n    @wraps(self.register)\n    def register(function):\n        self.set(torch_function, function)\n        return function\n\n    return register\n</code></pre>"},{"location":"utils/basex/","title":"basex","text":""},{"location":"utils/contextmanagers/","title":"Context Manager","text":""},{"location":"utils/contextmanagers/#danling.utils.contextmanagers.debug","title":"<code>debug(enable=True, error=Exception, exclude=None)</code>","text":"<p>Contextmanager to enter debug mode on <code>error</code> except for <code>exclude</code>.</p> <p><code>debug</code> is intended to be used to catch the error and enter debug mode. Since it is mainly for development purposed, we include an <code>enable</code> args so that it can be deactivated.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Whether to enable the contextmanager. Defaults to <code>True</code>.</p> <code>True</code> <code>error</code> <code>Exceptions</code> <p>The error to catch. Defaults to <code>Exception</code>.</p> <code>Exception</code> <code>exclude</code> <code>Optional[Exceptions]</code> <p>The error to exclude. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>danling/utils/contextmanagers.py</code> Python<pre><code>@contextmanager\ndef debug(\n    enable: bool = True,\n    error: Exceptions = Exception,\n    exclude: Optional[Exceptions] = None,\n):\n    \"\"\"\n    Contextmanager to enter debug mode on `error` except for `exclude`.\n\n    `debug` is intended to be used to catch the error and enter debug mode.\n    Since it is mainly for development purposed, we include an `enable` args so that it can be deactivated.\n\n    Args:\n        enable: Whether to enable the contextmanager.\n            Defaults to `True`.\n        error: The error to catch.\n            Defaults to `Exception`.\n        exclude: The error to exclude.\n            Defaults to `None`.\n    \"\"\"\n\n    if not enable:\n        yield\n        return\n    try:\n        yield\n    except error as exc:  # pylint: disable=broad-exception-caught\n        if exclude is not None and isinstance(exc, exclude):\n            raise exc\n        _, m, tb = sys.exc_info()\n        print(repr(m), file=sys.stderr)\n        pdb.post_mortem(tb)\n    finally:\n        pass\n</code></pre>"},{"location":"utils/decorators/","title":"Decorator","text":""},{"location":"utils/decorators/#danling.utils.decorators.catch","title":"<code>catch(error=Exception, exclude=None, callback=print_exc, *callback_args, **callback_kwargs)</code>","text":"<p>Decorator to catch <code>error</code> except for <code>exclude</code>. Detailed traceback will be printed to <code>stderr</code>.</p> <p><code>catch</code> is extremely useful for unfatal errors. For example, <code>Runner</code> saves checkpoint regularly, however, this might break running if the space is full. Decorating <code>save</code> method with <code>catch</code> will allow you to catch these errors and continue your running.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exceptions</code> <p>Exceptions to be caught.</p> <code>Exception</code> <code>exclude</code> <code>Exceptions | None</code> <p>Exceptions to be excluded.</p> <code>None</code> <code>callback</code> <code>Callable</code> <p>Callback to be called when an error occurs. The first four arguments to <code>callback</code> are <code>exc</code>, <code>func</code>, <code>args</code>, <code>kwargs</code>. Additional arguments should be passed with <code>*callback_args</code> and <code>**callback_kwargs</code>.</p> <code>print_exc</code> <code>callback_args</code> <p>Arguments to be passed to <code>callback</code>.</p> <code>()</code> <code>callback_kwargs</code> <p>Keyword arguments to be passed to <code>callback</code>.</p> <code>{}</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; def file_not_found(*args, **kwargs):\n...     raise FileNotFoundError\n&gt;&gt;&gt; func = file_not_found\n&gt;&gt;&gt; func()\nTraceback (most recent call last):\nFileNotFoundError\n&gt;&gt;&gt; func = catch(OSError)(file_not_found)\n&gt;&gt;&gt; func()\n&gt;&gt;&gt; func = catch(IOError)(file_not_found)\n&gt;&gt;&gt; func()\n&gt;&gt;&gt; func = catch(ZeroDivisionError)(file_not_found)\n&gt;&gt;&gt; func()\nTraceback (most recent call last):\nFileNotFoundError\n</code></pre> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>@flexible_decorator\ndef catch(  # pylint: disable=keyword-arg-before-vararg\n    error: Exceptions = Exception,\n    exclude: Exceptions | None = None,\n    callback: Callable = print_exc,\n    *callback_args,\n    **callback_kwargs,\n):\n    r\"\"\"\n    Decorator to catch `error` except for `exclude`.\n    Detailed traceback will be printed to `stderr`.\n\n    `catch` is extremely useful for unfatal errors.\n    For example, `Runner` saves checkpoint regularly, however, this might break running if the space is full.\n    Decorating `save` method with `catch` will allow you to catch these errors and continue your running.\n\n    Args:\n        error: Exceptions to be caught.\n        exclude: Exceptions to be excluded.\n        callback: Callback to be called when an error occurs.\n            The first four arguments to `callback` are `exc`, `func`, `args`, `kwargs`.\n            Additional arguments should be passed with `*callback_args` and `**callback_kwargs`.\n        callback_args: Arguments to be passed to `callback`.\n        callback_kwargs: Keyword arguments to be passed to `callback`.\n\n    Examples:\n        &gt;&gt;&gt; def file_not_found(*args, **kwargs):\n        ...     raise FileNotFoundError\n        &gt;&gt;&gt; func = file_not_found\n        &gt;&gt;&gt; func()\n        Traceback (most recent call last):\n        FileNotFoundError\n        &gt;&gt;&gt; func = catch(OSError)(file_not_found)\n        &gt;&gt;&gt; func()\n        &gt;&gt;&gt; func = catch(IOError)(file_not_found)\n        &gt;&gt;&gt; func()\n        &gt;&gt;&gt; func = catch(ZeroDivisionError)(file_not_found)\n        &gt;&gt;&gt; func()\n        Traceback (most recent call last):\n        FileNotFoundError\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):  # pylint: disable=inconsistent-return-statements\n            try:\n                return func(*args, **kwargs)\n            except error as exc:  # pylint: disable=broad-exception-caught\n                if exclude is not None and isinstance(exc, exclude):\n                    raise exc\n                callback(exc, func, args, kwargs, *callback_args, **callback_kwargs)\n\n        return wrapper\n\n    decorator.__doc__ = catch.__doc__\n\n    return decorator\n</code></pre>"},{"location":"utils/decorators/#danling.utils.decorators.ensure_dir","title":"<code>ensure_dir(func)</code>","text":"<p>Decorator to ensure a directory property exists.</p> Note <p>Please avoid using this with <code>cached_property</code>.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; @property\n... @ensure_dir\n... def dir(self) -&gt; str:\n...     return os.path.join(\"path\", \"to\", \"dir\")\n</code></pre> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>def ensure_dir(func):\n    r\"\"\"\n    Decorator to ensure a directory property exists.\n\n    Note:\n        Please avoid using this with `cached_property`.\n\n    Examples:\n        &gt;&gt;&gt; @property\n        ... @ensure_dir\n        ... def dir(self) -&gt; str:\n        ...     return os.path.join(\"path\", \"to\", \"dir\")\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        path = abspath(func(*args, **kwargs))\n        makedirs(path, exist_ok=True)\n        return path\n\n    return wrapper\n</code></pre>"},{"location":"utils/decorators/#danling.utils.decorators.flexible_decorator","title":"<code>flexible_decorator(maybe_decorator=None)</code>","text":"<p>Meta decorator to allow bracket-less decorator when no arguments are passed.</p> <p>Examples:</p> <p>For decorator defined as follows:</p> Python Console Session<pre><code>&gt;&gt;&gt; @flexible_decorator\n... def decorator(*args, **kwargs):\n...     def wrapper(func, *args, **kwargs):\n...         pass\n...     return wrapper\n</code></pre> <p>The following two are equivalent:</p> Python Console Session<pre><code>&gt;&gt;&gt; @decorator\n... def func(*args, **kwargs):\n...     pass\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; @decorator()\n... def func(*args, **kwargs):\n...     pass\n</code></pre> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>def flexible_decorator(maybe_decorator: Optional[Callable] = None):\n    r\"\"\"\n    Meta decorator to allow bracket-less decorator when no arguments are passed.\n\n    Examples:\n        For decorator defined as follows:\n\n        &gt;&gt;&gt; @flexible_decorator\n        ... def decorator(*args, **kwargs):\n        ...     def wrapper(func, *args, **kwargs):\n        ...         pass\n        ...     return wrapper\n\n        The following two are equivalent:\n\n        &gt;&gt;&gt; @decorator\n        ... def func(*args, **kwargs):\n        ...     pass\n\n        &gt;&gt;&gt; @decorator()\n        ... def func(*args, **kwargs):\n        ...     pass\n    \"\"\"\n\n    def decorator(func: Callable):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if len(args) == 1 and isfunction(args[0]):\n                return func(**kwargs)(args[0])\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    if maybe_decorator is None:\n        return decorator\n    return decorator(maybe_decorator)\n</code></pre>"},{"location":"utils/decorators/#danling.utils.decorators.method_cache","title":"<code>method_cache(maxsize=128, typed=False)</code>","text":"<p>Decorator to cache the result of an instance method.</p> <p><code>functools.lru_cache</code> uses a strong reference to the instance, which will make the instance immortal and break the garbage collection.</p> <p><code>method_cache</code> uses a weak reference to the instance to resolve this issue.</p> See Also Source code in <code>danling/utils/decorators.py</code> Python<pre><code>@flexible_decorator\ndef method_cache(maxsize: int | None = 128, typed: bool = False):\n    r\"\"\"\n    Decorator to cache the result of an instance method.\n\n    `functools.lru_cache` uses a strong reference to the instance,\n    which will make the instance immortal and break the garbage collection.\n\n    `method_cache` uses a weak reference to the instance to resolve this issue.\n\n    See Also:\n        https://rednafi.github.io/reflections/dont-wrap-instance-methods-with-functoolslru_cache-decorator-in-python.html\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            self_ref = ref(self)\n\n            @wraps(func)\n            @lru_cache(maxsize=maxsize, typed=typed)\n            def cached_method(*args, **kwargs):\n                return func(self_ref(), *args, **kwargs)\n\n            setattr(self, func.__name__, cached_method)\n            return cached_method(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"utils/decorators/#danling.utils.decorators.print_exc","title":"<code>print_exc(exc, func, args, kwargs, verbosity=40)</code>","text":"<p>Print exception raised by <code>func</code> with <code>args</code> and <code>kwargs</code> to <code>stderr</code>. This function serves as the default callback for catch.</p> <p>Parameters:</p> Name Type Description Default <code>verbosity</code> <code>int</code> <p>What level of traceback to print. 0-: No traceback. 0-10: Full information of arguments and key word arguments. 10-20: Stack trace to function calls. 40+: Function name and error messages.</p> <code>40</code> Source code in <code>danling/utils/decorators.py</code> Python<pre><code>def print_exc(exc, func, args, kwargs, verbosity: int = 40):  # pylint: disable=W0613\n    r\"\"\"\n    Print exception raised by `func` with `args` and `kwargs` to `stderr`.\n    This function serves as the default callback for catch.\n\n    Args:\n        verbosity: What level of traceback to print.\n            0-: No traceback.\n            0-10: Full information of arguments and key word arguments.\n            10-20: Stack trace to function calls.\n            40+: Function name and error messages.\n    \"\"\"\n\n    if verbosity &gt;= 0:\n        message = traceback.format_exc()\n        message += f\"\\nencoutered when calling {func}\"\n        if verbosity &lt;= 20:\n            message += \"\\n\\nstack:\\n\" + \"\\n\".join(traceback.format_stack()[:-2])\n        if verbosity &lt;= 10:\n            message += \"\\n\" + f\"args: {args}\\nkwargs: {kwargs}\"\n        try:\n            print(message, file=stderr, force=True)  # type: ignore\n        except TypeError:\n            print(message, file=stderr)\n</code></pre>"},{"location":"utils/io/","title":"IO","text":""},{"location":"utils/io/#danling.utils.io.is_json_serializable","title":"<code>is_json_serializable(obj)</code>","text":"<p>Check if <code>obj</code> is JSON serializable.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def is_json_serializable(obj: Any) -&gt; bool:\n    r\"\"\"\n    Check if `obj` is JSON serializable.\n    \"\"\"\n    try:\n        json.dumps(obj)\n        return True\n    except (TypeError, OverflowError):\n        return False\n</code></pre>"},{"location":"utils/io/#danling.utils.io.load","title":"<code>load(file, *args, **kwargs)</code>","text":"<p>Load any file with supported extensions.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def load(file: PathStr, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n    r\"\"\"\n    Load any file with supported extensions.\n    \"\"\"\n    if not os.path.isfile(file):\n        raise ValueError(f\"Trying to load {file!r} but it is not a file.\")\n    extension = os.path.splitext(file)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if not TORCH_AVAILABLE:\n            raise ImportError(f\"Trying to load {file!r} but torch is not installed.\")\n        return torch.load(file, *args, **kwargs)\n    if extension in NUMPY:\n        if not NUMPY_AVAILABLE:\n            raise ImportError(f\"Trying to load {file!r} but numpy is not installed.\")\n        return numpy.load(file, *args, **kwargs)\n    if extension in JSON:\n        with open(file) as fp:\n            return json.load(fp, *args, **kwargs)  # type: ignore\n    if extension in YAML:\n        with open(file) as fp:\n            return yaml.load(fp, *args, **kwargs)  # type: ignore\n    if extension in PICKLE:\n        with open(file, \"rb\") as fp:\n            return pickle.load(fp, *args, **kwargs)  # type: ignore\n    if extension in PANDAS_SUPPORTED:\n        return load_pandas(file, *args, **kwargs)\n    raise ValueError(f\"Tying to load {file!r} with unsupported extension={extension!r}\")\n</code></pre>"},{"location":"utils/io/#danling.utils.io.load_pandas","title":"<code>load_pandas(file, *args, **kwargs)</code>","text":"<p>Load any pandas data file with supported extensions.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def load_pandas(file: PathStr, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; Any:\n    r\"\"\"\n    Load any pandas data file with supported extensions.\n    \"\"\"\n    if not PANDAS_AVAILABLE:\n        raise ImportError(f\"Trying to load {file!r} but pandas is not installed.\")\n    if not os.path.isfile(file):\n        raise ValueError(f\"Trying to load {file!r} but it is not a file.\")\n    extension = os.path.splitext(file)[-1].lower()[1:]\n    if extension in PANDAS or extension in PICKLE:\n        return pandas.read_pickle(file, *args, **kwargs)\n    if extension in H5:\n        return pandas.read_hdf(file, *args, **kwargs)\n    if extension in CSV:\n        return pandas.read_csv(file, *args, **kwargs)\n    if extension in JSON:\n        return pandas.read_json(file, *args, **kwargs)\n    if extension in EXCEL:\n        return pandas.read_excel(file, *args, **kwargs)\n    if extension in XML:\n        return pandas.read_xml(file, *args, **kwargs)\n    if extension in SQL:\n        return pandas.read_sql(file, *args, **kwargs)\n    raise ValueError(f\"Tying to load {file!r} with unsupported extension={extension!r}\")\n</code></pre>"},{"location":"utils/io/#danling.utils.io.save","title":"<code>save(obj, file, *args, **kwargs)</code>","text":"<p>Save any file with supported extensions.</p> Source code in <code>danling/utils/io.py</code> Python<pre><code>def save(obj: Any, file: PathStr, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; File:\n    r\"\"\"\n    Save any file with supported extensions.\n    \"\"\"\n    extension = os.path.splitext(file)[-1].lower()[1:]\n    if extension in PYTORCH:\n        if not TORCH_AVAILABLE:\n            raise ImportError(f\"Trying to save {obj} to {file!r} but torch is not installed.\")\n        torch.save(obj, file, *args, **kwargs)\n    elif extension in NUMPY:\n        if not NUMPY_AVAILABLE:\n            raise ImportError(f\"Trying to save {obj} to {file!r} but numpy is not installed.\")\n        numpy.save(file, obj, *args, **kwargs)\n    elif extension in PANDAS:\n        if not PANDAS_AVAILABLE:\n            raise ImportError(f\"Trying to save {obj} to {file!r} but pandas is not installed.\")\n        pandas.to_pickle(obj, file, *args, **kwargs)\n    elif extension in CSV:\n        if isinstance(obj, pandas.DataFrame):\n            obj.to_csv(file, *args, **kwargs)\n        else:\n            raise NotImplementedError(f\"Trying to save {obj} to {file!r} but is not supported\")\n    elif extension in JSON:\n        if isinstance(obj, FlatDict):\n            obj.json(file)\n        else:\n            with open(file, \"w\") as fp:\n                json.dump(obj, fp, *args, **kwargs)  # type: ignore\n    elif extension in YAML:\n        if isinstance(obj, FlatDict):\n            obj.yaml(file)\n        else:\n            with open(file, \"w\") as fp:\n                yaml.dump(obj, fp, *args, **kwargs)  # type: ignore\n    elif extension in PICKLE:\n        with open(file, \"wb\") as fp:\n            pickle.dump(obj, fp, *args, **kwargs)  # type: ignore\n    else:\n        raise ValueError(f\"Tying to save {obj} to {file!r} with unsupported extension={extension!r}\")\n    return file\n</code></pre>"},{"location":"zh/about/license/","title":"License","text":"<p>\u200b\u7ffb\u8bd1\u200b</p> <p>\u200b\u672c\u6587\u200b\u5185\u5bb9\u200b\u4e3a\u200b\u7ffb\u8bd1\u200b\u7248\u672c\u200b\uff0c\u200b\u65e8\u5728\u200b\u4e3a\u200b\u7528\u6237\u200b\u63d0\u4f9b\u65b9\u4fbf\u200b\u3002 \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c3d\u529b\u200b\u786e\u4fdd\u200b\u7ffb\u8bd1\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\u3002 \u200b\u4f46\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u7ffb\u8bd1\u200b\u5185\u5bb9\u200b\u53ef\u80fd\u200b\u5305\u542b\u200b\u9519\u8bef\u200b\uff0c\u200b\u4ec5\u4f9b\u53c2\u8003\u200b\u3002 \u200b\u8bf7\u4ee5\u200b\u82f1\u6587\u200b\u539f\u6587\u200b\u4e3a\u51c6\u200b\u3002</p> <p>\u200b\u4e3a\u200b\u6ee1\u8db3\u200b\u5408\u89c4\u6027\u200b\u4e0e\u200b\u6267\u6cd5\u200b\u8981\u6c42\u200b\uff0c\u200b\u7ffb\u8bd1\u200b\u6587\u6863\u200b\u4e2d\u200b\u7684\u200b\u4efb\u4f55\u200b\u4e0d\u200b\u51c6\u786e\u200b\u6216\u200b\u6b67\u4e49\u200b\u4e4b\u5904\u200b\u5747\u200b\u4e0d\u200b\u5177\u6709\u200b\u7ea6\u675f\u529b\u200b\uff0c\u200b\u4e5f\u200b\u4e0d\u200b\u5177\u5907\u200b\u6cd5\u5f8b\u6548\u529b\u200b\u3002</p>"},{"location":"zh/about/license/#gnu-affero","title":"GNU AFFERO \u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1","text":"<p>\u200b\u7b2c\u200b3\u200b\u7248\u200b\uff0c2007\u200b\u5e74\u200b11\u200b\u6708\u200b19\u200b\u65e5\u200b</p> <p>\u200b\u7248\u6743\u6240\u6709\u200b \u00a9 2007 Free Software Foundation, Inc. https://fsf.org/</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p> <p>\u200b\u6bcf\u4e2a\u200b\u4eba\u200b\u90fd\u200b\u88ab\u200b\u5141\u8bb8\u200b\u590d\u5236\u200b\u548c\u200b\u5206\u53d1\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u6587\u4ef6\u200b\u7684\u200b\u9010\u5b57\u200b\u526f\u672c\u200b\uff0c\u200b\u4f46\u200b\u4e0d\u200b\u5141\u8bb8\u200b\u8fdb\u884c\u200b\u66f4\u6539\u200b\u3002</p>"},{"location":"zh/about/license/#_1","title":"\u5e8f\u8a00","text":"<p>GNU Affero \u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u662f\u200b\u4e00\u4e2a\u200b\u81ea\u7531\u200b\u7684\u200b\u3001\u200b\u5141\u8bb8\u200b\u590d\u5236\u200b\u7684\u200b\u8f6f\u4ef6\u200b\u548c\u200b\u5176\u4ed6\u200b\u7c7b\u578b\u200b\u4f5c\u54c1\u200b\u7684\u200b\u8bb8\u53ef\u200b\uff0c\u200b\u5728\u200b\u7f51\u7edc\u200b\u670d\u52a1\u5668\u8f6f\u4ef6\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5b83\u200b\u662f\u200b\u4e13\u95e8\u200b\u4e3a\u200b\u786e\u4fdd\u200b\u4e0e\u200b\u793e\u533a\u200b\u5408\u4f5c\u200b\u800c\u200b\u8bbe\u8ba1\u200b\u3002</p> <p>\u200b\u5927\u591a\u6570\u200b\u8f6f\u4ef6\u200b\u548c\u200b\u5176\u4ed6\u200b\u5b9e\u7528\u200b\u4f5c\u54c1\u200b\u7684\u200b\u8bb8\u53ef\u200b\u90fd\u200b\u662f\u200b\u4e3a\u4e86\u200b\u5265\u593a\u200b\u60a8\u200b\u5206\u4eab\u200b\u548c\u200b\u6539\u53d8\u200b\u4f5c\u54c1\u200b\u7684\u200b\u81ea\u7531\u200b\u3002\u200b\u76f8\u6bd4\u4e4b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u76ee\u7684\u200b\u662f\u200b\u4fdd\u8bc1\u200b\u60a8\u200b\u5206\u4eab\u200b\u548c\u200b\u6539\u53d8\u200b\u4e00\u4e2a\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u6240\u6709\u200b\u7248\u672c\u200b\u7684\u200b\u81ea\u7531\u200b\u2013\u200b\u786e\u4fdd\u200b\u5b83\u200b\u5bf9\u200b\u6240\u6709\u200b\u7528\u6237\u200b\u90fd\u200b\u662f\u200b\u81ea\u7531\u8f6f\u4ef6\u200b\u3002</p> <p>\u200b\u5f53\u200b\u6211\u4eec\u200b\u8c08\u8bba\u200b\u81ea\u7531\u8f6f\u4ef6\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u6307\u200b\u7684\u200b\u662f\u200b\u81ea\u7531\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u4ef7\u683c\u200b\u3002\u200b\u6211\u4eec\u200b\u7684\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u662f\u200b\u4e3a\u4e86\u200b\u786e\u4fdd\u60a8\u200b\u6709\u200b\u5206\u53d1\u200b\u81ea\u7531\u8f6f\u4ef6\u200b\u526f\u672c\u200b\u7684\u200b\u81ea\u7531\u200b\uff08\u200b\u5982\u679c\u200b\u60a8\u200b\u613f\u610f\u200b\uff0c\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u6536\u8d39\u200b\uff09\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u6536\u5230\u200b\u6e90\u4ee3\u7801\u200b\uff0c\u200b\u6216\u8005\u200b\u5982\u679c\u200b\u60a8\u200b\u60f3\u5f97\u5230\u200b\u5b83\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u6539\u53d8\u200b\u8f6f\u4ef6\u200b\u6216\u200b\u5728\u200b\u65b0\u200b\u7684\u200b\u81ea\u7531\u200b\u7a0b\u5e8f\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u5b83\u200b\u7684\u200b\u7247\u6bb5\u200b\uff0c\u200b\u800c\u4e14\u200b\u60a8\u200b\u77e5\u9053\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u505a\u200b\u8fd9\u4e9b\u200b\u4e8b\u60c5\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u5f00\u53d1\u8005\u200b\u901a\u8fc7\u200b\u4e24\u4e2a\u200b\u6b65\u9aa4\u200b\u4fdd\u62a4\u200b\u60a8\u200b\u7684\u200b\u6743\u5229\u200b\u3002(1)\u200b\u4e3b\u5f20\u200b\u8f6f\u4ef6\u200b\u7684\u200b\u7248\u6743\u200b\uff0c(2)\u200b\u5411\u200b\u60a8\u200b\u63d0\u4f9b\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\uff0c\u200b\u5141\u8bb8\u200b\u60a8\u200b\u5408\u6cd5\u200b\u5730\u200b\u590d\u5236\u200b\u3001\u200b\u5206\u53d1\u200b\u548c\u200b/\u200b\u6216\u200b\u4fee\u6539\u200b\u8be5\u8f6f\u4ef6\u200b\u3002</p> <p>\u200b\u634d\u536b\u200b\u6240\u6709\u200b\u7528\u6237\u200b\u81ea\u7531\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6b21\u8981\u200b\u597d\u5904\u200b\u662f\u200b\uff0c\u200b\u5982\u679c\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u66ff\u4ee3\u200b\u7248\u672c\u200b\u5f97\u5230\u200b\u5e7f\u6cdb\u200b\u4f7f\u7528\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4f9b\u200b\u5176\u4ed6\u200b\u5f00\u53d1\u8005\u200b\u4f7f\u7528\u200b\u3002\u200b\u8bb8\u591a\u200b\u81ea\u7531\u8f6f\u4ef6\u200b\u7684\u200b\u5f00\u53d1\u8005\u200b\u5bf9\u200b\u7531\u6b64\u200b\u4ea7\u751f\u200b\u7684\u200b\u5408\u4f5c\u200b\u611f\u5230\u200b\u632f\u594b\u200b\u548c\u200b\u9f13\u821e\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u5728\u200b\u7f51\u7edc\u200b\u670d\u52a1\u5668\u200b\u4e0a\u200b\u4f7f\u7528\u200b\u7684\u200b\u8f6f\u4ef6\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u7ed3\u679c\u200b\u53ef\u80fd\u200b\u65e0\u6cd5\u200b\u5b9e\u73b0\u200b\u3002GNU\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u5141\u8bb8\u200b\u5236\u4f5c\u200b\u4e00\u4e2a\u200b\u4fee\u6539\u200b\u8fc7\u200b\u7684\u200b\u7248\u672c\u200b\uff0c\u200b\u8ba9\u200b\u516c\u4f17\u200b\u5728\u200b\u670d\u52a1\u5668\u200b\u4e0a\u200b\u8bbf\u95ee\u200b\u5b83\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u9700\u8981\u200b\u5411\u200b\u516c\u4f17\u200b\u53d1\u5e03\u200b\u5176\u200b\u6e90\u4ee3\u7801\u200b\u3002</p> <p>GNU Affero\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u662f\u200b\u4e13\u95e8\u200b\u8bbe\u8ba1\u200b\u6765\u200b\u786e\u4fdd\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4fee\u6539\u200b\u540e\u200b\u7684\u200b\u6e90\u4ee3\u7801\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u793e\u533a\u200b\u4f7f\u7528\u200b\u3002\u200b\u5b83\u200b\u8981\u6c42\u200b\u7f51\u7edc\u200b\u670d\u52a1\u5668\u200b\u7684\u200b\u8fd0\u8425\u5546\u200b\u5411\u200b\u8be5\u200b\u670d\u52a1\u5668\u200b\u7684\u200b\u7528\u6237\u200b\u63d0\u4f9b\u200b\u8fd0\u884c\u200b\u5728\u200b\u90a3\u91cc\u200b\u7684\u200b\u4fee\u6539\u200b\u7248\u672c\u200b\u7684\u200b\u6e90\u4ee3\u7801\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5728\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u516c\u5f00\u200b\u8bbf\u95ee\u200b\u7684\u200b\u670d\u52a1\u5668\u200b\u4e0a\u200b\u516c\u5f00\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u4fee\u6539\u200b\u8fc7\u200b\u7684\u200b\u7248\u672c\u200b\uff0c\u200b\u4f7f\u200b\u516c\u4f17\u200b\u80fd\u591f\u200b\u83b7\u5f97\u200b\u4fee\u6539\u200b\u8fc7\u200b\u7684\u200b\u7248\u672c\u200b\u7684\u200b\u6e90\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4e00\u4e2a\u200b\u8f83\u200b\u65e9\u200b\u7684\u200b\u8bb8\u53ef\u8bc1\u200b\uff0c\u200b\u79f0\u4e3a\u200bAffero\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\uff0c\u200b\u7531\u200bAffero\u200b\u53d1\u5e03\u200b\uff0c\u200b\u65e8\u5728\u200b\u5b9e\u73b0\u200b\u7c7b\u4f3c\u200b\u76ee\u6807\u200b\u3002\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bb8\u53ef\u8bc1\u200b\uff0c\u200b\u4e0d\u662f\u200bAffero GPL\u200b\u7684\u200b\u4e00\u4e2a\u200b\u7248\u672c\u200b\uff0c\u200b\u4f46\u200bAffero\u200b\u5df2\u7ecf\u200b\u53d1\u5e03\u200b\u4e86\u200bAffero GPL\u200b\u7684\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7248\u672c\u200b\uff0c\u200b\u5141\u8bb8\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u8bb8\u53ef\u8bc1\u200b\u4e0b\u200b\u91cd\u65b0\u200b\u8bb8\u53ef\u200b\u3002</p> <p>\u200b\u5173\u4e8e\u200b\u590d\u5236\u200b\u3001\u200b\u5206\u53d1\u200b\u548c\u200b\u4fee\u6539\u200b\u7684\u200b\u786e\u5207\u200b\u6761\u6b3e\u200b\u548c\u200b\u6761\u4ef6\u200b\u5982\u4e0b\u200b\u3002</p>"},{"location":"zh/about/license/#_2","title":"\u6761\u6b3e\u200b\u4e0e\u200b\u6761\u4ef6","text":""},{"location":"zh/about/license/#0","title":"0. \u200b\u5b9a\u4e49\u200b.","text":"<p>\u201c\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u201d \u200b\u662f\u200b\u6307\u200bGNU Affero\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u7b2c\u4e09\u7248\u200b\u3002 \u201c\u200b\u7248\u6743\u200b\u201d \u200b\u4e5f\u200b\u6307\u200b\u9002\u7528\u200b\u4e8e\u200b\u5176\u4ed6\u200b\u7c7b\u578b\u200b\u4f5c\u54c1\u200b\u7684\u200b\u7c7b\u4f3c\u200b\u7248\u6743\u200b\u7684\u200b\u6cd5\u5f8b\u200b\uff0c\u200b\u5982\u200b\u534a\u5bfc\u4f53\u200b\u63a9\u6a21\u200b\u3002</p> <p>\u201c\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u201d \u200b\u662f\u200b\u6307\u200b\u5728\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u4e0b\u200b\u8bb8\u53ef\u200b\u7684\u200b\u4efb\u4f55\u200b\u6709\u200b\u7248\u6743\u200b\u7684\u200b\u4f5c\u54c1\u200b\u3002\u200b\u6bcf\u4e2a\u200b\u88ab\u200b\u8bb8\u53ef\u200b\u4eba\u200b\u90fd\u200b\u88ab\u200b\u79f0\u547c\u200b\u4e3a\u200b \u201c\u200b\u60a8\u200b\u201d\u3002\u201d\u200b\u88ab\u200b\u8bb8\u53ef\u200b\u4eba\u200b\u201d \u200b\u548c\u200b \u201c\u200b\u63a5\u53d7\u8005\u200b\u201d \u200b\u53ef\u4ee5\u200b\u662f\u200b\u4e2a\u4eba\u200b\u6216\u200b\u7ec4\u7ec7\u200b\u3002</p> <p>\u201c\u200b\u4fee\u6539\u200b\u201d \u200b\u4f5c\u54c1\u200b\u662f\u200b\u6307\u4ee5\u200b\u9700\u8981\u200b\u7248\u6743\u200b\u8bb8\u53ef\u200b\u7684\u200b\u65b9\u5f0f\u200b\u590d\u5236\u200b\u6216\u200b\u6539\u7f16\u200b\u8be5\u200b\u4f5c\u54c1\u200b\u7684\u200b\u5168\u90e8\u200b\u6216\u200b\u90e8\u5206\u200b\u5185\u5bb9\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u5236\u4f5c\u200b\u4e00\u4e2a\u200b\u5b8c\u5168\u200b\u7684\u200b\u526f\u672c\u200b\u3002\u200b\u7531\u6b64\u200b\u4ea7\u751f\u200b\u7684\u200b\u4f5c\u54c1\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u65e9\u671f\u200b\u4f5c\u54c1\u200b\u7684\u200b \u201c\u200b\u4fee\u6539\u7248\u200b\u201d \u200b\u6216\u200b \u201c\u200b\u57fa\u4e8e\u200b\u201d \u200b\u65e9\u671f\u200b\u4f5c\u54c1\u200b\u7684\u200b\u4f5c\u54c1\u200b\u3002 \u200b\u4e00\u4e2a\u200b \u201c\u200b\u6db5\u76d6\u200b\u7684\u200b\u4f5c\u54c1\u200b\u201d \u200b\u662f\u200b\u6307\u200b\u672a\u7ecf\u200b\u4fee\u6539\u200b\u7684\u200b\u7a0b\u5e8f\u200b\u6216\u200b\u57fa\u4e8e\u200b\u8be5\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u4f5c\u54c1\u200b\u3002</p> <p>\u201c\u200b\u4f20\u64ad\u200b\u201d \u200b\u4f5c\u54c1\u200b\u662f\u200b\u6307\u200b\u5728\u200b\u672a\u7ecf\u8bb8\u53ef\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5bf9\u200b\u4f5c\u54c1\u200b\u505a\u200b\u4efb\u4f55\u200b\u4e8b\u60c5\u200b\uff0c\u200b\u4f7f\u200b\u60a8\u200b\u5728\u200b\u9002\u7528\u200b\u7684\u200b\u7248\u6743\u6cd5\u200b\u4e0b\u200b\u627f\u62c5\u200b\u76f4\u63a5\u200b\u6216\u200b\u95f4\u63a5\u200b\u7684\u200b\u4fb5\u6743\u200b\u8d23\u4efb\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u8ba1\u7b97\u673a\u200b\u4e0a\u200b\u6267\u884c\u200b\u6216\u200b\u4fee\u6539\u200b\u79c1\u4eba\u200b\u526f\u672c\u200b\u9664\u5916\u200b\u3002\u200b\u4f20\u64ad\u200b\u5305\u62ec\u200b\u590d\u5236\u200b\u3001\u200b\u5206\u53d1\u200b\uff08\u200b\u65e0\u8bba\u200b\u662f\u5426\u200b\u4fee\u6539\u200b\uff09\u3001\u200b\u5411\u200b\u516c\u4f17\u200b\u63d0\u4f9b\u200b\uff0c\u200b\u5728\u200b\u4e00\u4e9b\u200b\u56fd\u5bb6\u200b\u8fd8\u200b\u5305\u62ec\u200b\u5176\u4ed6\u200b\u6d3b\u52a8\u200b\u3002 \u200b\u4f20\u64ad\u200b\u201d \u200b\u4f5c\u54c1\u200b\u662f\u200b\u6307\u4f7f\u200b\u5176\u4ed6\u200b\u5404\u65b9\u200b\u80fd\u591f\u200b\u5236\u4f5c\u200b\u6216\u200b\u63a5\u53d7\u200b\u526f\u672c\u200b\u7684\u200b\u4efb\u4f55\u200b\u4e00\u79cd\u200b\u4f20\u64ad\u200b\u3002\u200b\u4ec5\u4ec5\u200b\u662f\u200b\u901a\u8fc7\u200b\u8ba1\u7b97\u673a\u7f51\u7edc\u200b\u4e0e\u200b\u7528\u6237\u200b\u4e92\u52a8\u200b\uff0c\u200b\u800c\u200b\u6ca1\u6709\u200b\u8f6c\u8ba9\u200b\u526f\u672c\u200b\uff0c\u200b\u5e76\u200b\u4e0d\u662f\u200b\u4f20\u64ad\u200b\u3002</p> <p>\u200b\u4ea4\u4e92\u5f0f\u200b\u7528\u6237\u754c\u9762\u200b\u663e\u793a\u200b \u201c\u200b\u9002\u5f53\u200b\u7684\u200b\u6cd5\u5f8b\u200b\u58f0\u660e\u200b\u201d \u200b\u7684\u200b\u7a0b\u5ea6\u200b\u662f\u200b\uff0c\u200b\u5b83\u200b\u5305\u62ec\u200b\u4e00\u4e2a\u200b\u65b9\u4fbf\u200b\u548c\u200b\u663e\u773c\u200b\u7684\u200b\u529f\u80fd\u200b\uff0c(1)\u200b\u663e\u793a\u200b\u9002\u5f53\u200b\u7684\u200b\u7248\u6743\u200b\u58f0\u660e\u200b\uff0c(2)\u200b\u544a\u8bc9\u200b\u7528\u6237\u200b\u8be5\u200b\u4f5c\u54c1\u200b\u6ca1\u6709\u200b\u4fdd\u8bc1\u200b\uff08\u200b\u9664\u4e86\u200b\u63d0\u4f9b\u200b\u4fdd\u8bc1\u200b\u7684\u200b\u8303\u56f4\u200b\uff09\uff0c\u200b\u88ab\u200b\u8bb8\u53ef\u200b\u4eba\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u672c\u200b\u8bb8\u53ef\u200b\u4f20\u8fbe\u200b\u8be5\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5982\u4f55\u200b\u67e5\u770b\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u526f\u672c\u200b\u3002\u200b\u5982\u679c\u200b\u754c\u9762\u200b\u5448\u73b0\u200b\u7684\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7528\u6237\u200b\u547d\u4ee4\u200b\u6216\u200b\u9009\u9879\u200b\u7684\u200b\u5217\u8868\u200b\uff0c\u200b\u5982\u200b\u83dc\u5355\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5217\u8868\u200b\u4e2d\u200b\u7684\u200b\u7a81\u51fa\u200b\u9879\u76ee\u200b\u5c31\u200b\u7b26\u5408\u200b\u8fd9\u4e00\u200b\u6807\u51c6\u200b\u3002</p>"},{"location":"zh/about/license/#1","title":"1. \u200b\u6e90\u4ee3\u7801\u200b.","text":"<p>\u200b\u4f5c\u54c1\u200b\u7684\u200b \u201c\u200b\u6e90\u4ee3\u7801\u200b\u201d \u200b\u662f\u200b\u6307\u200b\u5bf9\u200b\u4f5c\u54c1\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b\u7684\u200b\u9996\u9009\u200b\u5f62\u5f0f\u200b\u3002\u201d\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u201d \u200b\u662f\u200b\u6307\u200b\u4f5c\u54c1\u200b\u7684\u200b\u4efb\u4f55\u200b\u975e\u200b\u6e90\u7801\u200b\u5f62\u5f0f\u200b\u3002</p> <p>\u201c\u200b\u6807\u51c6\u63a5\u53e3\u200b\u201d \u200b\u662f\u200b\u6307\u200b\u7531\u200b\u516c\u8ba4\u200b\u7684\u200b\u6807\u51c6\u200b\u673a\u6784\u200b\u5b9a\u4e49\u200b\u7684\u200b\u5b98\u65b9\u200b\u6807\u51c6\u200b\u7684\u200b\u63a5\u53e3\u200b\uff0c\u200b\u6216\u8005\u200b\u5728\u200b\u4e3a\u200b\u67d0\u200b\u4e00\u200b\u7279\u5b9a\u200b\u7f16\u7a0b\u8bed\u8a00\u200b\u6307\u5b9a\u200b\u63a5\u53e3\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6307\u5728\u200b\u4f7f\u7528\u200b\u8be5\u200b\u8bed\u8a00\u200b\u7684\u200b\u5f00\u53d1\u8005\u200b\u4e2d\u200b\u5e7f\u6cdb\u200b\u4f7f\u7528\u200b\u7684\u200b\u63a5\u53e3\u200b\u3002</p> <p>\u200b\u53ef\u200b\u6267\u884c\u200b\u4f5c\u54c1\u200b\u7684\u200b \u201c\u200b\u7cfb\u7edf\u200b\u5e93\u200b\u201d \u200b\u5305\u62ec\u200b\u9664\u200b\u4f5c\u54c1\u200b\u6574\u4f53\u200b\u4ee5\u5916\u200b\u7684\u200b\u4efb\u4f55\u200b\u4e1c\u897f\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u4e1c\u897f\u200b(a)\u200b\u4ee5\u200b\u6b63\u5e38\u200b\u7684\u200b\u5f62\u5f0f\u200b\u6253\u5305\u200b\u4e00\u4e2a\u200b\u4e3b\u8981\u200b\u90e8\u4ef6\u200b\uff0c\u200b\u4f46\u200b\u4e0d\u662f\u200b\u8be5\u200b\u4e3b\u8981\u200b\u90e8\u4ef6\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\uff0c\u200b\u4ee5\u53ca\u200b(b)\u200b\u4ec5\u200b\u7528\u4e8e\u200b\u4f7f\u200b\u4f5c\u54c1\u200b\u4e0e\u200b\u8be5\u200b\u4e3b\u8981\u200b\u90e8\u4ef6\u200b\u4e00\u8d77\u200b\u4f7f\u7528\u200b\uff0c\u200b\u6216\u200b\u7528\u4e8e\u200b\u5b9e\u73b0\u200b\u4e00\u4e2a\u200b\u6807\u51c6\u63a5\u53e3\u200b\uff0c\u200b\u8be5\u200b\u63a5\u53e3\u200b\u7684\u200b\u5b9e\u73b0\u200b\u5df2\u4ee5\u200b\u6e90\u4ee3\u7801\u200b\u5f62\u5f0f\u200b\u5411\u200b\u516c\u4f17\u200b\u63d0\u4f9b\u200b\u3002\u200b\u8fd9\u91cc\u200b\u7684\u200b \u201c\u200b\u4e3b\u8981\u200b\u90e8\u4ef6\u200b\u201d \u200b\u662f\u200b\u6307\u200b\u53ef\u200b\u6267\u884c\u200b\u4f5c\u54c1\u200b\u6240\u200b\u8fd0\u884c\u200b\u7684\u200b\u7279\u5b9a\u200b\u64cd\u4f5c\u7cfb\u7edf\u200b\uff08\u200b\u5982\u679c\u200b\u6709\u200b\u7684\u8bdd\u200b\uff09\u200b\u7684\u200b\u4e3b\u8981\u200b\u57fa\u672c\u200b\u90e8\u4ef6\u200b\uff08\u200b\u5185\u6838\u200b\u3001\u200b\u7a97\u53e3\u200b\u7cfb\u7edf\u200b\u7b49\u200b\uff09\uff0c\u200b\u6216\u200b\u7528\u4e8e\u200b\u5236\u4f5c\u200b\u8be5\u200b\u4f5c\u54c1\u200b\u7684\u200b\u7f16\u8bd1\u5668\u200b\uff0c\u200b\u6216\u200b\u7528\u4e8e\u200b\u8fd0\u884c\u200b\u8be5\u200b\u4f5c\u54c1\u200b\u7684\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u89e3\u91ca\u5668\u200b\u3002</p> <p>\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u5f62\u5f0f\u200b\u7684\u200b\u4f5c\u54c1\u200b\u7684\u200b \u201c\u200b\u76f8\u5e94\u200b\u6e90\u4ee3\u7801\u200b\u201d \u200b\u662f\u200b\u6307\u200b\u751f\u6210\u200b\u3001\u200b\u5b89\u88c5\u200b\u548c\u200b\uff08\u200b\u5bf9\u4e8e\u200b\u53ef\u200b\u6267\u884c\u200b\u4f5c\u54c1\u200b\uff09\u200b\u8fd0\u884c\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u4ee5\u53ca\u200b\u4fee\u6539\u200b\u4f5c\u54c1\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6240\u6709\u200b\u6e90\u4ee3\u7801\u200b\uff0c\u200b\u5305\u62ec\u200b\u63a7\u5236\u200b\u8fd9\u4e9b\u200b\u6d3b\u52a8\u200b\u7684\u200b\u811a\u672c\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u5b83\u200b\u4e0d\u200b\u5305\u62ec\u200b\u4f5c\u54c1\u200b\u7684\u200b\u7cfb\u7edf\u200b\u5e93\u200b\uff0c\u200b\u4e5f\u200b\u4e0d\u200b\u5305\u62ec\u200b\u5728\u200b\u6267\u884c\u200b\u8fd9\u4e9b\u200b\u6d3b\u52a8\u200b\u65f6\u200b\u672a\u7ecf\u200b\u4fee\u6539\u200b\u4f46\u200b\u4e0d\u200b\u5c5e\u4e8e\u200b\u4f5c\u54c1\u200b\u7684\u200b\u901a\u7528\u200b\u5de5\u5177\u200b\u6216\u200b\u666e\u904d\u200b\u53ef\u7528\u200b\u7684\u200b\u514d\u8d39\u200b\u7a0b\u5e8f\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u76f8\u5e94\u200b\u6e90\u200b\u5305\u62ec\u200b\u4e0e\u200b\u4f5c\u54c1\u200b\u7684\u200b\u6e90\u6587\u4ef6\u200b\u76f8\u5173\u200b\u7684\u200b\u63a5\u53e3\u5b9a\u4e49\u200b\u6587\u4ef6\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u4f5c\u54c1\u200b\u4e13\u95e8\u200b\u8bbe\u8ba1\u200b\u7684\u200b\u5171\u4eab\u200b\u5e93\u200b\u548c\u200b\u52a8\u6001\u200b\u94fe\u63a5\u200b\u7684\u200b\u5b50\u7a0b\u5e8f\u200b\u7684\u200b\u6e90\u4ee3\u7801\u200b\uff0c\u200b\u4f8b\u5982\u200b\u901a\u8fc7\u200b\u4eb2\u5bc6\u200b\u7684\u200b\u6570\u636e\u901a\u4fe1\u200b\u6216\u200b\u63a7\u5236\u6d41\u200b\u5728\u200b\u8fd9\u4e9b\u200b\u5b50\u7a0b\u5e8f\u200b\u548c\u200b\u4f5c\u54c1\u200b\u7684\u200b\u5176\u4ed6\u200b\u90e8\u5206\u200b\u4e4b\u95f4\u200b\u3002</p> <p>\u200b\u76f8\u5e94\u200b\u6e90\u200b\u4e0d\u200b\u9700\u8981\u200b\u5305\u62ec\u200b\u7528\u6237\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u76f8\u5e94\u200b\u6e90\u200b\u7684\u200b\u5176\u4ed6\u200b\u90e8\u5206\u200b\u81ea\u52a8\u200b\u91cd\u65b0\u200b\u751f\u6210\u200b\u7684\u200b\u4efb\u4f55\u200b\u4e1c\u897f\u200b\u3002</p> <p>\u200b\u6e90\u4ee3\u7801\u200b\u5f62\u5f0f\u200b\u7684\u200b\u4f5c\u54c1\u200b\u7684\u200b\u76f8\u5e94\u200b\u6e90\u200b\u662f\u200b\u6307\u200b\u540c\u4e00\u200b\u4f5c\u54c1\u200b\u3002</p>"},{"location":"zh/about/license/#2","title":"2. \u200b\u57fa\u672c\u200b\u6743\u9650\u200b.","text":"<p>\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u6388\u4e88\u200b\u7684\u200b\u6240\u6709\u200b\u6743\u5229\u200b\u90fd\u200b\u662f\u200b\u5728\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u7248\u6743\u200b\u671f\u9650\u5185\u200b\u6388\u4e88\u200b\u7684\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5728\u200b\u6ee1\u8db3\u200b\u6240\u8ff0\u200b\u6761\u4ef6\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u662f\u200b\u4e0d\u53ef\u200b\u64a4\u6d88\u200b\u7684\u200b\u3002\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u660e\u786e\u200b\u80af\u5b9a\u200b\u4e86\u200b\u60a8\u200b\u5bf9\u200b\u8fd0\u884c\u200b\u672a\u7ecf\u200b\u4fee\u6539\u200b\u7684\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u65e0\u9650\u200b\u8bb8\u53ef\u200b\u3002\u200b\u53ea\u6709\u200b\u5728\u200b\u8f93\u51fa\u200b\u7684\u200b\u5185\u5bb9\u200b\u6784\u6210\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u7684\u200b\u4f5c\u54c1\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u8fd0\u884c\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u4f5c\u54c1\u200b\u7684\u200b\u8f93\u51fa\u200b\u624d\u200b\u53d7\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u4fdd\u62a4\u200b\u3002\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u627f\u8ba4\u200b\u60a8\u200b\u7684\u200b\u5408\u7406\u200b\u4f7f\u7528\u6743\u200b\u6216\u200b\u7248\u6743\u6cd5\u200b\u6240\u200b\u89c4\u5b9a\u200b\u7684\u200b\u5176\u4ed6\u200b\u540c\u7b49\u200b\u6743\u5229\u200b\u3002</p> <p>\u200b\u53ea\u8981\u200b\u60a8\u200b\u7684\u200b\u8bb8\u53ef\u8bc1\u200b\u4ecd\u7136\u200b\u6709\u6548\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u65e0\u6761\u4ef6\u200b\u5730\u200b\u5236\u4f5c\u200b\u3001\u200b\u8fd0\u884c\u200b\u548c\u200b\u4f20\u64ad\u200b\u60a8\u200b\u6ca1\u6709\u200b\u8f6c\u8fbe\u200b\u7684\u200b\u6db5\u76d6\u200b\u4f5c\u54c1\u200b\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6db5\u76d6\u200b\u7684\u200b\u4f5c\u54c1\u200b\u4f20\u8fbe\u200b\u7ed9\u200b\u5176\u4ed6\u4eba\u200b\uff0c\u200b\u552f\u4e00\u200b\u7684\u200b\u76ee\u7684\u200b\u662f\u200b\u8ba9\u200b\u4ed6\u4eec\u200b\u4e13\u95e8\u200b\u4e3a\u200b\u60a8\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b\uff0c\u200b\u6216\u200b\u4e3a\u200b\u60a8\u200b\u63d0\u4f9b\u200b\u8fd0\u884c\u200b\u8fd9\u4e9b\u200b\u4f5c\u54c1\u200b\u7684\u200b\u8bbe\u65bd\u200b\uff0c\u200b\u524d\u63d0\u200b\u662f\u200b\u60a8\u200b\u5728\u200b\u4f20\u8fbe\u200b\u6240\u6709\u200b\u60a8\u200b\u4e0d\u200b\u63a7\u5236\u200b\u7248\u6743\u200b\u7684\u200b\u6750\u6599\u200b\u65f6\u200b\u9075\u5b88\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u6761\u6b3e\u200b\u3002\u200b\u90a3\u4e9b\u200b\u4e3a\u200b\u60a8\u200b\u5236\u4f5c\u200b\u6216\u200b\u8fd0\u884c\u200b\u6240\u6d89\u200b\u4f5c\u54c1\u200b\u7684\u200b\u4eba\u200b\u5fc5\u987b\u200b\u5b8c\u5168\u200b\u4ee3\u8868\u200b\u60a8\u200b\uff0c\u200b\u5728\u200b\u60a8\u200b\u7684\u200b\u6307\u5bfc\u200b\u548c\u200b\u63a7\u5236\u200b\u4e0b\u200b\uff0c\u200b\u6309\u7167\u200b\u7981\u6b62\u200b\u4ed6\u4eec\u200b\u5728\u200b\u4e0e\u200b\u60a8\u200b\u7684\u200b\u5173\u7cfb\u200b\u4e4b\u5916\u200b\u5236\u4f5c\u200b\u60a8\u200b\u7684\u200b\u7248\u6743\u200b\u6750\u6599\u200b\u7684\u200b\u4efb\u4f55\u200b\u526f\u672c\u200b\u7684\u200b\u6761\u6b3e\u200b\u6765\u200b\u8fdb\u884c\u200b\u3002</p> <p>\u200b\u5728\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4ec5\u200b\u5728\u200b\u4e0b\u8ff0\u200b\u6761\u4ef6\u200b\u4e0b\u200b\u5141\u8bb8\u200b\u8f6c\u8ba9\u200b\u3002\u200b\u4e0d\u200b\u5141\u8bb8\u200b\u8f6c\u200b\u6388\u6743\u200b\uff1b\u200b\u7b2c\u200b10\u200b\u6761\u200b\u89c4\u5b9a\u200b\u6ca1\u6709\u200b\u5fc5\u8981\u200b\u3002</p>"},{"location":"zh/about/license/#3","title":"3. \u200b\u4ece\u200b\u53cd\u200b\u89c4\u907f\u200b\u6cd5\u4e2d\u200b\u4fdd\u62a4\u200b\u7528\u6237\u200b\u7684\u200b\u5408\u6cd5\u6743\u5229\u200b.","text":"<p>\u200b\u6839\u636e\u200b\u4efb\u4f55\u200b\u5c65\u884c\u200b1996\u200b\u5e74\u200b12\u200b\u6708\u200b20\u200b\u65e5\u200b\u901a\u8fc7\u200b\u7684\u200b\u4e16\u754c\u77e5\u8bc6\u4ea7\u6743\u7ec4\u7ec7\u200b\u7248\u6743\u200b\u6761\u7ea6\u200b\u7b2c\u200b11\u200b\u6761\u200b\u89c4\u5b9a\u200b\u7684\u200b\u4e49\u52a1\u200b\u7684\u200b\u9002\u7528\u6cd5\u5f8b\u200b\uff0c\u200b\u6216\u200b\u7981\u6b62\u200b\u6216\u200b\u9650\u5236\u200b\u89c4\u907f\u200b\u6b64\u7c7b\u200b\u63aa\u65bd\u200b\u7684\u200b\u7c7b\u4f3c\u200b\u6cd5\u5f8b\u200b\uff0c\u200b\u4efb\u4f55\u200b\u6db5\u76d6\u200b\u7684\u200b\u4f5c\u54c1\u200b\u90fd\u200b\u4e0d\u5f97\u200b\u88ab\u200b\u89c6\u4e3a\u200b\u6709\u6548\u200b\u6280\u672f\u200b\u63aa\u65bd\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\u3002</p> <p>\u200b\u5f53\u200b\u60a8\u200b\u4f20\u8fbe\u200b\u4e00\u4e2a\u200b\u6db5\u76d6\u200b\u7684\u200b\u4f5c\u54c1\u200b\u65f6\u200b\uff0c\u200b\u60a8\u200b\u653e\u5f03\u200b\u4efb\u4f55\u200b\u7981\u6b62\u200b\u89c4\u907f\u200b\u6280\u672f\u200b\u63aa\u65bd\u200b\u7684\u200b\u6cd5\u5f8b\u200b\u6743\u529b\u200b\uff0c\u200b\u53ea\u8981\u200b\u8fd9\u79cd\u200b\u89c4\u907f\u200b\u662f\u200b\u901a\u8fc7\u200b\u5bf9\u200b\u6db5\u76d6\u200b\u7684\u200b\u4f5c\u54c1\u200b\u884c\u4f7f\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u4e0b\u200b\u7684\u200b\u6743\u5229\u200b\u800c\u200b\u5b9e\u73b0\u200b\u7684\u200b\uff0c\u200b\u5e76\u4e14\u200b\u60a8\u200b\u5426\u8ba4\u200b\u6709\u200b\u4efb\u4f55\u200b\u9650\u5236\u200b\u64cd\u4f5c\u200b\u6216\u200b\u4fee\u6539\u200b\u4f5c\u54c1\u200b\u7684\u200b\u610f\u56fe\u200b\uff0c\u200b\u4ee5\u200b\u4f5c\u4e3a\u200b\u5bf9\u200b\u4f5c\u54c1\u200b\u7684\u200b\u7528\u6237\u200b\u5f3a\u5236\u6267\u884c\u200b\u60a8\u200b\u6216\u200b\u7b2c\u4e09\u65b9\u200b\u7981\u6b62\u200b\u89c4\u907f\u200b\u6280\u672f\u200b\u63aa\u65bd\u200b\u7684\u200b\u6cd5\u5f8b\u200b\u6743\u5229\u200b\u7684\u200b\u624b\u6bb5\u200b\u3002</p>"},{"location":"zh/about/license/#4","title":"4. \u200b\u4f20\u9012\u200b\u9010\u5b57\u200b\u62f7\u8d1d\u200b.","text":"<p>\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6536\u5230\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u6e90\u4ee3\u7801\u200b\u540e\u200b\uff0c\u200b\u4ee5\u200b\u4efb\u4f55\u200b\u5a92\u4ecb\u200b\u4f20\u9012\u200b\u5176\u200b\u9010\u5b57\u200b\u62f7\u8d1d\u200b\uff0c\u200b\u4f46\u200b\u60a8\u200b\u5fc5\u987b\u200b\u5728\u200b\u6bcf\u4efd\u200b\u62f7\u8d1d\u200b\u4e0a\u200b\u9192\u76ee\u200b\u5730\u200b\u3001\u200b\u9002\u5f53\u200b\u5730\u200b\u53d1\u5e03\u200b\u9002\u5f53\u200b\u7684\u200b\u7248\u6743\u200b\u58f0\u660e\u200b\uff1b\u200b\u4fdd\u6301\u200b\u6240\u6709\u200b\u8bf4\u660e\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u548c\u200b\u6839\u636e\u200b\u7b2c\u200b7\u200b\u6761\u200b\u6dfb\u52a0\u200b\u7684\u200b\u4efb\u4f55\u200b\u975e\u200b\u8bb8\u53ef\u200b\u6761\u6b3e\u200b\u9002\u7528\u200b\u4e8e\u200b\u4ee3\u7801\u200b\u7684\u200b\u58f0\u660e\u200b\u5b8c\u6574\u65e0\u7f3a\u200b\uff1b\u200b\u4fdd\u6301\u200b\u6240\u6709\u200b\u5173\u4e8e\u200b\u6ca1\u6709\u200b\u4efb\u4f55\u200b\u4fdd\u8bc1\u200b\u7684\u200b\u58f0\u660e\u200b\u5b8c\u6574\u65e0\u7f3a\u200b\uff1b\u200b\u5e76\u200b\u5c06\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u62f7\u8d1d\u200b\u4e0e\u200b\u7a0b\u5e8f\u200b\u4e00\u8d77\u200b\u4ea4\u7ed9\u200b\u6240\u6709\u200b\u63a5\u6536\u8005\u200b\u3002</p> <p>\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u6bcf\u4efd\u200b\u62f7\u8d1d\u200b\u6536\u53d6\u200b\u4efb\u4f55\u200b\u8d39\u7528\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4e0d\u200b\u6536\u53d6\u200b\u4efb\u4f55\u200b\u8d39\u7528\u200b\uff0c\u200b\u60a8\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u63d0\u4f9b\u200b\u6709\u507f\u200b\u7684\u200b\u652f\u6301\u200b\u6216\u200b\u4fdd\u4fee\u200b\u4fdd\u62a4\u200b\u3002</p>"},{"location":"zh/about/license/#5","title":"5. \u200b\u4f20\u9012\u200b\u4fee\u6539\u200b\u540e\u200b\u7684\u200b\u6e90\u200b\u7248\u672c\u200b.","text":"<p>\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u7b2c\u200b 4 \u200b\u8282\u200b\u7684\u200b\u6761\u6b3e\u200b\uff0c\u200b\u4ee5\u200b\u6e90\u4ee3\u7801\u200b\u7684\u200b\u5f62\u5f0f\u200b\u4f20\u8fbe\u200b\u57fa\u4e8e\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u6216\u200b\u6839\u636e\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u4fee\u6539\u200b\u800c\u200b\u4ea7\u751f\u200b\u7684\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u4f46\u200b\u60a8\u200b\u5fc5\u987b\u200b\u6ee1\u8db3\u200b\u4ee5\u4e0b\u200b\u6240\u6709\u200b\u6761\u4ef6\u200b:</p> <p>a) \u200b\u4f5c\u54c1\u200b\u5fc5\u987b\u200b\u6709\u200b\u9192\u76ee\u200b\u7684\u200b\u58f0\u660e\u200b\uff0c\u200b\u8bf4\u660e\u200b\u60a8\u200b\u4fee\u6539\u200b\u4e86\u200b\u5b83\u200b\uff0c\u200b\u5e76\u200b\u7ed9\u51fa\u200b\u76f8\u5173\u200b\u7684\u200b\u65e5\u671f\u200b\u3002 b) \u200b\u4f5c\u54c1\u200b\u5fc5\u987b\u200b\u6709\u200b\u9192\u76ee\u200b\u7684\u200b\u58f0\u660e\u200b\uff0c\u200b\u8bf4\u660e\u200b\u5b83\u200b\u662f\u200b\u6839\u636e\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u548c\u200b\u6839\u636e\u200b\u7b2c\u200b7\u200b\u6761\u200b\u589e\u52a0\u200b\u7684\u200b\u6761\u4ef6\u200b\u53d1\u5e03\u200b\u7684\u200b\u3002\u200b\u8fd9\u4e00\u200b\u8981\u6c42\u200b\u4fee\u6539\u200b\u4e86\u200b\u7b2c\u200b4\u200b\u8282\u4e2d\u200b \u201c\u200b\u4fdd\u6301\u200b\u6240\u6709\u200b\u901a\u77e5\u200b\u7684\u200b\u5b8c\u6574\u6027\u200b\u201d \u200b\u7684\u200b\u8981\u6c42\u200b\u3002 c) \u200b\u60a8\u200b\u5fc5\u987b\u200b\u6839\u636e\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u5c06\u200b\u6574\u4e2a\u200b\u4f5c\u54c1\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u6574\u4f53\u200b\u8bb8\u53ef\u200b\u7ed9\u200b\u4efb\u4f55\u200b\u62e5\u6709\u200b\u5176\u200b\u526f\u672c\u200b\u7684\u200b\u4eba\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u5c06\u200b\u4e0e\u200b\u4efb\u4f55\u200b\u9002\u7528\u200b\u7684\u200b\u7b2c\u200b7\u200b\u6761\u200b\u9644\u52a0\u200b\u6761\u6b3e\u200b\u4e00\u8d77\u200b\uff0c\u200b\u9002\u7528\u200b\u4e8e\u200b\u6574\u4e2a\u200b\u4f5c\u54c1\u200b\u53ca\u5176\u200b\u6240\u6709\u200b\u90e8\u5206\u200b\uff0c\u200b\u65e0\u8bba\u200b\u5b83\u4eec\u200b\u662f\u200b\u5982\u4f55\u200b\u5305\u88c5\u200b\u7684\u200b\u3002\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u4e0d\u200b\u5141\u8bb8\u200b\u4ee5\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u65b9\u5f0f\u200b\u8bb8\u53ef\u200b\u8be5\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u4f46\u200b\u5982\u679c\u200b\u60a8\u200b\u5df2\u7ecf\u200b\u5355\u72ec\u200b\u6536\u5230\u200b\u4e86\u200b\u8fd9\u79cd\u200b\u8bb8\u53ef\u200b\uff0c\u200b\u5b83\u200b\u4e5f\u200b\u4e0d\u4f1a\u200b\u4f7f\u200b\u8fd9\u79cd\u200b\u8bb8\u53ef\u200b\u5931\u6548\u200b\u3002 \u200b\u5982\u679c\u200b\u4e00\u4e2a\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u4f5c\u54c1\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u5355\u72ec\u200b\u548c\u200b\u72ec\u7acb\u200b\u7684\u200b\u4f5c\u54c1\u200b\u7684\u200b\u6c47\u7f16\u200b\uff0c\u200b\u5176\u200b\u6027\u8d28\u200b\u4e0d\u662f\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u4f5c\u54c1\u200b\u7684\u200b\u5ef6\u4f38\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6ca1\u6709\u200b\u4e0e\u200b\u4e4b\u200b\u7ed3\u5408\u200b\u4ee5\u200b\u5f62\u6210\u200b\u66f4\u5927\u200b\u7684\u200b\u7a0b\u5e8f\u200b\uff0c\u200b\u5728\u200b\u5b58\u50a8\u200b\u6216\u200b\u5206\u53d1\u200b\u5a92\u4ecb\u200b\u7684\u200b\u67d0\u200b\u4e00\u5377\u200b\u4e0a\u200b\uff0c\u200b\u5982\u679c\u200b\u8be5\u200b\u6c47\u7f16\u200b\u53ca\u5176\u200b\u4ea7\u751f\u200b\u7684\u200b\u7248\u6743\u200b\u6ca1\u6709\u200b\u88ab\u200b\u7528\u6765\u200b\u9650\u5236\u200b\u6c47\u7f16\u200b\u7528\u6237\u200b\u7684\u200b\u8bbf\u95ee\u200b\u6216\u200b\u6cd5\u5f8b\u200b\u6743\u5229\u200b\uff0c\u200b\u8d85\u51fa\u200b\u5355\u4e2a\u200b\u4f5c\u54c1\u200b\u5141\u8bb8\u200b\u7684\u200b\u8303\u56f4\u200b\uff0c\u200b\u5219\u200b\u88ab\u200b\u79f0\u4e3a\u200b \u201c\u200b\u805a\u5408\u200b\u201d\u3002\u200b\u5c06\u200b\u4e00\u4e2a\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u7684\u200b\u4f5c\u54c1\u200b\u5305\u542b\u200b\u5728\u200b\u4e00\u4e2a\u200b\u603b\u4f53\u200b\u4e2d\u200b\u5e76\u200b\u4e0d\u200b\u5bfc\u81f4\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u9002\u7528\u200b\u4e8e\u200b\u603b\u4f53\u200b\u7684\u200b\u5176\u4ed6\u200b\u90e8\u5206\u200b\u3002</p>"},{"location":"zh/about/license/#6","title":"6. \u200b\u4f20\u9012\u200b\u975e\u6e90\u200b\u5f62\u5f0f\u200b.","text":"<p>\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u7b2c\u200b4\u200b\u6761\u200b\u548c\u200b\u7b2c\u200b5\u200b\u6761\u200b\u7684\u200b\u89c4\u5b9a\u200b\uff0c\u200b\u4ee5\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u7684\u200b\u5f62\u5f0f\u200b\u4f20\u9012\u200b\u88ab\u200b\u4fdd\u62a4\u200b\u7684\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u4f46\u200b\u60a8\u200b\u4e5f\u200b\u5fc5\u987b\u200b\u6839\u636e\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u89c4\u5b9a\u200b\uff0c\u200b\u4ee5\u200b\u4e0b\u5217\u200b\u65b9\u5f0f\u200b\u4e4b\u4e00\u200b\u4f20\u9012\u200b\u673a\u5668\u200b\u53ef\u8bfb\u200b\u7684\u200b\u76f8\u5e94\u200b\u6e90\u4ee3\u7801\u200b:</p> <p>a) \u200b\u5728\u200b\u5b9e\u4f53\u200b\u4ea7\u54c1\u200b\uff08\u200b\u5305\u62ec\u200b\u5b9e\u4f53\u200b\u9500\u552e\u200b\u5a92\u4ecb\u200b\uff09\u200b\u4e2d\u200b\u4f20\u9012\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\uff0c\u200b\u6216\u200b\u4f53\u73b0\u200b\u5728\u200b\u5b9e\u4f53\u200b\u4ea7\u54c1\u200b\uff08\u200b\u5305\u62ec\u200b\u5b9e\u4f53\u200b\u9500\u552e\u200b\u5a92\u4ecb\u200b\uff09\u200b\u4e2d\u200b\uff0c\u200b\u540c\u65f6\u200b\u5c06\u200b\u76f8\u5e94\u200b\u7684\u200b\u6e90\u4ee3\u7801\u200b\u56fa\u5b9a\u200b\u5728\u200b\u901a\u5e38\u200b\u7528\u4e8e\u200b\u8f6f\u4ef6\u200b\u4ea4\u6362\u200b\u7684\u200b\u8010\u7528\u200b\u5b9e\u4f53\u200b\u5a92\u4ecb\u200b\u4e0a\u200b\u3002 b) \u200b\u5728\u200b\u5b9e\u7269\u200b\u4ea7\u54c1\u200b\uff08\u200b\u5305\u62ec\u200b\u5b9e\u7269\u200b\u9500\u552e\u200b\u5a92\u4ecb\u200b\uff09\u200b\u4e2d\u200b\u4f20\u9012\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\uff0c\u200b\u6216\u200b\u5728\u200b\u5b9e\u7269\u200b\u4ea7\u54c1\u200b\uff08\u200b\u5305\u62ec\u200b\u5b9e\u7269\u200b\u9500\u552e\u200b\u5a92\u4ecb\u200b\uff09\u200b\u4e2d\u200b\u4f53\u73b0\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5e76\u200b\u9644\u6709\u200b\u4e00\u4efd\u200b\u81f3\u5c11\u200b\u4e09\u5e74\u200b\u6709\u6548\u200b\u7684\u200b\u4e66\u9762\u200b\u62a5\u4ef7\u200b\uff0c\u200b\u53ea\u8981\u200b\u60a8\u200b\u4e3a\u200b\u8be5\u200b\u4ea7\u54c1\u578b\u53f7\u200b\u63d0\u4f9b\u200b\u5907\u4ef6\u200b\u6216\u200b\u5ba2\u6237\u200b\u652f\u6301\u200b\uff0c\u200b\u5c31\u200b\u4e00\u76f4\u200b\u6709\u6548\u200b\u3002\u200b\u5411\u200b\u4efb\u4f55\u200b\u62e5\u6709\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u7684\u200b\u4eba\u200b\u63d0\u4f9b\u200b(1)\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u6240\u200b\u6db5\u76d6\u200b\u7684\u200b\u4ea7\u54c1\u200b\u4e2d\u200b\u6240\u6709\u200b\u8f6f\u4ef6\u200b\u7684\u200b\u76f8\u5e94\u200b\u6e90\u4ee3\u7801\u200b\u7684\u200b\u62f7\u8d1d\u200b\uff0c\u200b\u62f7\u8d1d\u200b\u5728\u200b\u901a\u5e38\u200b\u7528\u4e8e\u200b\u8f6f\u4ef6\u200b\u4ea4\u6362\u200b\u7684\u200b\u8010\u7528\u200b\u7269\u7406\u4ecb\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u5176\u200b\u4ef7\u683c\u200b\u4e0d\u200b\u8d85\u8fc7\u200b\u8d35\u65b9\u200b\u5b9e\u9645\u200b\u6267\u884c\u200b\u8fd9\u4e00\u200b\u4f20\u9012\u200b\u6e90\u4ee3\u7801\u200b\u7684\u200b\u5408\u7406\u200b\u6210\u672c\u200b\uff0c\u200b\u6216\u8005\u200b(2)\u200b\u4ece\u200b\u7f51\u7edc\u200b\u670d\u52a1\u5668\u200b\u4e0a\u200b\u514d\u8d39\u200b\u83b7\u53d6\u200b\u76f8\u5e94\u200b\u7684\u200b\u6e90\u4ee3\u7801\u200b\u62f7\u8d1d\u200b\u3002 c) \u200b\u5c06\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u7684\u200b\u5355\u4e2a\u200b\u526f\u672c\u200b\u4e0e\u200b\u63d0\u4f9b\u200b\u76f8\u5e94\u200b\u6765\u6e90\u200b\u7684\u200b\u4e66\u9762\u200b\u63d0\u8bae\u200b\u7684\u200b\u526f\u672c\u200b\u4e00\u8d77\u200b\u4f20\u9001\u200b\u3002\u200b\u53ea\u6709\u200b\u5728\u200b\u5076\u5c14\u200b\u548c\u200b\u975e\u5546\u4e1a\u6027\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5e76\u4e14\u200b\u53ea\u6709\u200b\u5728\u200b\u60a8\u200b\u6536\u5230\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u548c\u200b\u8fd9\u79cd\u200b\u63d0\u8bae\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u624d\u200b\u5141\u8bb8\u200b\u8fd9\u79cd\u200b\u9009\u62e9\u200b\uff0c\u200b\u7b26\u5408\u200b\u7b2c\u200b6b\u200b\u6b3e\u200b\u7684\u200b\u89c4\u5b9a\u200b\u3002 d) \u200b\u901a\u8fc7\u200b\u63d0\u4f9b\u200b\u4ece\u200b\u6307\u5b9a\u200b\u5730\u70b9\u200b\uff08\u200b\u514d\u8d39\u200b\u6216\u200b\u6536\u8d39\u200b\uff09\u200b\u83b7\u53d6\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5e76\u200b\u4ee5\u200b\u540c\u6837\u200b\u7684\u200b\u65b9\u5f0f\u200b\u901a\u8fc7\u200b\u540c\u4e00\u200b\u5730\u70b9\u200b\u63d0\u4f9b\u200b\u76f8\u5e94\u200b\u7684\u200b\u6e90\u7801\u200b\uff0c\u200b\u800c\u200b\u4e0d\u518d\u200b\u6536\u8d39\u200b\u3002\u200b\u60a8\u200b\u4e0d\u200b\u9700\u8981\u200b\u8981\u6c42\u200b\u63a5\u53d7\u8005\u200b\u5728\u200b\u590d\u5236\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u7684\u200b\u540c\u65f6\u200b\u590d\u5236\u200b\u76f8\u5e94\u200b\u7684\u200b\u6e90\u4ee3\u7801\u200b\u3002\u200b\u5982\u679c\u200b\u590d\u5236\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u7684\u200b\u5730\u65b9\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7f51\u7edc\u200b\u670d\u52a1\u5668\u200b\uff0c\u200b\u5bf9\u5e94\u200b\u6e90\u200b\u53ef\u4ee5\u200b\u5728\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u652f\u6301\u200b\u540c\u7b49\u200b\u590d\u5236\u200b\u8bbe\u65bd\u200b\u7684\u200b\u670d\u52a1\u5668\u200b\u4e0a\u200b\uff08\u200b\u7531\u200b\u60a8\u200b\u6216\u200b\u7b2c\u4e09\u65b9\u200b\u8fd0\u8425\u200b\uff09\uff0c\u200b\u53ea\u8981\u200b\u60a8\u200b\u5728\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u65c1\u8fb9\u200b\u4fdd\u6301\u200b\u660e\u786e\u200b\u7684\u200b\u6307\u793a\u200b\uff0c\u200b\u8bf4\u660e\u200b\u5728\u200b\u54ea\u91cc\u200b\u53ef\u4ee5\u200b\u627e\u5230\u200b\u5bf9\u5e94\u200b\u6e90\u200b\u3002\u200b\u65e0\u8bba\u200b\u5bf9\u5e94\u200b\u6e90\u200b\u5728\u200b\u54ea\u4e2a\u200b\u670d\u52a1\u5668\u200b\u4e0a\u200b\uff0c\u200b\u60a8\u200b\u90fd\u200b\u6709\u200b\u4e49\u52a1\u200b\u786e\u4fdd\u200b\u5728\u200b\u6ee1\u8db3\u200b\u8fd9\u4e9b\u200b\u8981\u6c42\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u65f6\u95f4\u200b\u5185\u200b\u63d0\u4f9b\u200b\u5bf9\u5e94\u200b\u6e90\u200b\u3002 e) \u200b\u4f7f\u7528\u200b\u70b9\u5bf9\u70b9\u200b\u4f20\u8f93\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4f20\u9001\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4f46\u200b\u60a8\u200b\u5fc5\u987b\u200b\u544a\u77e5\u200b\u5176\u4ed6\u200b\u540c\u884c\u200b\uff0c\u200b\u6839\u636e\u200b\u7b2c\u200b6d\u200b\u6b3e\u200b\uff0c\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u548c\u200b\u4f5c\u54c1\u200b\u7684\u200b\u5bf9\u5e94\u200b\u6e90\u200b\u6b63\u5728\u200b\u514d\u8d39\u200b\u63d0\u4f9b\u200b\u7ed9\u200b\u516c\u4f17\u200b\u3002 \u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u7684\u200b\u53ef\u200b\u5206\u79bb\u200b\u90e8\u5206\u200b\uff0c\u200b\u5176\u200b\u6e90\u4ee3\u7801\u200b\u4f5c\u4e3a\u200b\u7cfb\u7edf\u200b\u5e93\u200b\u88ab\u200b\u6392\u9664\u200b\u5728\u200b\u76f8\u5e94\u200b\u7684\u200b\u6e90\u7801\u200b\u4e4b\u5916\u200b\uff0c\u200b\u4e0d\u200b\u9700\u8981\u200b\u5305\u62ec\u200b\u5728\u200b\u4f20\u8fbe\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u4f5c\u54c1\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u7528\u6237\u200b\u4ea7\u54c1\u200b \u201c\u200b\u662f\u200b\u6307\u200b(1) \u201c\u200b\u6d88\u8d39\u54c1\u200b\u201d\uff0c\u200b\u5373\u200b\u901a\u5e38\u200b\u7528\u4e8e\u200b\u4e2a\u4eba\u200b\u3001\u200b\u5bb6\u5ead\u200b\u6216\u200b\u5bb6\u5c45\u200b\u7528\u9014\u200b\u7684\u200b\u4efb\u4f55\u200b\u6709\u5f62\u200b\u4e2a\u4eba\u8d22\u4ea7\u200b\uff0c\u200b\u6216\u200b(2)\u200b\u4e3a\u200b\u7eb3\u5165\u200b\u4f4f\u5b85\u200b\u800c\u200b\u8bbe\u8ba1\u200b\u6216\u200b\u51fa\u552e\u200b\u7684\u200b\u4efb\u4f55\u200b\u4e1c\u897f\u200b\u3002\u200b\u5728\u200b\u786e\u5b9a\u200b\u4e00\u4e2a\u200b\u4ea7\u54c1\u200b\u662f\u5426\u662f\u200b\u6d88\u8d39\u54c1\u200b\u65f6\u200b\uff0c\u200b\u6709\u200b\u7591\u95ee\u200b\u7684\u200b\u60c5\u51b5\u200b\u5e94\u200b\u4ee5\u200b\u6709\u5229\u4e8e\u200b\u627f\u4fdd\u200b\u7684\u200b\u65b9\u5f0f\u200b\u89e3\u51b3\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u7279\u5b9a\u200b\u7528\u6237\u200b\u6536\u5230\u200b\u7684\u200b\u7279\u5b9a\u200b\u4ea7\u54c1\u200b\uff0c\u201d\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200b\u201d \u200b\u662f\u200b\u6307\u200b\u8be5\u7c7b\u200b\u4ea7\u54c1\u200b\u7684\u200b\u5178\u578b\u200b\u6216\u200b\u5e38\u89c1\u200b\u7528\u9014\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u8003\u8651\u200b\u7279\u5b9a\u200b\u7528\u6237\u200b\u7684\u200b\u5730\u4f4d\u200b\u6216\u200b\u7279\u5b9a\u200b\u7528\u6237\u200b\u5b9e\u9645\u200b\u4f7f\u7528\u200b\u6216\u200b\u671f\u671b\u200b\u6216\u200b\u9884\u671f\u200b\u4f7f\u7528\u200b\u8be5\u200b\u4ea7\u54c1\u200b\u7684\u200b\u65b9\u5f0f\u200b\u3002\u200b\u4e00\u4e2a\u200b\u4ea7\u54c1\u200b\u662f\u200b\u6d88\u8d39\u7c7b\u200b\u4ea7\u54c1\u200b\uff0c\u200b\u65e0\u8bba\u200b\u8be5\u200b\u4ea7\u54c1\u200b\u662f\u5426\u200b\u6709\u200b\u5927\u91cf\u200b\u7684\u200b\u5546\u4e1a\u200b\u3001\u200b\u5de5\u4e1a\u200b\u6216\u975e\u200b\u6d88\u8d39\u7c7b\u200b\u7528\u9014\u200b\uff0c\u200b\u9664\u975e\u200b\u8fd9\u4e9b\u200b\u7528\u9014\u200b\u662f\u200b\u8be5\u200b\u4ea7\u54c1\u200b\u7684\u200b\u552f\u4e00\u200b\u91cd\u8981\u200b\u4f7f\u7528\u200b\u65b9\u5f0f\u200b\u3002</p> <p>\u200b\u7528\u6237\u200b\u4ea7\u54c1\u200b\u7684\u200b \u201c\u200b\u5b89\u88c5\u200b\u4fe1\u606f\u200b\u201d \u200b\u662f\u200b\u6307\u200b\u5728\u200b\u8be5\u200b\u7528\u6237\u200b\u4ea7\u54c1\u200b\u4e2d\u200b\u4ece\u200b\u5176\u200b\u5bf9\u5e94\u200b\u6e90\u200b\u7684\u200b\u4fee\u6539\u200b\u7248\u672c\u200b\u5b89\u88c5\u200b\u548c\u200b\u6267\u884c\u200b\u6240\u200b\u6db5\u76d6\u200b\u4f5c\u54c1\u200b\u7684\u200b\u4fee\u6539\u200b\u7248\u672c\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u4efb\u4f55\u200b\u65b9\u6cd5\u200b\u3001\u200b\u7a0b\u5e8f\u200b\u3001\u200b\u6388\u6743\u200b\u5bc6\u94a5\u200b\u6216\u200b\u5176\u4ed6\u200b\u4fe1\u606f\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u4fe1\u606f\u200b\u5fc5\u987b\u200b\u8db3\u4ee5\u200b\u786e\u4fdd\u200b\u5728\u200b\u4efb\u4f55\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u90fd\u200b\u4e0d\u4f1a\u200b\u4ec5\u4ec5\u200b\u56e0\u4e3a\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4fee\u6539\u200b\u800c\u200b\u963b\u6b62\u200b\u6216\u200b\u5e72\u6270\u200b\u4fee\u6539\u200b\u540e\u200b\u7684\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u7684\u200b\u7ee7\u7eed\u200b\u8fd0\u884c\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u60a8\u200b\u5728\u200b\u7528\u6237\u200b\u4ea7\u54c1\u200b\u4e2d\u200b\uff0c\u200b\u6216\u200b\u4e0e\u200b\u7528\u6237\u200b\u4ea7\u54c1\u200b\u4e00\u8d77\u200b\uff0c\u200b\u6216\u200b\u4e13\u95e8\u200b\u5728\u200b\u7528\u6237\u200b\u4ea7\u54c1\u200b\u4e2d\u200b\u4f7f\u7528\u200b\uff0c\u200b\u5e76\u200b\u4f5c\u4e3a\u200b\u4ea4\u6613\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\uff0c\u200b\u5c06\u200b\u7528\u6237\u200b\u4ea7\u54c1\u200b\u7684\u200b\u62e5\u6709\u6743\u200b\u548c\u200b\u4f7f\u7528\u6743\u200b\u6c38\u4e45\u200b\u6216\u200b\u56fa\u5b9a\u200b\u5730\u200b\u8f6c\u8ba9\u200b\u7ed9\u200b\u63a5\u53d7\u8005\u200b\uff08\u200b\u65e0\u8bba\u200b\u4ea4\u6613\u200b\u5982\u4f55\u200b\u5b9a\u6027\u200b\uff09\uff0c\u200b\u6839\u636e\u200b\u672c\u6761\u200b\u89c4\u5b9a\u200b\u8f6c\u8ba9\u200b\u7684\u200b\u5bf9\u5e94\u200b\u6e90\u200b\u5fc5\u987b\u200b\u9644\u6709\u200b\u5b89\u88c5\u200b\u4fe1\u606f\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u6216\u200b\u4efb\u4f55\u200b\u7b2c\u4e09\u65b9\u200b\u90fd\u200b\u6ca1\u6709\u200b\u4fdd\u7559\u200b\u5728\u200b\u7528\u6237\u200b\u4ea7\u54c1\u200b\u4e0a\u200b\u5b89\u88c5\u200b\u4fee\u6539\u200b\u8fc7\u200b\u7684\u200b\u76ee\u6807\u200b\u4ee3\u7801\u200b\u7684\u200b\u80fd\u529b\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f5c\u54c1\u200b\u5df2\u7ecf\u200b\u5b89\u88c5\u200b\u5728\u200bROM\u200b\u4e2d\u200b\uff09\uff0c\u200b\u5219\u200b\u8be5\u200b\u8981\u6c42\u200b\u4e0d\u200b\u9002\u7528\u200b\u3002</p> <p>\u200b\u63d0\u4f9b\u200b\u5b89\u88c5\u200b\u4fe1\u606f\u200b\u7684\u200b\u8981\u6c42\u200b\u4e0d\u200b\u5305\u62ec\u200b\u7ee7\u7eed\u200b\u4e3a\u200b\u88ab\u200b\u63a5\u53d7\u8005\u200b\u4fee\u6539\u200b\u6216\u200b\u5b89\u88c5\u200b\u7684\u200b\u4f5c\u54c1\u200b\u6216\u200b\u88ab\u200b\u4fee\u6539\u200b\u6216\u200b\u5b89\u88c5\u200b\u7684\u200b\u7528\u6237\u200b\u4ea7\u54c1\u200b\u63d0\u4f9b\u200b\u652f\u6301\u200b\u670d\u52a1\u200b\u3001\u200b\u4fdd\u8bc1\u200b\u6216\u200b\u66f4\u65b0\u200b\u7684\u200b\u8981\u6c42\u200b\u3002\u200b\u5f53\u200b\u4fee\u6539\u200b\u672c\u8eab\u200b\u5bf9\u200b\u7f51\u7edc\u200b\u7684\u200b\u8fd0\u884c\u200b\u4ea7\u751f\u200b\u5b9e\u8d28\u6027\u200b\u7684\u200b\u4e0d\u5229\u200b\u5f71\u54cd\u200b\u6216\u200b\u8fdd\u53cd\u200b\u4e86\u200b\u7f51\u7edc\u200b\u4e0a\u200b\u7684\u200b\u901a\u4fe1\u200b\u89c4\u5219\u200b\u548c\u200b\u534f\u8bae\u200b\u65f6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u62d2\u7edd\u200b\u8bbf\u95ee\u200b\u7f51\u7edc\u200b\u3002 \u200b\u6839\u636e\u200b\u672c\u200b\u8282\u200b\u89c4\u5b9a\u200b\uff0c\u200b\u6240\u200b\u4f20\u8fbe\u200b\u7684\u200b\u76f8\u5e94\u200b\u6e90\u7801\u200b\u548c\u200b\u6240\u200b\u63d0\u4f9b\u200b\u7684\u200b\u5b89\u88c5\u200b\u4fe1\u606f\u200b\u5fc5\u987b\u200b\u662f\u200b\u516c\u5f00\u200b\u8bb0\u5f55\u200b\u7684\u200b\u683c\u5f0f\u200b\uff08\u200b\u5e76\u200b\u4ee5\u200b\u6e90\u4ee3\u7801\u200b\u7684\u200b\u5f62\u5f0f\u200b\u5411\u200b\u516c\u4f17\u200b\u63d0\u4f9b\u200b\u5b9e\u73b0\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u5fc5\u987b\u200b\u4e0d\u200b\u9700\u8981\u200b\u7279\u6b8a\u200b\u7684\u200b\u5bc6\u7801\u200b\u6216\u200b\u94a5\u5319\u200b\u6765\u200b\u89e3\u5305\u200b\u3001\u200b\u9605\u8bfb\u200b\u6216\u200b\u590d\u5236\u200b\u3002</p>"},{"location":"zh/about/license/#7","title":"7. \u200b\u9644\u52a0\u200b\u6761\u6b3e\u200b.","text":"<p>\u201c\u200b\u9644\u52a0\u200b\u8bb8\u53ef\u200b\u201d \u200b\u662f\u200b\u5bf9\u200b\u672c\u200b\u8bb8\u53ef\u200b\u6761\u6b3e\u200b\u7684\u200b\u8865\u5145\u200b\uff0c\u200b\u5bf9\u200b\u5176\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6216\u200b\u591a\u4e2a\u200b\u6761\u4ef6\u200b\u4f5c\u51fa\u200b\u4f8b\u5916\u200b\u89c4\u5b9a\u200b\u3002\u200b\u9002\u7528\u200b\u4e8e\u200b\u6574\u4e2a\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u9644\u52a0\u200b\u8bb8\u53ef\u200b\u5e94\u200b\u88ab\u200b\u89c6\u4e3a\u200b\u5305\u62ec\u200b\u5728\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u4e2d\u200b\uff0c\u200b\u53ea\u8981\u200b\u5b83\u4eec\u200b\u5728\u200b\u9002\u7528\u6cd5\u5f8b\u200b\u4e0b\u200b\u6709\u6548\u200b\u3002\u200b\u5982\u679c\u200b\u9644\u52a0\u200b\u8bb8\u53ef\u200b\u53ea\u200b\u9002\u7528\u200b\u4e8e\u672c\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\uff0c\u200b\u5219\u200b\u8be5\u200b\u90e8\u5206\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u8fd9\u4e9b\u200b\u8bb8\u53ef\u200b\u5355\u72ec\u200b\u4f7f\u7528\u200b\uff0c\u200b\u4f46\u200b\u6574\u4e2a\u200b\u7a0b\u5e8f\u200b\u4ecd\u200b\u53d7\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u7ba1\u8f96\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u8003\u8651\u200b\u9644\u52a0\u200b\u8bb8\u53ef\u200b\u3002</p> <p>\u200b\u5f53\u200b\u60a8\u200b\u8f6c\u9001\u200b\u4e00\u4efd\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u4f5c\u54c1\u200b\u7684\u200b\u526f\u672c\u200b\u65f6\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u4ece\u200b\u8be5\u200b\u526f\u672c\u200b\u6216\u200b\u5176\u200b\u4efb\u4f55\u200b\u90e8\u5206\u200b\u4e2d\u200b\u5220\u9664\u200b\u4efb\u4f55\u200b\u9644\u52a0\u200b\u8bb8\u53ef\u200b\u3002(\u200b\u5728\u200b\u67d0\u4e9b\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5f53\u200b\u60a8\u200b\u4fee\u6539\u200b\u4f5c\u54c1\u200b\u65f6\u200b\uff0c\u200b\u9644\u52a0\u200b\u8bb8\u53ef\u200b\u53ef\u80fd\u200b\u88ab\u200b\u5199\u6210\u200b\u9700\u8981\u200b\u81ea\u5df1\u200b\u5220\u9664\u200b)\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u60a8\u200b\u6dfb\u52a0\u200b\u5230\u200b\u6db5\u76d6\u200b\u7684\u200b\u4f5c\u54c1\u200b\u4e2d\u200b\u7684\u200b\u6750\u6599\u200b\u4e0a\u200b\u653e\u7f6e\u200b\u989d\u5916\u200b\u7684\u200b\u8bb8\u53ef\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u8fd9\u4e9b\u200b\u6750\u6599\u200b\uff0c\u200b\u60a8\u200b\u6709\u200b\u6216\u200b\u53ef\u4ee5\u200b\u7ed9\u4e88\u200b\u9002\u5f53\u200b\u7684\u200b\u7248\u6743\u200b\u8bb8\u53ef\u200b\u3002</p> <p>\u200b\u5c3d\u7ba1\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u6709\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u89c4\u5b9a\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u60a8\u200b\u6dfb\u52a0\u200b\u5230\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u4f5c\u54c1\u200b\u4e2d\u200b\u7684\u200b\u6750\u6599\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\uff08\u200b\u5982\u679c\u200b\u5f97\u5230\u200b\u8be5\u200b\u6750\u6599\u200b\u7684\u200b\u7248\u6743\u200b\u6301\u6709\u4eba\u200b\u7684\u200b\u6388\u6743\u200b\uff09\u200b\u7528\u200b\u4ee5\u4e0b\u200b\u6761\u6b3e\u200b\u8865\u5145\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u6761\u6b3e\u200b:</p> <p>a) \u200b\u4ee5\u200b\u4e0d\u540c\u4e8e\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7b2c\u200b15\u200b\u6761\u200b\u548c\u200b\u7b2c\u200b16\u200b\u6761\u200b\u7684\u200b\u6761\u6b3e\u200b\u58f0\u660e\u200b\u4fdd\u8bc1\u200b\u6216\u200b\u9650\u5236\u200b\u8d23\u4efb\u200b\uff1b\u200b\u6216\u200b b) \u200b\u8981\u6c42\u200b\u5728\u200b\u8be5\u200b\u6750\u6599\u200b\u6216\u200b\u5305\u542b\u200b\u8be5\u200b\u6750\u6599\u200b\u7684\u200b\u4f5c\u54c1\u200b\u6240\u200b\u663e\u793a\u200b\u7684\u200b\u9002\u5f53\u200b\u6cd5\u5f8b\u200b\u58f0\u660e\u200b\u4e2d\u200b\u4fdd\u7559\u200b\u7279\u5b9a\u200b\u7684\u200b\u5408\u7406\u200b\u6cd5\u5f8b\u200b\u58f0\u660e\u200b\u6216\u200b\u4f5c\u8005\u200b\u5f52\u5c5e\u200b\uff1b\u200b\u6216\u200b c) \u200b\u7981\u6b62\u200b\u6b6a\u66f2\u200b\u8be5\u200b\u6750\u6599\u200b\u7684\u200b\u6765\u6e90\u200b\uff0c\u200b\u6216\u200b\u8981\u6c42\u200b\u4ee5\u200b\u5408\u7406\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5c06\u200b\u8be5\u200b\u6750\u6599\u200b\u7684\u200b\u4fee\u6539\u200b\u7248\u672c\u200b\u6807\u8bb0\u200b\u4e3a\u200b\u4e0e\u200b\u539f\u59cb\u200b\u7248\u672c\u200b\u4e0d\u540c\u200b\uff1b\u200b\u6216\u200b d) \u200b\u9650\u5236\u200b\u4e3a\u200b\u5ba3\u4f20\u200b\u76ee\u7684\u200b\u4f7f\u7528\u200b\u8be5\u200b\u6750\u6599\u200b\u7684\u200b\u8bb8\u53ef\u200b\u4eba\u200b\u6216\u200b\u4f5c\u8005\u200b\u7684\u200b\u59d3\u540d\u200b\uff1b\u200b\u6216\u200b e) \u200b\u62d2\u7edd\u200b\u6839\u636e\u200b\u5546\u6807\u6cd5\u200b\u6388\u4e88\u200b\u4f7f\u7528\u200b\u67d0\u4e9b\u200b\u5546\u53f7\u200b\u3001\u200b\u5546\u6807\u200b\u6216\u200b\u670d\u52a1\u200b\u6807\u5fd7\u200b\u7684\u200b\u6743\u5229\u200b\uff1b\u200b\u6216\u200b f) \u200b\u8981\u6c42\u200b\u5c06\u200b\u6750\u6599\u200b\uff08\u200b\u6216\u200b\u6750\u6599\u200b\u7684\u200b\u4fee\u6539\u200b\u7248\u672c\u200b\uff09\u200b\u8f6c\u200b\u4ea4\u7ed9\u200b\u63a5\u53d7\u8005\u200b\u7684\u200b\u4efb\u4f55\u4eba\u200b\u5bf9\u200b\u8fd9\u4e9b\u200b\u5408\u540c\u200b\u5047\u8bbe\u200b\u76f4\u63a5\u200b\u52a0\u200b\u5728\u200b\u8fd9\u4e9b\u200b\u8bb8\u53ef\u200b\u4eba\u200b\u548c\u200b\u4f5c\u8005\u200b\u8eab\u4e0a\u200b\u7684\u200b\u4efb\u4f55\u200b\u8d23\u4efb\u200b\u8fdb\u884c\u200b\u8d54\u507f\u200b\u3002 \u200b\u6240\u6709\u200b\u5176\u4ed6\u200b\u975e\u200b\u8bb8\u53ef\u200b\u6027\u200b\u7684\u200b\u9644\u52a0\u200b\u6761\u6b3e\u200b\u90fd\u200b\u88ab\u200b\u89c6\u4e3a\u200b\u7b2c\u200b10\u200b\u6761\u200b\u610f\u4e49\u200b\u4e0a\u200b\u7684\u200b \u201c\u200b\u8fdb\u4e00\u6b65\u200b\u9650\u5236\u200b\u201d\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u6536\u5230\u200b\u7684\u200b\u7a0b\u5e8f\u200b\u6216\u200b\u5176\u200b\u4efb\u4f55\u200b\u90e8\u5206\u200b\u5305\u542b\u200b\u4e00\u4e2a\u200b\u901a\u77e5\u200b\uff0c\u200b\u8bf4\u660e\u200b\u5b83\u200b\u53d7\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u7ba1\u8f96\u200b\uff0c\u200b\u540c\u65f6\u200b\u8fd8\u6709\u200b\u4e00\u4e2a\u200b\u5c5e\u4e8e\u200b\u8fdb\u4e00\u6b65\u200b\u9650\u5236\u200b\u7684\u200b\u6761\u6b3e\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5220\u9664\u200b\u8be5\u200b\u6761\u6b3e\u200b\u3002\u200b\u5982\u679c\u200b\u8bb8\u53ef\u200b\u6587\u4ef6\u200b\u5305\u542b\u200b\u8fdb\u4e00\u6b65\u200b\u7684\u200b\u9650\u5236\u200b\uff0c\u200b\u4f46\u200b\u5141\u8bb8\u200b\u6839\u636e\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u8fdb\u884c\u200b\u518d\u200b\u8bb8\u53ef\u200b\u6216\u200b\u8f6c\u8ba9\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u7684\u200b\u4f5c\u54c1\u200b\u4e2d\u200b\u6dfb\u52a0\u200b\u53d7\u8be5\u200b\u8bb8\u53ef\u200b\u6587\u4ef6\u200b\u6761\u6b3e\u200b\u7ba1\u8f96\u200b\u7684\u200b\u6750\u6599\u200b\uff0c\u200b\u4f46\u200b\u8fdb\u4e00\u6b65\u200b\u7684\u200b\u9650\u5236\u200b\u5728\u200b\u8fd9\u79cd\u200b\u518d\u200b\u8bb8\u53ef\u200b\u6216\u200b\u8f6c\u8ba9\u200b\u4e2d\u200b\u4e0d\u200b\u5b58\u5728\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u60a8\u200b\u6309\u7167\u200b\u672c\u8282\u200b\u7684\u200b\u89c4\u5b9a\u200b\u5411\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u7684\u200b\u4f5c\u54c1\u200b\u6dfb\u52a0\u200b\u6761\u6b3e\u200b\uff0c\u200b\u60a8\u200b\u5fc5\u987b\u200b\u5728\u200b\u76f8\u5173\u200b\u7684\u200b\u6e90\u6587\u4ef6\u200b\u4e2d\u200b\u58f0\u660e\u200b\u9002\u7528\u200b\u4e8e\u200b\u8fd9\u4e9b\u200b\u6587\u4ef6\u200b\u7684\u200b\u9644\u52a0\u200b\u6761\u6b3e\u200b\uff0c\u200b\u6216\u8005\u200b\u8bf4\u660e\u200b\u5728\u200b\u54ea\u91cc\u200b\u53ef\u4ee5\u200b\u627e\u5230\u200b\u9002\u7528\u200b\u6761\u6b3e\u200b\u3002</p> <p>\u200b\u989d\u5916\u200b\u7684\u200b\u6761\u6b3e\u200b\uff0c\u200b\u4e0d\u7ba1\u200b\u662f\u200b\u5141\u8bb8\u200b\u7684\u200b\u8fd8\u200b\u662f\u975e\u200b\u5141\u8bb8\u200b\u7684\u200b\uff0c\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u4ee5\u200b\u5355\u72ec\u200b\u7684\u200b\u4e66\u9762\u200b\u8bb8\u53ef\u200b\u7684\u200b\u5f62\u5f0f\u200b\u8bf4\u660e\u200b\uff0c\u200b\u6216\u8005\u200b\u4f5c\u4e3a\u200b\u4f8b\u5916\u60c5\u51b5\u200b\u8bf4\u660e\u200b\uff1b\u200b\u4e0a\u8ff0\u200b\u8981\u6c42\u200b\u9002\u7528\u200b\u4e8e\u200b\u4efb\u4f55\u200b\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\u3002</p>"},{"location":"zh/about/license/#8","title":"8. \u200b\u7ec8\u6b62\u200b.","text":"<p>\u200b\u9664\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u660e\u786e\u89c4\u5b9a\u200b\u7684\u200b\u60c5\u51b5\u200b\u5916\u200b\uff0c\u200b\u60a8\u200b\u4e0d\u5f97\u200b\u4f20\u64ad\u200b\u6216\u200b\u4fee\u6539\u200b\u8986\u76d6\u200b\u4f5c\u54c1\u200b\u3002\u200b\u4efb\u4f55\u200b\u4ee5\u200b\u5176\u4ed6\u200b\u65b9\u5f0f\u200b\u4f20\u64ad\u200b\u6216\u200b\u4fee\u6539\u200b\u4f5c\u54c1\u200b\u7684\u200b\u5c1d\u8bd5\u200b\u90fd\u200b\u662f\u200b\u65e0\u6548\u200b\u7684\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u81ea\u52a8\u200b\u7ec8\u6b62\u200b\u60a8\u200b\u5728\u200b\u672c\u200b\u8bb8\u53ef\u200b\u4e0b\u200b\u7684\u200b\u6743\u5229\u200b\uff08\u200b\u5305\u62ec\u200b\u6839\u636e\u200b\u7b2c\u200b11\u200b\u8282\u200b\u7b2c\u4e09\u6bb5\u200b\u6388\u4e88\u200b\u7684\u200b\u4efb\u4f55\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\uff09\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u505c\u6b62\u200b\u6240\u6709\u200b\u8fdd\u53cd\u200b\u672c\u200b\u8bb8\u53ef\u200b\u7684\u200b\u884c\u4e3a\u200b\uff0c\u200b\u90a3\u4e48\u200b\u60a8\u200b\u4ece\u200b\u67d0\u200b\u4e00\u200b\u7279\u5b9a\u200b\u7248\u6743\u200b\u4eba\u5904\u200b\u83b7\u5f97\u200b\u7684\u200b\u8bb8\u53ef\u200b\u5c06\u200b\u88ab\u200b\u6062\u590d\u200b\uff08a\uff09\u200b\u6682\u65f6\u6027\u200b\u7684\u200b\uff0c\u200b\u9664\u975e\u200b\u5e76\u200b\u76f4\u5230\u200b\u7248\u6743\u200b\u4eba\u200b\u660e\u786e\u200b\u5e76\u200b\u6700\u7ec8\u200b\u7ec8\u6b62\u200b\u60a8\u200b\u7684\u200b\u8bb8\u53ef\u200b\uff0c\u200b\u4ee5\u53ca\u200b\uff08b\uff09\u200b\u6c38\u4e45\u6027\u200b\u7684\u200b\uff0c\u200b\u5982\u679c\u200b\u7248\u6743\u200b\u4eba\u200b\u672a\u80fd\u200b\u5728\u200b\u505c\u6b62\u200b\u540e\u200b\u7684\u200b60\u200b\u5929\u200b\u5185\u200b\u901a\u8fc7\u200b\u67d0\u79cd\u200b\u5408\u7406\u200b\u7684\u200b\u65b9\u5f0f\u200b\u901a\u77e5\u200b\u60a8\u200b\u4fb5\u6743\u884c\u4e3a\u200b\u3002</p> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u5982\u679c\u200b\u7248\u6743\u200b\u6301\u6709\u4eba\u200b\u4ee5\u200b\u67d0\u79cd\u200b\u5408\u7406\u200b\u7684\u200b\u65b9\u5f0f\u200b\u901a\u77e5\u200b\u60a8\u200b\u4fb5\u6743\u884c\u4e3a\u200b\uff0c\u200b\u8fd9\u200b\u662f\u200b\u60a8\u200b\u7b2c\u4e00\u6b21\u200b\u6536\u5230\u200b\u8be5\u200b\u7248\u6743\u200b\u6301\u6709\u4eba\u200b\u8fdd\u53cd\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u901a\u77e5\u200b\uff08\u200b\u9488\u5bf9\u200b\u4efb\u4f55\u200b\u4f5c\u54c1\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u60a8\u200b\u5728\u200b\u6536\u5230\u200b\u901a\u77e5\u200b\u540e\u200b30\u200b\u5929\u200b\u5185\u200b\u7ea0\u6b63\u200b\u4e86\u200b\u4fb5\u6743\u884c\u4e3a\u200b\uff0c\u200b\u90a3\u4e48\u200b\u60a8\u200b\u4ece\u200b\u67d0\u200b\u4e00\u200b\u7248\u6743\u200b\u6301\u6709\u4eba\u200b\u83b7\u5f97\u200b\u7684\u200b\u8bb8\u53ef\u8bc1\u200b\u5c06\u200b\u88ab\u200b\u6c38\u4e45\u200b\u6062\u590d\u200b\u3002</p> <p>\u200b\u7ec8\u6b62\u200b\u60a8\u200b\u5728\u200b\u672c\u8282\u200b\u4e0b\u200b\u7684\u200b\u6743\u5229\u200b\u5e76\u200b\u4e0d\u200b\u7ec8\u6b62\u200b\u90a3\u4e9b\u200b\u6839\u636e\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u4ece\u200b\u60a8\u200b\u90a3\u91cc\u200b\u83b7\u5f97\u200b\u526f\u672c\u200b\u6216\u200b\u6743\u5229\u200b\u7684\u200b\u5404\u65b9\u200b\u7684\u200b\u8bb8\u53ef\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u7684\u200b\u6743\u5229\u200b\u5df2\u7ecf\u200b\u88ab\u200b\u7ec8\u6b62\u200b\uff0c\u200b\u800c\u4e14\u200b\u6ca1\u6709\u200b\u6c38\u4e45\u200b\u6062\u590d\u200b\uff0c\u200b\u60a8\u200b\u5c31\u200b\u6ca1\u6709\u200b\u8d44\u683c\u200b\u6839\u636e\u200b\u7b2c\u200b10\u200b\u6761\u200b\u83b7\u5f97\u200b\u76f8\u540c\u200b\u6750\u6599\u200b\u7684\u200b\u65b0\u200b\u8bb8\u53ef\u200b\u3002</p>"},{"location":"zh/about/license/#9","title":"9. \u200b\u62e5\u6709\u200b\u526f\u672c\u200b\u4e0d\u200b\u9700\u8981\u200b\u63a5\u53d7\u200b.","text":"<p>\u200b\u60a8\u200b\u4e0d\u200b\u9700\u8981\u200b\u4e3a\u4e86\u200b\u63a5\u6536\u200b\u6216\u200b\u8fd0\u884c\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u526f\u672c\u200b\u800c\u200b\u63a5\u53d7\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u3002\u200b\u4ec5\u4ec5\u200b\u7531\u4e8e\u200b\u4f7f\u7528\u200b\u70b9\u5bf9\u70b9\u200b\u4f20\u8f93\u200b\u6765\u200b\u63a5\u6536\u200b\u62f7\u8d1d\u200b\u800c\u200b\u53d1\u751f\u200b\u7684\u200b\u88ab\u200b\u4fdd\u62a4\u200b\u4f5c\u54c1\u200b\u7684\u200b\u8f85\u52a9\u200b\u4f20\u64ad\u200b\uff0c\u200b\u4e5f\u200b\u540c\u6837\u200b\u4e0d\u200b\u9700\u8981\u200b\u63a5\u53d7\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u9664\u4e86\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u4e4b\u5916\u200b\uff0c\u200b\u6ca1\u6709\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u4e1c\u897f\u200b\u5141\u8bb8\u200b\u60a8\u200b\u4f20\u64ad\u200b\u6216\u200b\u4fee\u6539\u200b\u4efb\u4f55\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u7684\u200b\u4f5c\u54c1\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u4e0d\u200b\u63a5\u53d7\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u884c\u4e3a\u200b\u5c31\u200b\u4fb5\u72af\u200b\u4e86\u200b\u7248\u6743\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u901a\u8fc7\u200b\u4fee\u6539\u200b\u6216\u200b\u4f20\u64ad\u200b\u6db5\u76d6\u200b\u7684\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u60a8\u200b\u8868\u660e\u200b\u60a8\u200b\u63a5\u53d7\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8fd9\u6837\u200b\u505a\u200b\u3002</p>"},{"location":"zh/about/license/#10","title":"10. \u200b\u4e0b\u6e38\u200b\u63a5\u53d7\u8005\u200b\u7684\u200b\u81ea\u52a8\u200b\u8bb8\u53ef\u200b.","text":"<p>\u200b\u6bcf\u5f53\u200b\u60a8\u200b\u4f20\u9012\u200b\u4e00\u4e2a\u200b\u6db5\u76d6\u200b\u7684\u200b\u4f5c\u54c1\u200b\u65f6\u200b\uff0c\u200b\u63a5\u6536\u8005\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u4ece\u200b\u539f\u59cb\u200b\u8bb8\u53ef\u200b\u4eba\u200b\u90a3\u91cc\u200b\u5f97\u5230\u200b\u4e00\u4e2a\u200b\u8bb8\u53ef\u8bc1\u200b\uff0c\u200b\u5728\u200b\u9075\u5b88\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u524d\u63d0\u200b\u4e0b\u200b\uff0c\u200b\u8fd0\u884c\u200b\u3001\u200b\u4fee\u6539\u200b\u548c\u200b\u4f20\u64ad\u200b\u8be5\u200b\u4f5c\u54c1\u200b\u3002\u200b\u60a8\u200b\u4e0d\u200b\u8d1f\u8d23\u200b\u6267\u884c\u200b\u7b2c\u4e09\u65b9\u200b\u5bf9\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u9075\u5b88\u200b\u3002</p> <p>\u200b\u5b9e\u4f53\u200b\u4ea4\u6613\u200b \u201c\u200b\u662f\u200b\u6307\u200b\u8f6c\u8ba9\u200b\u4e00\u4e2a\u200b\u7ec4\u7ec7\u200b\u7684\u200b\u63a7\u5236\u6743\u200b\uff0c\u200b\u6216\u200b\u4e00\u4e2a\u200b\u7ec4\u7ec7\u200b\u7684\u200b\u5927\u90e8\u5206\u200b\u8d44\u4ea7\u200b\uff0c\u200b\u6216\u200b\u62c6\u5206\u200b\u4e00\u4e2a\u200b\u7ec4\u7ec7\u200b\uff0c\u200b\u6216\u200b\u5408\u5e76\u200b\u7ec4\u7ec7\u200b\u7684\u200b\u4ea4\u6613\u200b\u3002\u200b\u5982\u679c\u200b\u5b9e\u4f53\u200b\u4ea4\u6613\u200b\u5bfc\u81f4\u200b\u8986\u76d6\u200b\u4f5c\u54c1\u200b\u7684\u200b\u4f20\u64ad\u200b\uff0c\u200b\u90a3\u4e48\u200b\u6536\u5230\u200b\u4f5c\u54c1\u200b\u526f\u672c\u200b\u7684\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u4ea4\u6613\u200b\u65b9\u200b\u4e5f\u200b\u4f1a\u200b\u6536\u5230\u200b\u8be5\u65b9\u200b\u7684\u200b\u6743\u76ca\u200b\u524d\u8eab\u200b\u6839\u636e\u200b\u524d\u6bb5\u200b\u89c4\u5b9a\u200b\u6240\u200b\u62e5\u6709\u200b\u6216\u200b\u53ef\u4ee5\u200b\u7ed9\u4e88\u200b\u7684\u200b\u4efb\u4f55\u200b\u4f5c\u54c1\u200b\u8bb8\u53ef\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u4ece\u200b\u6743\u76ca\u200b\u524d\u200b\u8eab\u5904\u200b\u83b7\u5f97\u200b\u4f5c\u54c1\u200b\u5bf9\u5e94\u200b\u6e90\u200b\u7684\u200b\u62e5\u6709\u6743\u200b\uff0c\u200b\u5982\u679c\u200b\u6743\u76ca\u200b\u524d\u8eab\u200b\u62e5\u6709\u200b\u6216\u200b\u901a\u8fc7\u200b\u5408\u7406\u200b\u52aa\u529b\u200b\u53ef\u4ee5\u200b\u83b7\u5f97\u200b\u3002</p> <p>\u200b\u60a8\u200b\u4e0d\u5f97\u200b\u5bf9\u200b\u884c\u4f7f\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u4e0b\u200b\u6388\u4e88\u200b\u6216\u200b\u786e\u8ba4\u200b\u7684\u200b\u6743\u5229\u200b\u65bd\u52a0\u200b\u4efb\u4f55\u200b\u8fdb\u4e00\u6b65\u200b\u7684\u200b\u9650\u5236\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u60a8\u200b\u4e0d\u5f97\u200b\u5bf9\u200b\u884c\u4f7f\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u6240\u200b\u6388\u4e88\u200b\u7684\u200b\u6743\u5229\u200b\u5f81\u6536\u200b\u8bb8\u53ef\u8d39\u200b\u3001\u200b\u7279\u8bb8\u6743\u200b\u4f7f\u7528\u8d39\u200b\u6216\u200b\u5176\u4ed6\u8d39\u7528\u200b\uff0c\u200b\u4e5f\u200b\u4e0d\u5f97\u200b\u63d0\u8d77\u200b\u8bc9\u8bbc\u200b\uff08\u200b\u5305\u62ec\u200b\u8bc9\u8bbc\u200b\u4e2d\u200b\u7684\u200b\u4ea4\u53c9\u200b\u7d22\u8d54\u200b\u6216\u200b\u53cd\u200b\u7d22\u8d54\u200b\uff09\uff0c\u200b\u6307\u63a7\u200b\u5236\u4f5c\u200b\u3001\u200b\u4f7f\u7528\u200b\u3001\u200b\u9500\u552e\u200b\u3001\u200b\u63d0\u4f9b\u200b\u9500\u552e\u200b\u6216\u200b\u8fdb\u53e3\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u6216\u200b\u5176\u200b\u4efb\u4f55\u200b\u90e8\u5206\u200b\u4fb5\u72af\u200b\u4e86\u200b\u4efb\u4f55\u200b\u4e13\u5229\u200b\u6743\u5229\u200b\u3002</p>"},{"location":"zh/about/license/#11","title":"11. \u200b\u4e13\u5229\u200b.","text":"<p>\u200b\u8d21\u732e\u8005\u200b \u201c\u200b\u662f\u200b\u6307\u200b\u6388\u6743\u200b\u6839\u636e\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u4f7f\u7528\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u6216\u672c\u200b\u7a0b\u5e8f\u200b\u6240\u200b\u57fa\u4e8e\u200b\u7684\u200b\u4f5c\u54c1\u200b\u7684\u200b\u7248\u6743\u200b\u6301\u6709\u4eba\u200b\u3002\u200b\u8fd9\u6837\u200b\u6388\u6743\u200b\u7684\u200b\u4f5c\u54c1\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u8d21\u732e\u8005\u200b\u7684\u200b \u201c\u200b\u8d21\u732e\u8005\u200b\u7248\u672c\u200b\u201d\u3002</p> <p>\u200b\u8d21\u732e\u8005\u200b\u7684\u200b \u201c\u200b\u57fa\u672c\u200b\u4e13\u5229\u200b\u6743\u5229\u200b\u8981\u6c42\u200b\u201d \u200b\u662f\u200b\u6307\u200b\u8d21\u732e\u8005\u200b\u62e5\u6709\u200b\u6216\u200b\u63a7\u5236\u200b\u7684\u200b\u6240\u6709\u200b\u4e13\u5229\u200b\u6743\u5229\u200b\u8981\u6c42\u200b\uff0c\u200b\u4e0d\u7ba1\u200b\u662f\u200b\u5df2\u7ecf\u200b\u83b7\u5f97\u200b\u7684\u200b\u8fd8\u662f\u200b\u4ee5\u540e\u200b\u83b7\u5f97\u200b\u7684\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u4e13\u5229\u200b\u6743\u5229\u200b\u8981\u6c42\u200b\u5c06\u200b\u88ab\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u5141\u8bb8\u200b\u7684\u200b\u67d0\u79cd\u200b\u65b9\u5f0f\u200b\u6240\u200b\u4fb5\u72af\u200b\uff0c\u200b\u5373\u200b\u5236\u4f5c\u200b\u3001\u200b\u4f7f\u7528\u200b\u6216\u200b\u9500\u552e\u200b\u5176\u200b\u8d21\u732e\u8005\u200b\u7248\u672c\u200b\uff0c\u200b\u4f46\u200b\u4e0d\u200b\u5305\u62ec\u200b\u4ec5\u200b\u56e0\u200b\u8fdb\u4e00\u6b65\u200b\u4fee\u6539\u200b\u8d21\u732e\u8005\u200b\u7248\u672c\u200b\u800c\u200b\u88ab\u200b\u4fb5\u72af\u200b\u7684\u200b\u6743\u5229\u200b\u8981\u6c42\u200b\u3002\u200b\u5c31\u200b\u672c\u200b\u5b9a\u4e49\u200b\u800c\u8a00\u200b\uff0c\u201d\u200b\u63a7\u5236\u200b\u201d \u200b\u5305\u62ec\u200b\u4ee5\u200b\u7b26\u5408\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u8981\u6c42\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6388\u4e88\u200b\u4e13\u5229\u200b\u5206\u200b\u8bb8\u53ef\u200b\u7684\u200b\u6743\u5229\u200b\u3002</p> <p>\u200b\u6bcf\u4e2a\u200b\u8d21\u732e\u8005\u200b\u6839\u636e\u200b\u8d21\u732e\u8005\u200b\u7684\u200b\u57fa\u672c\u200b\u4e13\u5229\u200b\u6743\u5229\u200b\u8981\u6c42\u200b\uff0c\u200b\u6388\u4e88\u200b\u60a8\u200b\u975e\u200b\u72ec\u5360\u6027\u200b\u7684\u200b\u3001\u200b\u5168\u7403\u6027\u200b\u7684\u200b\u3001\u200b\u514d\u200b\u7248\u7a0e\u200b\u7684\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\uff0c\u200b\u4ee5\u200b\u5236\u9020\u200b\u3001\u200b\u4f7f\u7528\u200b\u3001\u200b\u9500\u552e\u200b\u3001\u200b\u63d0\u4f9b\u200b\u9500\u552e\u200b\u3001\u200b\u8fdb\u53e3\u200b\u548c\u200b\u4ee5\u200b\u5176\u4ed6\u200b\u65b9\u5f0f\u200b\u8fd0\u884c\u200b\u3001\u200b\u4fee\u6539\u200b\u548c\u200b\u4f20\u64ad\u200b\u5176\u200b\u8d21\u732e\u8005\u200b\u7248\u672c\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u5728\u200b\u4ee5\u4e0b\u200b\u4e09\u6bb5\u200b\u4e2d\u200b\uff0c\u201d\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\u201d \u200b\u662f\u200b\u6307\u200b\u4e0d\u200b\u5b9e\u65bd\u200b\u4e13\u5229\u200b\u7684\u200b\u4efb\u4f55\u200b\u660e\u793a\u200b\u534f\u8bae\u200b\u6216\u200b\u627f\u8bfa\u200b\uff0c\u200b\u65e0\u8bba\u200b\u5176\u200b\u540d\u79f0\u200b\u5982\u4f55\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u660e\u786e\u200b\u5141\u8bb8\u200b\u5b9e\u65bd\u200b\u4e13\u5229\u200b\u6216\u200b\u4e0d\u200b\u8d77\u8bc9\u200b\u4e13\u5229\u200b\u4fb5\u6743\u200b\u7684\u200b\u7ea6\u5b9a\u200b\uff09\u3002\u200b\u5411\u200b\u4e00\u65b9\u200b \u201c\u200b\u6388\u4e88\u200b\u201d \u200b\u8fd9\u79cd\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\u610f\u5473\u7740\u200b\u4f5c\u51fa\u200b\u8fd9\u79cd\u200b\u534f\u8bae\u200b\u6216\u200b\u627f\u8bfa\u200b\uff0c\u200b\u4e0d\u200b\u5bf9\u200b\u8be5\u65b9\u200b\u5b9e\u65bd\u200b\u4e13\u5229\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u60a8\u200b\u5728\u200b\u77e5\u60c5\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4f9d\u9760\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\u4f20\u9012\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6db5\u76d6\u200b\u7684\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u800c\u200b\u8be5\u200b\u4f5c\u54c1\u200b\u7684\u200b\u76f8\u5e94\u200b\u6765\u6e90\u200b\u5e76\u200b\u6ca1\u6709\u200b\u901a\u8fc7\u200b\u516c\u5f00\u200b\u7684\u200b\u7f51\u7edc\u200b\u670d\u52a1\u5668\u200b\u6216\u200b\u5176\u4ed6\u200b\u5bb9\u6613\u200b\u83b7\u5f97\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u4f9b\u200b\u4efb\u4f55\u4eba\u200b\u6839\u636e\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u6761\u6b3e\u200b\u514d\u8d39\u200b\u590d\u5236\u200b\uff0c\u200b\u90a3\u4e48\u200b\u60a8\u200b\u5fc5\u987b\u200b(1)\u200b\u4f7f\u200b\u76f8\u5e94\u200b\u6765\u6e90\u200b\u53ef\u4ee5\u200b\u83b7\u5f97\u200b\uff0c\u200b\u6216\u8005\u200b(2)\u200b\u5b89\u6392\u200b\u5265\u593a\u200b\u81ea\u5df1\u200b\u5bf9\u200b\u8be5\u200b\u7279\u5b9a\u200b\u4f5c\u54c1\u200b\u7684\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\u5229\u76ca\u200b\uff0c\u200b\u6216\u8005\u200b(3)\u200b\u4ee5\u200b\u7b26\u5408\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u8981\u6c42\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5b89\u6392\u200b\u5c06\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\u6269\u5c55\u200b\u5230\u200b\u4e0b\u6e38\u200b\u63a5\u53d7\u8005\u200b\u3002\u201d\u200b\u660e\u77e5\u6545\u72af\u200b\u201d \u200b\u662f\u200b\u6307\u200b\u60a8\u200b\u5b9e\u9645\u200b\u77e5\u9053\u200b\uff0c\u200b\u5982\u679c\u200b\u6ca1\u6709\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\uff0c\u200b\u60a8\u200b\u5728\u200b\u67d0\u4e2a\u200b\u56fd\u5bb6\u200b\u4f20\u9012\u200b\u6240\u6d89\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u6216\u8005\u200b\u60a8\u200b\u7684\u200b\u63a5\u53d7\u8005\u200b\u5728\u200b\u67d0\u4e2a\u200b\u56fd\u5bb6\u200b\u4f7f\u7528\u200b\u6240\u6d89\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u5c06\u200b\u4fb5\u72af\u200b\u60a8\u200b\u6709\u200b\u7406\u7531\u200b\u76f8\u4fe1\u200b\u5728\u200b\u8be5\u56fd\u200b\u6709\u6548\u200b\u7684\u200b\u4e00\u9879\u200b\u6216\u200b\u591a\u9879\u200b\u53ef\u200b\u8bc6\u522b\u200b\u4e13\u5229\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u6839\u636e\u200b\u4e00\u9879\u200b\u4ea4\u6613\u200b\u6216\u200b\u5b89\u6392\u200b\u6216\u200b\u4e0e\u200b\u4e4b\u200b\u76f8\u5173\u200b\uff0c\u200b\u60a8\u200b\u8f6c\u8ba9\u200b\u6216\u200b\u901a\u8fc7\u200b\u4fc3\u6210\u200b\u8f6c\u8ba9\u200b\u4f20\u64ad\u200b\u88ab\u200b\u4fdd\u62a4\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u5e76\u200b\u5411\u200b\u63a5\u53d7\u200b\u88ab\u200b\u4fdd\u62a4\u200b\u4f5c\u54c1\u200b\u7684\u200b\u67d0\u4e9b\u200b\u5f53\u4e8b\u65b9\u200b\u6388\u4e88\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\uff0c\u200b\u6388\u6743\u200b\u4ed6\u4eec\u200b\u4f7f\u7528\u200b\u3001\u200b\u4f20\u64ad\u200b\u3001\u200b\u4fee\u6539\u200b\u6216\u200b\u8f6c\u8ba9\u200b\u88ab\u200b\u4fdd\u62a4\u200b\u4f5c\u54c1\u200b\u7684\u200b\u7279\u5b9a\u200b\u526f\u672c\u200b\uff0c\u200b\u90a3\u4e48\u200b\u60a8\u200b\u6388\u4e88\u200b\u7684\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\u5c06\u200b\u81ea\u52a8\u200b\u6269\u5c55\u200b\u5230\u200b\u88ab\u200b\u4fdd\u62a4\u200b\u4f5c\u54c1\u200b\u548c\u200b\u57fa\u4e8e\u200b\u8be5\u200b\u4f5c\u54c1\u200b\u7684\u200b\u6240\u6709\u200b\u63a5\u53d7\u8005\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\u4e0d\u200b\u5305\u62ec\u200b\u5728\u200b\u5176\u200b\u8986\u76d6\u8303\u56f4\u200b\u5185\u200b\uff0c\u200b\u7981\u6b62\u200b\u884c\u4f7f\u200b\u6216\u200b\u4ee5\u200b\u4e0d\u200b\u884c\u4f7f\u200b\u672c\u200b\u8bb8\u53ef\u200b\u5177\u4f53\u200b\u6388\u4e88\u200b\u7684\u200b\u4e00\u9879\u200b\u6216\u200b\u591a\u9879\u200b\u6743\u5229\u200b\u4e3a\u200b\u6761\u4ef6\u200b\uff0c\u200b\u5219\u200b\u4e3a\u200b \u201c\u200b\u6b67\u89c6\u6027\u200b\u7684\u200b\u201d\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u662f\u200b\u4e0e\u200b\u4ece\u4e8b\u200b\u8f6f\u4ef6\u200b\u5206\u9500\u200b\u4e1a\u52a1\u200b\u7684\u200b\u7b2c\u4e09\u65b9\u200b\u8fbe\u6210\u200b\u7684\u200b\u5b89\u6392\u200b\u7684\u200b\u4e00\u65b9\u200b\uff0c\u200b\u6839\u636e\u200b\u8be5\u200b\u5b89\u6392\u200b\uff0c\u200b\u60a8\u200b\u6839\u636e\u200b\u60a8\u200b\u4f20\u9012\u200b\u4f5c\u54c1\u200b\u7684\u200b\u6d3b\u52a8\u200b\u8303\u56f4\u200b\u5411\u200b\u7b2c\u4e09\u65b9\u200b\u4ed8\u6b3e\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6839\u636e\u200b\u8be5\u200b\u5b89\u6392\u200b\uff0c\u200b\u7b2c\u4e09\u200b\u65b9\u5411\u200b\u4efb\u4f55\u200b\u5c06\u200b\u4ece\u200b\u60a8\u200b\u90a3\u91cc\u200b\u83b7\u5f97\u200b\u6240\u6d89\u200b\u4f5c\u54c1\u200b\u7684\u200b\u4e00\u65b9\u200b\u6388\u4e88\u200b\uff0c\u200b\u5219\u200b\u60a8\u200b\u4e0d\u5f97\u200b\u4f20\u9012\u200b\u6240\u6d89\u200b\u4f5c\u54c1\u200b\u3002\u200b\u6b67\u89c6\u6027\u200b\u7684\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\uff1a(a)\u200b\u4e0e\u200b\u60a8\u200b\u4f20\u9012\u200b\u7684\u200b\u4f5c\u54c1\u200b\u7684\u200b\u526f\u672c\u200b\uff08\u200b\u6216\u200b\u7531\u200b\u8fd9\u4e9b\u200b\u526f\u672c\u200b\u5236\u4f5c\u200b\u7684\u200b\u526f\u672c\u200b\uff09\u200b\u6709\u5173\u200b\uff0c\u200b\u6216\u200b(b)\u200b\u4e3b\u8981\u200b\u4e3a\u200b\u5305\u542b\u200b\u8be5\u200b\u4f5c\u54c1\u200b\u7684\u200b\u7279\u5b9a\u200b\u4ea7\u54c1\u200b\u6216\u200b\u6c47\u7f16\u200b\u5e76\u200b\u4e0e\u200b\u4e4b\u200b\u6709\u5173\u200b\uff0c\u200b\u9664\u975e\u200b\u60a8\u200b\u5728\u200b2007\u200b\u5e74\u200b3\u200b\u6708\u200b28\u200b\u65e5\u200b\u4e4b\u524d\u200b\u8fbe\u6210\u200b\u8be5\u200b\u5b89\u6392\u200b\uff0c\u200b\u6216\u200b\u6388\u4e88\u200b\u8be5\u200b\u4e13\u5229\u200b\u8bb8\u53ef\u200b\u3002</p> <p>\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u4e2d\u200b\u7684\u200b\u4efb\u4f55\u200b\u5185\u5bb9\u200b\u90fd\u200b\u4e0d\u5e94\u200b\u88ab\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u6392\u9664\u200b\u6216\u200b\u9650\u5236\u200b\u4efb\u4f55\u200b\u9690\u542b\u200b\u7684\u200b\u8bb8\u53ef\u200b\u6216\u200b\u5176\u4ed6\u200b\u5bf9\u200b\u4fb5\u6743\u200b\u7684\u200b\u6297\u8fa9\u200b\uff0c\u200b\u6839\u636e\u200b\u9002\u7528\u200b\u7684\u200b\u4e13\u5229\u6cd5\u200b\uff0c\u200b\u60a8\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6709\u200b\u8fd9\u6837\u200b\u7684\u200b\u673a\u4f1a\u200b\u3002</p>"},{"location":"zh/about/license/#12","title":"12. \u200b\u4e0d\u200b\u653e\u5f03\u200b\u4ed6\u4eba\u200b\u7684\u200b\u81ea\u7531\u200b.","text":"<p>\u200b\u5982\u679c\u200b\u5f3a\u52a0\u200b\u7ed9\u200b\u60a8\u200b\u7684\u200b\u6761\u4ef6\u200b\uff08\u200b\u65e0\u8bba\u662f\u200b\u901a\u8fc7\u200b\u6cd5\u9662\u200b\u547d\u4ee4\u200b\u3001\u200b\u534f\u8bae\u200b\u6216\u200b\u5176\u4ed6\u200b\u65b9\u5f0f\u200b\uff09\u200b\u4e0e\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u6761\u4ef6\u200b\u76f8\u200b\u62b5\u89e6\u200b\uff0c\u200b\u5b83\u4eec\u200b\u5e76\u200b\u4e0d\u80fd\u200b\u514d\u9664\u200b\u60a8\u200b\u5bf9\u200b\u672c\u200b\u8bb8\u53ef\u200b\u6761\u4ef6\u200b\u7684\u200b\u9075\u5b88\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u4e0d\u80fd\u200b\u5728\u200b\u8f6c\u8ba9\u200b\u4f5c\u54c1\u200b\u65f6\u200b\u540c\u65f6\u200b\u6ee1\u8db3\u200b\u60a8\u200b\u5728\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u4e0b\u200b\u7684\u200b\u4e49\u52a1\u200b\u548c\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u76f8\u5173\u200b\u7684\u200b\u4e49\u52a1\u200b\uff0c\u200b\u90a3\u4e48\u200b\u4f5c\u4e3a\u200b\u7ed3\u679c\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u4e0d\u200b\u8f6c\u8ba9\u200b\u5b83\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u540c\u610f\u200b\u7684\u200b\u6761\u6b3e\u200b\u89c4\u5b9a\u200b\uff0c\u200b\u60a8\u200b\u6709\u200b\u4e49\u52a1\u200b\u5411\u200b\u63a5\u53d7\u200b\u60a8\u200b\u4f20\u9001\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u4eba\u200b\u6536\u53d6\u200b\u7248\u7a0e\u200b\uff0c\u200b\u90a3\u4e48\u200b\u60a8\u200b\u8981\u200b\u540c\u65f6\u200b\u6ee1\u8db3\u200b\u8fd9\u4e9b\u200b\u6761\u6b3e\u200b\u548c\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u8981\u6c42\u200b\uff0c\u200b\u552f\u4e00\u200b\u7684\u200b\u529e\u6cd5\u200b\u5c31\u662f\u200b\u5b8c\u5168\u200b\u4e0d\u200b\u4f20\u9001\u200b\u8be5\u200b\u7a0b\u5e8f\u200b\u3002</p>"},{"location":"zh/about/license/#13-gnu","title":"13. \u200b\u8fdc\u7a0b\u200b\u7f51\u7edc\u200b\u4ea4\u4e92\u200b\uff1b\u200b\u4e0e\u200bGNU\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u4e00\u8d77\u200b\u4f7f\u7528\u200b.","text":"<p>\u200b\u5c3d\u7ba1\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u6709\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u89c4\u5b9a\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u4fee\u6539\u200b\u672c\u200b\u7a0b\u5e8f\u200b\uff0c\u200b\u60a8\u200b\u7684\u200b\u4fee\u6539\u200b\u7248\u672c\u200b\u5fc5\u987b\u200b\u5728\u200b\u663e\u8457\u200b\u4f4d\u7f6e\u200b\u5411\u200b\u6240\u6709\u200b\u901a\u8fc7\u200b\u8ba1\u7b97\u673a\u7f51\u7edc\u200b\u8fdc\u7a0b\u200b\u4e0e\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u4e92\u52a8\u200b\u7684\u200b\u7528\u6237\u200b\uff08\u200b\u5982\u679c\u200b\u60a8\u200b\u7684\u200b\u7248\u672c\u200b\u652f\u6301\u200b\u8fd9\u79cd\u200b\u4e92\u52a8\u200b\uff09\u200b\u63d0\u4f9b\u200b\u673a\u4f1a\u200b\uff0c\u200b\u901a\u8fc7\u200b\u4e00\u4e9b\u200b\u6807\u51c6\u200b\u6216\u200b\u4e60\u60ef\u200b\u7684\u200b\u4fc3\u8fdb\u200b\u8f6f\u4ef6\u200b\u590d\u5236\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u4ece\u200b\u7f51\u7edc\u200b\u670d\u52a1\u5668\u200b\u4e0a\u200b\u514d\u8d39\u200b\u63d0\u4f9b\u200b\u76f8\u5e94\u200b\u7684\u200b\u6e90\u7801\u200b\u3002\u200b\u8be5\u200b\u76f8\u5e94\u200b\u6e90\u7801\u200b\u5e94\u200b\u5305\u62ec\u200b\u6839\u636e\u200b\u4e0b\u200b\u6bb5\u200b\u89c4\u5b9a\u200b\u7eb3\u5165\u200bGNU\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u7b2c\u200b3\u200b\u7248\u200b\u7684\u200b\u4efb\u4f55\u200b\u4f5c\u54c1\u200b\u7684\u200b\u76f8\u5e94\u200b\u6e90\u7801\u200b\u3002</p> <p>\u200b\u5c3d\u7ba1\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u6709\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u89c4\u5b9a\u200b\uff0c\u200b\u60a8\u200b\u6709\u200b\u6743\u5229\u200b\u5c06\u200b\u4efb\u4f55\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u7684\u200b\u4f5c\u54c1\u200b\u4e0e\u200b\u5728\u200bGNU\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u7b2c\u200b3\u200b\u7248\u4e0b\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u4f5c\u54c1\u200b\u94fe\u63a5\u200b\u6216\u200b\u7ed3\u5408\u200b\u6210\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b\u7ec4\u5408\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u5e76\u200b\u4f20\u9012\u200b\u7531\u6b64\u200b\u4ea7\u751f\u200b\u7684\u200b\u4f5c\u54c1\u200b\u3002\u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u6761\u6b3e\u200b\u5c06\u200b\u7ee7\u7eed\u200b\u9002\u7528\u200b\u4e8e\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u4f5c\u54c1\u200b\u7684\u200b\u90e8\u5206\u200b\uff0c\u200b\u4f46\u200b\u4e0e\u200b\u4e4b\u200b\u7ed3\u5408\u200b\u7684\u200b\u4f5c\u54c1\u200b\u5c06\u200b\u7ee7\u7eed\u200b\u53d7\u200bGNU\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u7b2c\u200b3\u200b\u7248\u200b\u7684\u200b\u7ba1\u8f96\u200b\u3002</p>"},{"location":"zh/about/license/#14","title":"14. \u200b\u672c\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u4fee\u8ba2\u7248\u200b.","text":"<p>\u200b\u81ea\u7531\u8f6f\u4ef6\u200b\u57fa\u91d1\u4f1a\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4e0d\u65f6\u200b\u5730\u200b\u53d1\u5e03\u200bGNU Affero\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u4fee\u8ba2\u7248\u200b\u548c\u200b/\u200b\u6216\u200b\u65b0\u200b\u7248\u672c\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u65b0\u200b\u7248\u672c\u200b\u5728\u7cbe\u795e\u4e0a\u200b\u4e0e\u200b\u76ee\u524d\u200b\u7684\u200b\u7248\u672c\u200b\u76f8\u4f3c\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u7ec6\u8282\u200b\u4e0a\u200b\u53ef\u80fd\u200b\u6709\u6240\u4e0d\u540c\u200b\uff0c\u200b\u4ee5\u200b\u89e3\u51b3\u200b\u65b0\u200b\u7684\u200b\u95ee\u9898\u200b\u6216\u200b\u5173\u5207\u200b\u3002</p> <p>\u200b\u6bcf\u4e2a\u200b\u7248\u672c\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200b\u533a\u5206\u200b\u7684\u200b\u7248\u672c\u53f7\u200b\u3002\u200b\u5982\u679c\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u6307\u5b9a\u200b\u67d0\u4e2a\u200b\u7f16\u53f7\u200b\u7684\u200b GNU Affero \u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b \u201c\u200b\u6216\u200b\u4efb\u4f55\u200b\u540e\u6765\u200b\u7684\u200b\u7248\u672c\u200b\u201d \u200b\u9002\u7528\u200b\u4e8e\u200b\u5b83\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u9075\u5b88\u200b\u8be5\u200b\u7f16\u53f7\u200b\u7684\u200b\u7248\u672c\u200b\u6216\u200b\u81ea\u7531\u8f6f\u4ef6\u200b\u57fa\u91d1\u4f1a\u200b\u53d1\u5e03\u200b\u7684\u200b\u4efb\u4f55\u200b\u540e\u6765\u200b\u7684\u200b\u7248\u672c\u200b\u7684\u200b\u6761\u6b3e\u200b\u548c\u200b\u6761\u4ef6\u200b\u3002\u200b\u5982\u679c\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u6ca1\u6709\u200b\u6307\u5b9a\u200b GNU Affero \u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u7248\u672c\u53f7\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u81ea\u7531\u8f6f\u4ef6\u200b\u57fa\u91d1\u4f1a\u200b\u53d1\u5e03\u200b\u7684\u200b\u4efb\u4f55\u200b\u7248\u672c\u200b\u3002 \u200b\u5982\u679c\u200b\u672c\u200b\u8ba1\u5212\u200b\u89c4\u5b9a\u200b\u4ee3\u7406\u4eba\u200b\u53ef\u4ee5\u200b\u51b3\u5b9a\u200b\u672a\u6765\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u54ea\u4e2a\u200b\u7248\u672c\u200b\u7684\u200b GNU Affero \u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\uff0c\u200b\u90a3\u4e48\u200b\u8be5\u200b\u4ee3\u7406\u4eba\u200b\u5bf9\u200b\u67d0\u4e2a\u200b\u7248\u672c\u200b\u7684\u200b\u516c\u5f00\u200b\u63a5\u53d7\u200b\u58f0\u660e\u200b\u5c06\u200b\u6c38\u4e45\u200b\u6388\u6743\u200b\u60a8\u200b\u4e3a\u200b\u672c\u200b\u8ba1\u5212\u200b\u9009\u62e9\u200b\u8be5\u200b\u7248\u672c\u200b\u3002</p> <p>\u200b\u4ee5\u540e\u200b\u7684\u200b\u8bb8\u53ef\u8bc1\u200b\u7248\u672c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u7ed9\u200b\u60a8\u200b\u989d\u5916\u200b\u7684\u200b\u6216\u200b\u4e0d\u540c\u200b\u7684\u200b\u6743\u9650\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u4efb\u4f55\u200b\u4f5c\u8005\u200b\u6216\u200b\u7248\u6743\u200b\u6301\u6709\u4eba\u200b\u90fd\u200b\u4e0d\u4f1a\u200b\u56e0\u4e3a\u200b\u60a8\u200b\u9009\u62e9\u200b\u4e86\u200b\u540e\u6765\u200b\u7684\u200b\u7248\u672c\u200b\u800c\u200b\u627f\u62c5\u200b\u989d\u5916\u200b\u7684\u200b\u4e49\u52a1\u200b\u3002</p>"},{"location":"zh/about/license/#15","title":"15. \u200b\u514d\u8d23\u200b\u58f0\u660e\u200b.","text":"<p>\u200b\u5728\u200b\u9002\u7528\u6cd5\u5f8b\u200b\u5141\u8bb8\u200b\u7684\u200b\u8303\u56f4\u200b\u5185\u200b\uff0c\u200b\u5bf9\u200b\u8be5\u200b\u7a0b\u5e8f\u200b\u6ca1\u6709\u200b\u4efb\u4f55\u200b\u4fdd\u8bc1\u200b\u3002\u200b\u9664\u975e\u200b\u53e6\u6709\u200b\u4e66\u9762\u200b\u8bf4\u660e\u200b\uff0c\u200b\u5426\u5219\u200b\u7248\u6743\u200b\u6301\u6709\u4eba\u200b\u548c\u200b/\u200b\u6216\u200b\u5176\u4ed6\u200b\u5404\u65b9\u200b \u201c\u200b\u6309\u200b\u539f\u6837\u200b\u201d \u200b\u63d0\u4f9b\u200b\u8be5\u200b\u7a0b\u5e8f\u200b\uff0c\u200b\u4e0d\u200b\u63d0\u4f9b\u200b\u4efb\u4f55\u200b\u660e\u793a\u200b\u6216\u200b\u6697\u793a\u200b\u7684\u200b\u4fdd\u8bc1\u200b\uff0c\u200b\u5305\u62ec\u200b\u4f46\u200b\u4e0d\u200b\u9650\u4e8e\u200b\u5bf9\u200b\u9002\u9500\u200b\u6027\u200b\u548c\u200b\u7279\u5b9a\u200b\u7528\u9014\u200b\u7684\u200b\u9002\u7528\u6027\u200b\u7684\u200b\u6697\u793a\u200b\u4fdd\u8bc1\u200b\u3002\u200b\u5173\u4e8e\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u8d28\u91cf\u200b\u548c\u200b\u6027\u80fd\u200b\u7684\u200b\u5168\u90e8\u200b\u98ce\u9669\u200b\u7531\u200b\u60a8\u200b\u627f\u62c5\u200b\u3002\u200b\u5982\u679c\u200b\u8be5\u200b\u7a0b\u5e8f\u200b\u88ab\u200b\u8bc1\u660e\u200b\u6709\u200b\u7f3a\u9677\u200b\uff0c\u200b\u60a8\u200b\u5c06\u200b\u627f\u62c5\u200b\u6240\u6709\u200b\u5fc5\u8981\u200b\u7684\u200b\u670d\u52a1\u200b\u3001\u200b\u4fee\u7406\u200b\u6216\u200b\u7ea0\u6b63\u200b\u7684\u200b\u8d39\u7528\u200b\u3002</p>"},{"location":"zh/about/license/#16","title":"16. \u200b\u8d54\u507f\u200b\u8d23\u4efb\u200b\u7684\u200b\u9650\u5236\u200b.","text":"<p>\u200b\u5728\u200b\u4efb\u4f55\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u9664\u975e\u200b\u9002\u7528\u6cd5\u5f8b\u200b\u8981\u6c42\u200b\u6216\u200b\u4e66\u9762\u200b\u540c\u610f\u200b\uff0c\u200b\u4efb\u4f55\u200b\u7248\u6743\u200b\u6301\u6709\u4eba\u200b\u6216\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u6309\u200b\u4e0a\u8ff0\u200b\u89c4\u5b9a\u200b\u4fee\u6539\u200b\u548c\u200b/\u200b\u6216\u200b\u4f20\u9012\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u5f53\u4e8b\u4eba\u200b\u90fd\u200b\u4e0d\u200b\u5bf9\u200b\u60a8\u200b\u7684\u200b\u635f\u5bb3\u200b\u8d1f\u8d23\u200b\uff0c\u200b\u5305\u62ec\u200b\u56e0\u200b\u4f7f\u7528\u200b\u6216\u200b\u65e0\u6cd5\u200b\u4f7f\u7528\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u800c\u200b\u5f15\u8d77\u200b\u7684\u200b\u4efb\u4f55\u200b\u4e00\u822c\u200b\u7684\u200b\u3001\u200b\u7279\u6b8a\u200b\u7684\u200b\u3001\u200b\u5076\u7136\u200b\u7684\u200b\u6216\u200b\u95f4\u63a5\u200b\u7684\u200b\u635f\u5bb3\u200b\uff08\u200b\u5305\u62ec\u200b\u4f46\u200b\u4e0d\u200b\u9650\u4e8e\u200b\u6570\u636e\u200b\u4e22\u5931\u200b\u6216\u200b\u6570\u636e\u200b\u4e0d\u200b\u51c6\u786e\u200b\u6216\u200b\u60a8\u200b\u6216\u200b\u7b2c\u4e09\u65b9\u200b\u906d\u53d7\u200b\u7684\u200b\u635f\u5931\u200b\u6216\u672c\u200b\u7a0b\u5e8f\u200b\u65e0\u6cd5\u200b\u4e0e\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u7a0b\u5e8f\u200b\u4e00\u8d77\u200b\u8fd0\u884c\u200b\uff09\uff0c\u200b\u5373\u4f7f\u200b\u8be5\u200b\u6301\u6709\u4eba\u200b\u6216\u200b\u5176\u4ed6\u200b\u5f53\u4e8b\u4eba\u200b\u5df2\u200b\u88ab\u200b\u544a\u77e5\u200b\u8fd9\u79cd\u200b\u635f\u5bb3\u200b\u7684\u200b\u53ef\u80fd\u6027\u200b\u3002</p>"},{"location":"zh/about/license/#17-1516","title":"17. \u200b\u7b2c\u200b15\u200b\u6761\u200b\u548c\u200b\u7b2c\u200b16\u200b\u6761\u200b\u7684\u200b\u89e3\u91ca\u200b.","text":"<p>\u200b\u5982\u679c\u200b\u4ee5\u4e0a\u200b\u89c4\u5b9a\u200b\u7684\u200b\u514d\u8d23\u200b\u58f0\u660e\u200b\u548c\u200b\u8d23\u4efb\u200b\u9650\u5236\u200b\u4e0d\u80fd\u200b\u6839\u636e\u200b\u5176\u200b\u6761\u6b3e\u200b\u5728\u200b\u5f53\u5730\u200b\u4ea7\u751f\u200b\u6cd5\u5f8b\u6548\u529b\u200b\uff0c\u200b\u5ba1\u67e5\u200b\u6cd5\u9662\u200b\u5e94\u200b\u9002\u7528\u200b\u6700\u200b\u63a5\u8fd1\u200b\u4e8e\u200b\u7edd\u5bf9\u200b\u653e\u5f03\u200b\u4e0e\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u6709\u5173\u200b\u7684\u200b\u6240\u6709\u200b\u6c11\u4e8b\u8d23\u4efb\u200b\u7684\u200b\u5f53\u5730\u200b\u6cd5\u5f8b\u200b\uff0c\u200b\u9664\u975e\u200b\u5728\u200b\u6536\u53d6\u200b\u8d39\u7528\u200b\u7684\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u526f\u672c\u200b\u4e0a\u200b\u9644\u6709\u200b\u4fdd\u8bc1\u200b\u6216\u200b\u8d23\u4efb\u200b\u627f\u62c5\u200b\u3002</p> <p>\u200b\u4ee5\u4e0a\u200b\u662f\u200b\u6761\u6b3e\u200b\u548c\u200b\u6761\u4ef6\u200b</p>"},{"location":"zh/about/license/#_3","title":"\u5982\u4f55\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u6761\u6b3e\u200b\u5e94\u7528\u200b\u4e8e\u200b\u60a8\u200b\u7684\u200b\u65b0\u200b\u7a0b\u5e8f","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u5f00\u53d1\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b\u7a0b\u5e8f\u200b\uff0c\u200b\u5e76\u200b\u5e0c\u671b\u200b\u5b83\u200b\u5bf9\u200b\u516c\u4f17\u200b\u6709\u200b\u6700\u5927\u200b\u7684\u200b\u7528\u5904\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u8fd9\u4e00\u200b\u76ee\u6807\u200b\u7684\u200b\u6700\u597d\u200b\u65b9\u6cd5\u200b\u662f\u200b\u4f7f\u200b\u5b83\u200b\u6210\u4e3a\u200b\u81ea\u7531\u8f6f\u4ef6\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u4eba\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8fd9\u4e9b\u200b\u6761\u6b3e\u200b\u4e0b\u200b\u91cd\u65b0\u200b\u53d1\u5e03\u200b\u548c\u200b\u4fee\u6539\u200b\u3002</p> <p>\u200b\u8981\u200b\u505a\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u8bf7\u200b\u5728\u200b\u7a0b\u5e8f\u200b\u4e2d\u200b\u9644\u4e0a\u200b\u4ee5\u4e0b\u200b\u901a\u77e5\u200b\u3002\u200b\u6700\u200b\u5b89\u5168\u200b\u7684\u200b\u505a\u6cd5\u200b\u662f\u200b\u628a\u200b\u5b83\u4eec\u200b\u9644\u5728\u200b\u6bcf\u4e2a\u200b\u6e90\u6587\u4ef6\u200b\u7684\u200b\u5f00\u5934\u200b\uff0c\u200b\u4ee5\u200b\u6700\u200b\u6709\u6548\u200b\u5730\u200b\u8bf4\u660e\u200b\u6392\u9664\u200b\u62c5\u4fdd\u200b\u7684\u200b\u60c5\u51b5\u200b\uff1b\u200b\u6bcf\u4e2a\u200b\u6587\u4ef6\u200b\u81f3\u5c11\u200b\u8981\u200b\u6709\u200b \u201c\u200b\u7248\u6743\u200b\u201d \u200b\u4e00\u884c\u200b\u548c\u200b\u4e00\u4e2a\u200b\u6307\u5411\u200b\u5b8c\u6574\u200b\u901a\u77e5\u200b\u7684\u200b\u6307\u9488\u200b\u3002</p> Text Only<pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as\npublished by the Free Software Foundation, either version 3 of the\nLicense, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>\u200b\u7ffb\u8bd1\u200b\uff1a</p> Text Only<pre><code>&lt;\u200b\u7528\u200b\u4e00\u884c\u200b\u5b57\u6765\u200b\u8bf4\u660e\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u540d\u79f0\u200b\u548c\u200b\u5b83\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u4e8b\u60c5\u200b\u7684\u200b\u7b80\u5355\u200b\u6982\u5ff5\u200b\u3002&gt;\nCopyright (C) &lt;\u200b\u5e74\u200b&gt; &lt;\u200b\u4f5c\u8005\u59d3\u540d\u200b&gt; \u200b\u7248\u6743\u6240\u6709\u200b\u3002\n\n\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u662f\u200b\u81ea\u7531\u8f6f\u4ef6\u200b\uff1a\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u81ea\u7531\u8f6f\u4ef6\u200b\u57fa\u91d1\u4f1a\u200b\u53d1\u5e03\u200b\u7684\u200bGNU Affero\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u6761\u6b3e\u200b\uff0c\u200b\u5373\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u7b2c\u200b3\u200b\u7248\u200b\u6216\u200b\uff08\u200b\u60a8\u200b\u9009\u62e9\u200b\u7684\u200b\uff09\u200b\u4efb\u4f55\u200b\u540e\u6765\u200b\u7684\u200b\u7248\u672c\u200b\u91cd\u65b0\u200b\u53d1\u5e03\u200b\u5b83\u200b\u548c\u200b/\u200b\u6216\u200b\u4fee\u6539\u200b\u5b83\u200b\u3002\u3002\n\n\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u53d1\u5e03\u200b\u662f\u200b\u5e0c\u671b\u200b\u5b83\u200b\u80fd\u200b\u8d77\u5230\u200b\u4f5c\u7528\u200b\u3002\u200b\u4f46\u200b\u6ca1\u6709\u200b\u4efb\u4f55\u200b\u4fdd\u8bc1\u200b\uff1b\u200b\u751a\u81f3\u200b\u6ca1\u6709\u200b\u9690\u542b\u200b\u7684\u200b\u4fdd\u8bc1\u200b\u3002\u200b\u672c\u200b\u7a0b\u5e8f\u200b\u7684\u200b\u5206\u53d1\u200b\u662f\u200b\u5e0c\u671b\u200b\u5b83\u200b\u662f\u200b\u6709\u7528\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u6ca1\u6709\u200b\u4efb\u4f55\u200b\u4fdd\u8bc1\u200b\uff0c\u200b\u751a\u81f3\u200b\u6ca1\u6709\u200b\u9690\u542b\u200b\u7684\u200b\u9002\u9500\u5bf9\u8def\u200b\u6216\u200b\u9002\u5408\u200b\u67d0\u4e00\u200b\u7279\u5b9a\u200b\u76ee\u7684\u200b\u7684\u200b\u4fdd\u8bc1\u200b\u3002 \u200b\u53c2\u89c1\u200b GNU Affero\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u7ec6\u8282\u200b\u3002\n\n\u200b\u60a8\u200b\u5e94\u8be5\u200b\u5df2\u7ecf\u200b\u6536\u5230\u200b\u4e86\u200b\u4e00\u4efd\u200bGNU Affero\u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u7684\u200b\u526f\u672c\u200b\u3002 \u200b\u5982\u679c\u200b\u6ca1\u6709\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u89c1\u200b&lt;https://www.gnu.org/licenses/&gt;\u3002\n\n\u200b\u8fd8\u8981\u200b\u589e\u52a0\u200b\u5982\u4f55\u200b\u901a\u8fc7\u200b\u7535\u5b50\u200b\u548c\u200b\u7eb8\u8d28\u200b\u90ae\u4ef6\u200b\u4e0e\u200b\u60a8\u200b\u8054\u7cfb\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002\n</code></pre> <p>\u200b\u5982\u679c\u200b\u60a8\u200b\u7684\u200b\u8f6f\u4ef6\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8ba1\u7b97\u673a\u7f51\u7edc\u200b\u4e0e\u200b\u7528\u6237\u200b\u8fdb\u884c\u200b\u8fdc\u7a0b\u200b\u4ea4\u4e92\u200b\uff0c\u200b\u60a8\u200b\u4e5f\u200b\u5e94\u8be5\u200b\u786e\u4fdd\u200b\u5b83\u200b\u4e3a\u200b\u7528\u6237\u200b\u63d0\u4f9b\u200b\u4e00\u79cd\u200b\u83b7\u5f97\u200b\u5176\u200b\u6e90\u4ee3\u7801\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u7684\u200b\u7a0b\u5e8f\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7f51\u7edc\u5e94\u7528\u200b\u7a0b\u5e8f\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u754c\u9762\u200b\u53ef\u4ee5\u200b\u663e\u793a\u200b\u4e00\u4e2a\u200b \u201c\u200b\u6e90\u4ee3\u7801\u200b\u201d \u200b\u7684\u200b\u94fe\u63a5\u200b\uff0c\u200b\u5f15\u5bfc\u200b\u7528\u6237\u200b\u8fdb\u5165\u200b\u4ee3\u7801\u200b\u7684\u200b\u5b58\u6863\u200b\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u7528\u200b\u5f88\u591a\u200b\u65b9\u6cd5\u200b\u63d0\u4f9b\u200b\u6e90\u7801\u200b\uff0c\u200b\u4e0d\u540c\u200b\u7684\u200b\u89e3\u51b3\u65b9\u6848\u200b\u5bf9\u200b\u4e0d\u540c\u200b\u7684\u200b\u7a0b\u5e8f\u200b\u4f1a\u200b\u66f4\u597d\u200b\uff1b\u200b\u5177\u4f53\u200b\u8981\u6c42\u200b\u89c1\u200b\u7b2c\u200b13\u200b\u8282\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u6709\u200b\u5fc5\u8981\u200b\uff0c\u200b\u60a8\u200b\u8fd8\u200b\u5e94\u8be5\u200b\u8ba9\u200b\u60a8\u200b\u7684\u200b\u96c7\u4e3b\u200b\uff08\u200b\u5982\u679c\u200b\u60a8\u200b\u662f\u200b\u7a0b\u5e8f\u5458\u200b\uff09\u200b\u6216\u200b\u5b66\u6821\u200b\uff08\u200b\u5982\u679c\u200b\u6709\u200b\u7684\u8bdd\u200b\uff09\u200b\u4e3a\u200b\u8be5\u200b\u7a0b\u5e8f\u200b\u7b7e\u7f72\u200b\u4e00\u4efd\u200b \u201c\u200b\u7248\u6743\u200b\u514d\u8d23\u200b\u58f0\u660e\u200b\u201d\u3002\u200b\u6709\u5173\u200b\u8fd9\u65b9\u9762\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5982\u4f55\u200b\u7533\u8bf7\u200b\u548c\u200b\u9075\u5b88\u200bGNU AGPL\uff0c\u200b\u8bf7\u200b\u53c2\u89c1\u200bhttps://www.gnu.org/licenses/\u3002</p>"},{"location":"zh/about/privacy/","title":"Privacy Notice","text":"<p>\u200b\u7ffb\u8bd1\u200b</p> <p>\u200b\u672c\u6587\u200b\u5185\u5bb9\u200b\u4e3a\u200b\u673a\u5668\u7ffb\u8bd1\u200b\u7248\u672c\u200b\uff0c\u200b\u65e8\u5728\u200b\u4e3a\u200b\u7528\u6237\u200b\u63d0\u4f9b\u65b9\u4fbf\u200b\u3002 \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c3d\u529b\u200b\u786e\u4fdd\u200b\u7ffb\u8bd1\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\u3002 \u200b\u4f46\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u7ffb\u8bd1\u200b\u5185\u5bb9\u200b\u53ef\u80fd\u200b\u5305\u542b\u200b\u9519\u8bef\u200b\uff0c\u200b\u4ec5\u4f9b\u53c2\u8003\u200b\u3002 \u200b\u8bf7\u4ee5\u200b\u82f1\u6587\u200b\u539f\u6587\u200b\u4e3a\u51c6\u200b\u3002</p> <p>\u200b\u4e3a\u200b\u6ee1\u8db3\u200b\u5408\u89c4\u6027\u200b\u4e0e\u200b\u6267\u6cd5\u200b\u8981\u6c42\u200b\uff0c\u200b\u7ffb\u8bd1\u200b\u6587\u6863\u200b\u4e2d\u200b\u7684\u200b\u4efb\u4f55\u200b\u4e0d\u200b\u51c6\u786e\u200b\u6216\u200b\u6b67\u4e49\u200b\u4e4b\u5904\u200b\u5747\u200b\u4e0d\u200b\u5177\u6709\u200b\u7ea6\u675f\u529b\u200b\uff0c\u200b\u4e5f\u200b\u4e0d\u200b\u5177\u5907\u200b\u6cd5\u5f8b\u6548\u529b\u200b\u3002</p> <p>\u200b\u6700\u540e\u200b\u4fee\u8ba2\u200b\u65e5\u671f\u200b</p> <p>\u200b\u672c\u200b\u58f0\u660e\u200b\u6700\u540e\u200b\u66f4\u65b0\u200b\u4e8e\u200b2024\u200b\u5e74\u200b5\u200b\u6708\u200b4\u200b\u65e5\u200b\u3002</p>"},{"location":"zh/about/privacy/#_1","title":"\u9690\u79c1\u200b\u58f0\u660e","text":"<p>\u200b\u672c\u200b\u9690\u79c1\u200b\u58f0\u660e\u200b\u9002\u7528\u200b\u4e8e\u4e39\u7075\u200b\u56e2\u961f\u200b\uff08\u200b\u4e5f\u200b\u88ab\u79f0\u4f5c\u200b\u4e39\u7075\u200b\uff09\uff08\u200b\u4ee5\u4e0b\u200b\u7b80\u79f0\u200b\u201c\u200b\u6211\u4eec\u200b\u201d\uff09\uff0c\u200b\u63cf\u8ff0\u200b\u4e86\u200b\u5f53\u200b\u60a8\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\uff08\u201c\u200b\u670d\u52a1\u200b\u201d\uff09\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u4ee5\u53ca\u200b\u4e3a\u4f55\u200b\u53ef\u80fd\u200b\u6536\u96c6\u200b\u3001\u200b\u5b58\u50a8\u200b\u3001\u200b\u4f7f\u7528\u200b\u548c\u200b/\u200b\u6216\u200b\u5171\u4eab\u200b\uff08\u201c\u200b\u5904\u7406\u200b\u201d\uff09\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002\u200b\u4f8b\u5982\u200b\u5f53\u200b\u60a8\u200b\uff1a</p> <ul> <li>\u200b\u8bbf\u95ee\u200b\u6211\u4eec\u200b\u7684\u200b\u7f51\u7ad9\u200b danling.org \u200b\u6216\u200b\u4efb\u4f55\u200b\u94fe\u63a5\u200b\u5230\u200b\u672c\u200b\u9690\u79c1\u200b\u58f0\u660e\u200b\u7684\u200b\u6211\u4eec\u200b\u7684\u200b\u7f51\u7ad9\u200b\u65f6\u200b</li> </ul> <p>\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u968f\u65f6\u200b\u901a\u8fc7\u200b\u70b9\u51fb\u200b\u4e0b\u9762\u200b\u7684\u200b\u6309\u94ae\u200b\u66f4\u6539\u200b\u60a8\u200b\u7684\u200b\u9690\u79c1\u200b\u8bbe\u7f6e\u200b\uff1a</p> <p>\u200b\u9690\u79c1\u200b\u63a7\u5236\u200b</p> <p>\u200b\u6709\u200b\u95ee\u9898\u200b\u6216\u200b\u5173\u6ce8\u200b\uff1f \u200b\u9605\u8bfb\u200b\u672c\u200b\u9690\u79c1\u200b\u58f0\u660e\u200b\u5c06\u200b\u5e2e\u52a9\u200b\u60a8\u200b\u4e86\u89e3\u200b\u60a8\u200b\u7684\u200b\u9690\u79c1\u200b\u6743\u5229\u200b\u548c\u200b\u9009\u62e9\u200b\u3002 \u200b\u5982\u679c\u200b\u60a8\u200b\u4e0d\u200b\u540c\u610f\u200b\u6211\u4eec\u200b\u7684\u200b\u58f0\u660e\u200b\u548c\u200b\u505a\u6cd5\u200b\uff0c\u200b\u8bf7\u200b\u4e0d\u8981\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u3002 \u200b\u5982\u679c\u200b\u60a8\u200b\u4ecd\u200b\u6709\u200b\u4efb\u4f55\u200b\u95ee\u9898\u200b\u6216\u200b\u5173\u6ce8\u200b\uff0c\u200b\u8bf7\u200b\u901a\u8fc7\u200bprivacy@danling.org\u200b\u4e0e\u200b\u6211\u4eec\u200b\u8054\u7cfb\u200b\u3002</p>"},{"location":"zh/about/privacy/#0","title":"0. \u200b\u5173\u952e\u70b9\u200b\u603b\u7ed3","text":"<p>\u200b\u672c\u200b\u603b\u7ed3\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u6211\u4eec\u200b\u9690\u79c1\u200b\u58f0\u660e\u200b\u7684\u200b\u5173\u952e\u70b9\u200b\uff0c\u200b\u4f46\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u70b9\u51fb\u200b\u6bcf\u4e2a\u200b\u5173\u952e\u70b9\u200b\u540e\u200b\u7684\u200b\u94fe\u63a5\u200b\u6216\u200b\u4f7f\u7528\u200b\u4e0b\u9762\u200b\u7684\u200b\u76ee\u5f55\u200b\u627e\u5230\u200b\u60a8\u200b\u6240\u200b\u67e5\u627e\u200b\u7684\u200b\u90e8\u5206\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u8be6\u60c5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u54ea\u4e9b\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff1f</p> <p>\u200b\u5f53\u200b\u60a8\u200b\u8bbf\u95ee\u200b\u3001\u200b\u4f7f\u7528\u200b\u6216\u200b\u5bfc\u822a\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6839\u636e\u200b\u60a8\u200b\u4e0e\u200b\u6211\u4eec\u200b\u4ee5\u53ca\u200b\u670d\u52a1\u200b\u7684\u200b\u4e92\u52a8\u200b\u65b9\u5f0f\u200b\u3001\u200b\u60a8\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u9009\u62e9\u200b\u4ee5\u53ca\u200b\u60a8\u200b\u4f7f\u7528\u200b\u7684\u200b\u4ea7\u54c1\u200b\u548c\u200b\u529f\u80fd\u200b\u6765\u200b\u5904\u7406\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u6211\u4eec\u200b\u6536\u96c6\u200b\u54ea\u4e9b\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u4efb\u4f55\u200b\u654f\u611f\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u5417\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u4e0d\u200b\u5904\u7406\u200b\u4efb\u4f55\u200b\u654f\u611f\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4ece\u200b\u7b2c\u4e09\u65b9\u200b\u6536\u96c6\u200b\u4fe1\u606f\u200b\u5417\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u4e0d\u200b\u4ece\u200b\u7b2c\u4e09\u65b9\u200b\u6536\u96c6\u200b\u4efb\u4f55\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u4ee5\u200b\u63d0\u4f9b\u200b\u3001\u200b\u6539\u5584\u200b\u548c\u200b\u7ba1\u7406\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\uff0c\u200b\u4e0e\u200b\u60a8\u200b\u6c9f\u901a\u200b\uff0c\u200b\u8fdb\u884c\u200b\u5b89\u5168\u200b\u548c\u200b\u9632\u200b\u6b3a\u8bc8\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u9075\u5b88\u200b\u6cd5\u5f8b\u200b\u3002 \u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u5728\u200b\u5f97\u5230\u200b\u60a8\u200b\u7684\u200b\u540c\u610f\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u51fa\u4e8e\u200b\u5176\u4ed6\u200b\u76ee\u7684\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002 \u200b\u6211\u4eec\u200b\u4ec5\u200b\u5728\u200b\u6709\u200b\u5408\u6cd5\u200b\u6cd5\u5f8b\u200b\u7406\u7531\u200b\u65f6\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u5728\u200b\u54ea\u4e9b\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u4ee5\u53ca\u200b\u4e0e\u200b\u54ea\u4e9b\u200b\u65b9\u200b\u6211\u4eec\u200b\u5171\u4eab\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u5728\u200b\u7279\u5b9a\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u4e0e\u200b\u7279\u5b9a\u200b\u7b2c\u4e09\u65b9\u200b\u5171\u4eab\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u6211\u4eec\u200b\u4f55\u65f6\u200b\u4ee5\u53ca\u200b\u4e0e\u200b\u8c01\u200b\u5171\u4eab\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u4fdd\u62a4\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u5b89\u5168\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5b9e\u65bd\u200b\u4e86\u200b\u7ec4\u7ec7\u200b\u548c\u200b\u6280\u672f\u200b\u6d41\u7a0b\u200b\u548c\u200b\u7a0b\u5e8f\u200b\u6765\u200b\u4fdd\u62a4\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u3002 \u200b\u7136\u800c\u200b\uff0c\u200b\u4efb\u4f55\u200b\u901a\u8fc7\u200b\u4e92\u8054\u7f51\u200b\u7684\u200b\u7535\u5b50\u200b\u4f20\u8f93\u200b\u6216\u200b\u4fe1\u606f\u200b\u5b58\u50a8\u6280\u672f\u200b\u90fd\u200b\u65e0\u6cd5\u200b\u4fdd\u8bc1\u200b\u662f\u200b100%\u200b\u5b89\u5168\u200b\u7684\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u65e0\u6cd5\u200b\u627f\u8bfa\u200b\u6216\u200b\u4fdd\u8bc1\u200b\u9ed1\u5ba2\u200b\u3001\u200b\u7f51\u7edc\u200b\u72af\u7f6a\u5206\u5b50\u200b\u6216\u200b\u5176\u4ed6\u200b\u672a\u7ecf\u200b\u6388\u6743\u200b\u7684\u200b\u7b2c\u4e09\u65b9\u200b\u4e0d\u4f1a\u200b\u7834\u574f\u200b\u6211\u4eec\u200b\u7684\u200b\u5b89\u5168\u63aa\u65bd\u200b\u5e76\u200b\u4e0d\u200b\u5f53\u5730\u200b\u6536\u96c6\u200b\u3001\u200b\u8bbf\u95ee\u200b\u3001\u200b\u7a83\u53d6\u200b\u6216\u200b\u4fee\u6539\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u4fdd\u6301\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u5b89\u5168\u200b\u3002</p> <p>\u200b\u60a8\u200b\u6709\u200b\u54ea\u4e9b\u200b\u6743\u5229\u200b\uff1f</p> <p>\u200b\u6839\u636e\u200b\u60a8\u200b\u6240\u5728\u200b\u5730\u7406\u4f4d\u7f6e\u200b\uff0c\u200b\u9002\u7528\u200b\u7684\u200b\u9690\u79c1\u200b\u6cd5\u200b\u53ef\u80fd\u200b\u610f\u5473\u7740\u200b\u60a8\u200b\u5bf9\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u6709\u200b\u67d0\u4e9b\u200b\u6743\u5229\u200b\u3002</p> <p>\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u60a8\u200b\u6709\u200b\u54ea\u4e9b\u200b\u9690\u79c1\u200b\u6743\u5229\u200b\u3002</p> <p>\u200b\u60a8\u200b\u5982\u4f55\u200b\u884c\u4f7f\u200b\u60a8\u200b\u7684\u200b\u6743\u5229\u200b\uff1f</p> <p>\u200b\u884c\u4f7f\u200b\u60a8\u200b\u7684\u200b\u6743\u5229\u200b\u7684\u200b\u6700\u200b\u7b80\u5355\u200b\u65b9\u5f0f\u200b\u662f\u200b\u901a\u8fc7\u200bprivacy@danling.org\u200b\u4e0e\u200b\u6211\u4eec\u200b\u8054\u7cfb\u200b\u3002 \u200b\u6211\u4eec\u200b\u5c06\u200b\u6839\u636e\u200b\u9002\u7528\u200b\u7684\u200b\u6570\u636e\u4fdd\u62a4\u200b\u6cd5\u200b\u8003\u8651\u200b\u5e76\u200b\u91c7\u53d6\u884c\u52a8\u200b\u3002</p>"},{"location":"zh/about/privacy/#1","title":"1. \u200b\u6211\u4eec\u200b\u6536\u96c6\u200b\u54ea\u4e9b\u200b\u4fe1\u606f\u200b\uff1f","text":""},{"location":"zh/about/privacy/#_2","title":"\u60a8\u200b\u5411\u200b\u6211\u4eec\u200b\u62ab\u9732\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f","text":"<p>\u200b\u7b80\u800c\u8a00\u4e4b\u200b</p> <p>\u200b\u6211\u4eec\u200b\u6536\u96c6\u200b\u60a8\u200b\u5411\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u6536\u96c6\u200b\u60a8\u200b\u81ea\u613f\u200b\u5411\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff0c\u200b\u5f53\u200b\u60a8\u200b\u8868\u8fbe\u200b\u5bf9\u200b\u6211\u4eec\u200b\u6216\u200b\u6211\u4eec\u200b\u7684\u200b\u4ea7\u54c1\u200b\u548c\u200b\u670d\u52a1\u200b\u7684\u200b\u5174\u8da3\u200b\u3001\u200b\u53c2\u4e0e\u200b\u670d\u52a1\u200b\u4e0a\u200b\u7684\u200b\u6d3b\u52a8\u200b\u6216\u200b\u4ee5\u200b\u5176\u4ed6\u200b\u65b9\u5f0f\u200b\u8054\u7cfb\u200b\u6211\u4eec\u200b\u65f6\u200b\u3002</p> <p>\u200b\u654f\u611f\u200b\u4e2a\u4eba\u4fe1\u606f\u200b</p> <p>\u200b\u6211\u4eec\u200b\u4e0d\u200b\u4ece\u200b\u60a8\u200b\u90a3\u91cc\u200b\u6536\u96c6\u200b\u4efb\u4f55\u200b\u654f\u611f\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u3002</p>"},{"location":"zh/about/privacy/#_3","title":"\u81ea\u52a8\u200b\u6536\u96c6\u200b\u7684\u200b\u4fe1\u606f","text":"<p>\u200b\u7b80\u800c\u8a00\u4e4b\u200b</p> <p>\u200b\u5f53\u200b\u60a8\u200b\u8bbf\u95ee\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u65f6\u200b\uff0c\u200b\u67d0\u4e9b\u200b\u4fe1\u606f\u200b\u2014\u2014\u200b\u5982\u200bIP\u200b\u5730\u5740\u200b\u548c\u200b/\u200b\u6216\u200b\u6d4f\u89c8\u5668\u200b\u548c\u200b\u8bbe\u5907\u200b\u7279\u5f81\u200b\u2014\u2014\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u88ab\u200b\u6536\u96c6\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5728\u200b\u60a8\u200b\u8bbf\u95ee\u200b\u3001\u200b\u4f7f\u7528\u200b\u6216\u200b\u5bfc\u822a\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u65f6\u200b\u81ea\u52a8\u200b\u6536\u96c6\u200b\u67d0\u4e9b\u200b\u4fe1\u606f\u200b\u3002 \u200b\u8fd9\u4e9b\u200b\u4fe1\u606f\u200b\u4e0d\u4f1a\u200b\u900f\u9732\u200b\u60a8\u200b\u7684\u200b\u7279\u5b9a\u200b\u8eab\u4efd\u200b\uff08\u200b\u5982\u200b\u60a8\u200b\u7684\u200b\u59d3\u540d\u200b\u6216\u200b\u8054\u7cfb\u200b\u4fe1\u606f\u200b\uff09\uff0c\u200b\u4f46\u200b\u53ef\u80fd\u200b\u5305\u62ec\u200b\u8bbe\u5907\u200b\u548c\u200b\u4f7f\u7528\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5982\u200b\u60a8\u200b\u7684\u200bIP\u200b\u5730\u5740\u200b\u3001\u200b\u6d4f\u89c8\u5668\u200b\u548c\u200b\u8bbe\u5907\u200b\u7279\u6027\u200b\u3001\u200b\u64cd\u4f5c\u7cfb\u7edf\u200b\u3001\u200b\u8bed\u8a00\u200b\u504f\u597d\u200b\u3001\u200b\u5f15\u7528\u200bURL\u3001\u200b\u8bbe\u5907\u200b\u540d\u79f0\u200b\u3001\u200b\u56fd\u5bb6\u200b\u3001\u200b\u4f4d\u7f6e\u200b\u3001\u200b\u6709\u5173\u200b\u60a8\u200b\u5982\u4f55\u200b\u4ee5\u53ca\u200b\u4f55\u65f6\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5176\u4ed6\u200b\u6280\u672f\u200b\u4fe1\u606f\u200b\u3002 \u200b\u8fd9\u4e9b\u200b\u4fe1\u606f\u200b\u4e3b\u8981\u200b\u662f\u200b\u4e3a\u4e86\u200b\u7ef4\u62a4\u200b\u6211\u4eec\u200b\u670d\u52a1\u200b\u7684\u200b\u5b89\u5168\u6027\u200b\u548c\u200b\u8fd0\u4f5c\u200b\u6240\u200b\u9700\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u6211\u4eec\u200b\u5185\u90e8\u200b\u7684\u200b\u5206\u6790\u200b\u548c\u200b\u62a5\u544a\u200b\u76ee\u7684\u200b\u3002</p> <p>\u200b\u50cf\u200b\u8bb8\u591a\u200b\u4f01\u4e1a\u200b\u4e00\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u901a\u8fc7\u200bcookies\u200b\u548c\u200b\u7c7b\u4f3c\u200b\u6280\u672f\u200b\u6536\u96c6\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u6536\u96c6\u200b\u7684\u200b\u4fe1\u606f\u200b\u5305\u62ec\u200b\uff1a</p> <ul> <li>\u200b\u6807\u8bc6\u7b26\u200b\u3002   \u200b\u6807\u8bc6\u7b26\u200b\u662f\u200b\u5f53\u200b\u60a8\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u751f\u6210\u200b\u7684\u200b\u7279\u5b9a\u200b\u4e8e\u200b\u8bbe\u5907\u200b\u548c\u200b\u6d4f\u89c8\u5668\u200b\u7684\u200b\u552f\u4e00\u200b\u968f\u673a\u200b\u5b57\u7b26\u4e32\u200b\u3002   \u200b\u8be5\u200b\u6807\u8bc6\u7b26\u200b\u5b58\u50a8\u200b\u5728\u200b\u60a8\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u7684\u200b\u4e00\u4e2a\u200bcookie\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u200b\u6211\u4eec\u200b\u80fd\u591f\u200b\u5728\u200b\u591a\u4e2a\u200b\u4f1a\u8bdd\u200b\u4e2d\u200b\u4ee5\u53ca\u200b\u60a8\u200b\u8fd4\u56de\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u65f6\u200b\u8bc6\u522b\u200b\u60a8\u200b\u3002   \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u6e05\u9664\u200b\u6d4f\u89c8\u5668\u200b\u7f13\u5b58\u200b\u968f\u65f6\u200b\u5220\u9664\u200b\u6b64\u200bcookie\u3002</li> <li>\u200b\u65e5\u5fd7\u200b\u548c\u200b\u4f7f\u7528\u200b\u6570\u636e\u200b\u3002   \u200b\u65e5\u5fd7\u200b\u548c\u200b\u4f7f\u7528\u200b\u6570\u636e\u200b\u662f\u200b\u4e0e\u200b\u670d\u52a1\u200b\u76f8\u5173\u200b\u7684\u200b\u3001\u200b\u8bca\u65ad\u200b\u3001\u200b\u4f7f\u7528\u200b\u548c\u200b\u6027\u80fd\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5f53\u200b\u60a8\u200b\u8bbf\u95ee\u200b\u6216\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u65f6\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u5668\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u6536\u96c6\u200b\uff0c\u200b\u5e76\u200b\u8bb0\u5f55\u200b\u5728\u200b\u65e5\u5fd7\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u3002   \u200b\u6839\u636e\u200b\u60a8\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b\u4e92\u52a8\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u65e5\u5fd7\u200b\u6570\u636e\u200b\u53ef\u80fd\u200b\u5305\u62ec\u200b\u60a8\u200b\u7684\u200bIP\u200b\u5730\u5740\u200b\u3001\u200b\u8bbe\u5907\u200b\u4fe1\u606f\u200b\u3001\u200b\u6d4f\u89c8\u5668\u200b\u7c7b\u578b\u200b\u548c\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u6709\u5173\u200b\u60a8\u200b\u5728\u200b\u670d\u52a1\u200b\u4e2d\u200b\u7684\u200b\u6d3b\u52a8\u200b\u7684\u200b\u4fe1\u606f\u200b\uff08\u200b\u5982\u200b\u4e0e\u200b\u60a8\u200b\u4f7f\u7528\u200b\u76f8\u5173\u200b\u7684\u200b\u65e5\u671f\u200b/\u200b\u65f6\u95f4\u200b\u6233\u200b\u3001\u200b\u6d4f\u89c8\u200b\u548c\u200b\u67e5\u770b\u200b\u7684\u200b\u9875\u9762\u200b\u548c\u200b\u6587\u4ef6\u200b\u3001\u200b\u641c\u7d22\u200b\u4ee5\u53ca\u200b\u60a8\u200b\u91c7\u53d6\u200b\u7684\u200b\u5176\u4ed6\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u4f8b\u5982\u200b\u60a8\u200b\u4f7f\u7528\u200b\u7684\u200b\u529f\u80fd\u200b\uff09\uff0c\u200b\u8bbe\u5907\u200b\u4e8b\u4ef6\u200b\u4fe1\u606f\u200b\uff08\u200b\u5982\u200b\u7cfb\u7edf\u6d3b\u52a8\u200b\u3001\u200b\u9519\u8bef\u62a5\u544a\u200b\uff08\u200b\u6709\u65f6\u200b\u79f0\u4e3a\u200b\u2019\u200b\u5d29\u6e83\u200b\u8f6c\u50a8\u200b\u2019\uff09\u200b\u548c\u200b\u786c\u4ef6\u200b\u8bbe\u7f6e\u200b\uff09\u3002</li> <li>\u200b\u8bbe\u5907\u200b\u6570\u636e\u200b\u3002   \u200b\u6211\u4eec\u200b\u6536\u96c6\u200b\u8bbe\u5907\u200b\u6570\u636e\u200b\uff0c\u200b\u5982\u200b\u60a8\u200b\u7528\u4e8e\u200b\u8bbf\u95ee\u200b\u670d\u52a1\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u3001\u200b\u7535\u8bdd\u200b\u3001\u200b\u5e73\u677f\u200b\u6216\u200b\u5176\u4ed6\u200b\u8bbe\u5907\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002   \u200b\u6839\u636e\u200b\u6240\u200b\u4f7f\u7528\u200b\u7684\u200b\u8bbe\u5907\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u8bbe\u5907\u200b\u6570\u636e\u200b\u53ef\u80fd\u200b\u5305\u62ec\u200b\u5982\u4e0b\u200b\u4fe1\u606f\u200b\uff1a\u200b\u60a8\u200b\u7684\u200bIP\u200b\u5730\u5740\u200b\uff08\u200b\u6216\u200b\u4ee3\u7406\u670d\u52a1\u5668\u200b\uff09\u3001\u200b\u8bbe\u5907\u200b\u548c\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u8bc6\u522b\u200b\u53f7\u200b\u3001\u200b\u4f4d\u7f6e\u200b\u3001\u200b\u6d4f\u89c8\u5668\u200b\u7c7b\u578b\u200b\u3001\u200b\u786c\u4ef6\u200b\u578b\u53f7\u200b\u3001\u200b\u4e92\u8054\u7f51\u200b\u670d\u52a1\u63d0\u4f9b\u5546\u200b\u548c\u200b/\u200b\u6216\u200b\u79fb\u52a8\u200b\u8fd0\u8425\u5546\u200b\u3001\u200b\u64cd\u4f5c\u7cfb\u7edf\u200b\u548c\u200b\u7cfb\u7edf\u914d\u7f6e\u200b\u4fe1\u606f\u200b\u3002</li> <li>\u200b\u4f4d\u7f6e\u200b\u6570\u636e\u200b\u3002   \u200b\u6211\u4eec\u200b\u6536\u96c6\u200b\u4f4d\u7f6e\u200b\u6570\u636e\u200b\uff0c\u200b\u5982\u200b\u60a8\u200b\u8bbe\u5907\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u662f\u200b\u7cbe\u786e\u200b\u7684\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4e0d\u200b\u7cbe\u786e\u200b\u7684\u200b\u3002   \u200b\u6211\u4eec\u200b\u6536\u96c6\u200b\u591a\u5c11\u200b\u4fe1\u606f\u200b\u53d6\u51b3\u4e8e\u200b\u60a8\u200b\u7528\u4e8e\u200b\u8bbf\u95ee\u200b\u670d\u52a1\u200b\u7684\u200b\u8bbe\u5907\u200b\u7684\u200b\u7c7b\u578b\u200b\u548c\u200b\u8bbe\u7f6e\u200b\u3002   \u200b\u4f8b\u5982\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f7f\u7528\u200bGPS\u200b\u548c\u200b\u5176\u4ed6\u200b\u6280\u672f\u200b\u6765\u200b\u6536\u96c6\u200b\u5730\u7406\u4f4d\u7f6e\u200b\u6570\u636e\u200b\uff0c\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\u60a8\u200b\u5f53\u524d\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff08\u200b\u57fa\u4e8e\u200b\u60a8\u200b\u7684\u200bIP\u200b\u5730\u5740\u200b\uff09\u3002   \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u4e0d\u8ba9\u200b\u6211\u4eec\u200b\u6536\u96c6\u200b\u6b64\u200b\u4fe1\u606f\u200b\uff0c\u200b\u65b9\u6cd5\u200b\u662f\u200b\u62d2\u7edd\u200b\u8bbf\u95ee\u4fe1\u606f\u200b\u6216\u200b\u5728\u200b\u60a8\u200b\u7684\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u7981\u7528\u200b\u4f4d\u7f6e\u200b\u8bbe\u7f6e\u200b\u3002</li> </ul>"},{"location":"zh/about/privacy/#_4","title":"\u6211\u4eec\u200b\u6536\u96c6\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u7c7b\u522b","text":"<p>\u200b\u8fc7\u53bb\u200b\u5341\u4e8c\u200b\uff0812\uff09\u200b\u4e2a\u200b\u6708\u200b\u5185\u200b\uff0c\u200b\u6211\u4eec\u200b\u6536\u96c6\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u7c7b\u522b\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff1a</p> \u200b\u7c7b\u522b\u200b \u200b\u793a\u4f8b\u200b \u200b\u5df2\u200b\u6536\u96c6\u200b A. \u200b\u6807\u8bc6\u7b26\u200b \u200b\u8054\u7cfb\u65b9\u5f0f\u200b\uff0c\u200b\u5982\u200b\u771f\u5b9e\u200b\u59d3\u540d\u200b\u3001\u200b\u522b\u540d\u200b\u3001\u200b\u90ae\u653f\u200b\u5730\u5740\u200b\u3001\u200b\u7535\u8bdd\u200b\u6216\u200b\u79fb\u52a8\u200b\u8054\u7cfb\u200b\u53f7\u7801\u200b\u3001\u200b\u72ec\u7279\u200b\u7684\u200b\u4e2a\u4eba\u200b\u6807\u8bc6\u7b26\u200b\u3001\u200b\u5728\u7ebf\u200b\u6807\u8bc6\u7b26\u200b\u3001\u200b\u4e92\u8054\u7f51\u534f\u8bae\u200b\u5730\u5740\u200b\u3001\u200b\u7535\u5b50\u90ae\u4ef6\u200b\u5730\u5740\u200b\u548c\u200b\u5e10\u6237\u200b\u540d\u79f0\u200b \u200b\u662f\u200b B. \u200b\u52a0\u5229\u798f\u5c3c\u4e9a\u200b\u5ba2\u6237\u200b\u8bb0\u5f55\u200b\u6cd5\u4e2d\u200b\u5b9a\u4e49\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b \u200b\u59d3\u540d\u200b\u3001\u200b\u8054\u7cfb\u200b\u4fe1\u606f\u200b\u3001\u200b\u6559\u80b2\u200b\u3001\u200b\u5c31\u4e1a\u200b\u3001\u200b\u5c31\u4e1a\u200b\u5386\u53f2\u200b\u548c\u200b\u8d22\u52a1\u200b\u4fe1\u606f\u200b \u200b\u5426\u200b C. \u200b\u5dde\u200b\u6216\u200b\u8054\u90a6\u200b\u6cd5\u5f8b\u200b\u4e0b\u200b\u7684\u200b\u53d7\u200b\u4fdd\u62a4\u200b\u5206\u7c7b\u200b\u7279\u5f81\u200b \u200b\u6027\u522b\u200b\u3001\u200b\u5e74\u9f84\u200b\u3001\u200b\u51fa\u751f\u65e5\u671f\u200b\u3001\u200b\u79cd\u65cf\u200b\u548c\u200b\u6c11\u65cf\u200b\u3001\u200b\u56fd\u7c4d\u200b\u3001\u200b\u5a5a\u59fb\u72b6\u51b5\u200b\u548c\u200b\u5176\u4ed6\u200b\u4eba\u53e3\u200b\u7edf\u8ba1\u6570\u636e\u200b \u200b\u5426\u200b D. \u200b\u5546\u4e1a\u4fe1\u606f\u200b \u200b\u4ea4\u6613\u200b\u4fe1\u606f\u200b\u3001\u200b\u8d2d\u4e70\u200b\u5386\u53f2\u200b\u3001\u200b\u8d22\u52a1\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\u548c\u200b\u652f\u4ed8\u200b\u4fe1\u606f\u200b \u200b\u5426\u200b E. \u200b\u751f\u7269\u200b\u8bc6\u522b\u200b\u4fe1\u606f\u200b \u200b\u6307\u7eb9\u200b\u548c\u200b\u58f0\u7eb9\u200b \u200b\u5426\u200b F. \u200b\u4e92\u8054\u7f51\u200b\u6216\u200b\u5176\u4ed6\u200b\u7c7b\u4f3c\u200b\u7f51\u7edc\u200b\u6d3b\u52a8\u200b \u200b\u6d4f\u89c8\u200b\u5386\u53f2\u200b\u3001\u200b\u641c\u7d22\u200b\u5386\u53f2\u200b\u3001\u200b\u5728\u7ebf\u200b\u884c\u4e3a\u200b\u3001\u200b\u5174\u8da3\u200b\u6570\u636e\u200b\u548c\u200b\u4e0e\u200b\u6211\u4eec\u200b\u548c\u200b\u5176\u4ed6\u200b\u7f51\u7ad9\u200b\u3001\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u3001\u200b\u7cfb\u7edf\u200b\u548c\u200b\u5e7f\u544a\u200b\u7684\u200b\u4e92\u52a8\u200b \u200b\u662f\u200b G. \u200b\u5730\u7406\u4f4d\u7f6e\u200b\u6570\u636e\u200b \u200b\u8bbe\u5907\u200b\u4f4d\u7f6e\u200b \u200b\u662f\u200b H. \u200b\u97f3\u9891\u200b\u3001\u200b\u7535\u5b50\u200b\u3001\u200b\u611f\u89c9\u200b\u6216\u200b\u7c7b\u4f3c\u200b\u4fe1\u606f\u200b \u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4e1a\u52a1\u200b\u6d3b\u52a8\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u56fe\u50cf\u200b\u548c\u200b\u97f3\u9891\u200b\u3001\u200b\u89c6\u9891\u200b\u6216\u200b\u901a\u8bdd\u5f55\u97f3\u200b \u200b\u5426\u200b I. \u200b\u4e0e\u200b\u804c\u4e1a\u200b\u76f8\u5173\u200b\u7684\u200b\u4fe1\u606f\u200b \u200b\u4e3a\u4e86\u200b\u5728\u200b\u4e1a\u52a1\u200b\u5c42\u9762\u200b\u63d0\u4f9b\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u800c\u200b\u6536\u96c6\u200b\u7684\u200b\u5546\u4e1a\u200b\u8054\u7cfb\u200b\u4fe1\u606f\u200b\u6216\u200b\u804c\u52a1\u200b\u540d\u79f0\u200b\u3001\u200b\u5de5\u4f5c\u200b\u5386\u53f2\u200b\u548c\u200b\u804c\u4e1a\u8d44\u683c\u200b \u200b\u5426\u200b J. \u200b\u6559\u80b2\u200b\u4fe1\u606f\u200b \u200b\u5b66\u751f\u200b\u8bb0\u5f55\u200b\u548c\u200b\u76ee\u5f55\u200b\u4fe1\u606f\u200b \u200b\u5426\u200b K. \u200b\u4ece\u200b\u6536\u96c6\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u4e2d\u200b\u63a8\u65ad\u51fa\u200b\u7684\u200b\u63a8\u8bba\u200b \u200b\u4ece\u200b\u4e0a\u8ff0\u200b\u4efb\u4f55\u200b\u6536\u96c6\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u4e2d\u200b\u63a8\u65ad\u51fa\u200b\u7684\u200b\u7528\u4e8e\u200b\u521b\u5efa\u200b\u4e2a\u4eba\u200b\u504f\u597d\u200b\u548c\u200b\u7279\u5f81\u200b\u7684\u200b\u6982\u51b5\u200b\u6216\u200b\u6458\u8981\u200b \u200b\u662f\u200b L. \u200b\u654f\u611f\u200b\u4e2a\u4eba\u4fe1\u606f\u200b \u200b\u5426\u200b <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u80fd\u200b\u5728\u200b\u60a8\u200b\u4e0e\u200b\u6211\u4eec\u200b\u4eb2\u81ea\u200b\u3001\u200b\u5728\u7ebf\u200b\u6216\u200b\u901a\u8fc7\u200b\u7535\u8bdd\u200b\u6216\u200b\u90ae\u4ef6\u200b\u4e0e\u200b\u6211\u4eec\u200b\u4e92\u52a8\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u6536\u96c6\u200b\u5176\u4ed6\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff0c\u200b\u5305\u62ec\u200b\uff1a</p> <ul> <li>\u200b\u901a\u8fc7\u200b\u6211\u4eec\u200b\u7684\u200b\u5ba2\u6237\u200b\u652f\u6301\u200b\u6e20\u9053\u200b\u83b7\u5f97\u200b\u5e2e\u52a9\u200b\uff1b</li> <li>\u200b\u53c2\u4e0e\u200b\u5ba2\u6237\u200b\u8c03\u67e5\u200b\u6216\u200b\u7ade\u8d5b\u200b\uff1b\u200b\u4ee5\u53ca\u200b</li> <li>\u200b\u4fc3\u8fdb\u200b\u6211\u4eec\u200b\u670d\u52a1\u200b\u7684\u200b\u4ea4\u4ed8\u200b\u5e76\u200b\u56de\u5e94\u200b\u60a8\u200b\u7684\u200b\u67e5\u8be2\u200b\u3002</li> </ul> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u5728\u200b\u4ee5\u4e0b\u200b\u671f\u9650\u5185\u200b\u4f7f\u7528\u200b\u548c\u200b\u4fdd\u7559\u200b\u6240\u200b\u6536\u96c6\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u4e3a\u200b\u60a8\u200b\u63d0\u4f9b\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\uff0c\u200b\u5e76\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u9075\u5b88\u200b\u6211\u4eec\u200b\u7684\u200b\u6cd5\u5f8b\u4e49\u52a1\u200b\u3001\u200b\u89e3\u51b3\u200b\u4e89\u8bae\u200b\u548c\u200b\u6267\u884c\u200b\u6211\u4eec\u200b\u7684\u200b\u534f\u8bae\u200b\uff1a</p> <ul> <li>A \u200b\u7c7b\u200b\uff1a\u200b\u65e0\u9650\u671f\u200b</li> <li>F \u200b\u7c7b\u200b\uff1a\u200b\u65e0\u9650\u671f\u200b</li> <li>G \u200b\u7c7b\u200b\uff1a\u200b\u65e0\u9650\u671f\u200b</li> <li>K \u200b\u7c7b\u200b\uff1a\u200b\u65e0\u9650\u671f\u200b</li> </ul>"},{"location":"zh/about/privacy/#2","title":"2. \u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\uff1f","text":"<p>\u200b\u7b80\u800c\u8a00\u4e4b\u200b</p> <p>\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u4ee5\u200b\u63d0\u4f9b\u200b\u3001\u200b\u6539\u5584\u200b\u548c\u200b\u7ba1\u7406\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\uff0c\u200b\u4e0e\u200b\u60a8\u200b\u6c9f\u901a\u200b\uff0c\u200b\u8fdb\u884c\u200b\u5b89\u5168\u200b\u548c\u200b\u9632\u200b\u6b3a\u8bc8\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u9075\u5b88\u200b\u6cd5\u5f8b\u200b\u3002 \u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u5728\u200b\u5f97\u5230\u200b\u60a8\u200b\u7684\u200b\u540c\u610f\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u51fa\u4e8e\u200b\u5176\u4ed6\u200b\u76ee\u7684\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u51fa\u4e8e\u200b\u591a\u79cd\u200b\u539f\u56e0\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff0c\u200b\u8fd9\u200b\u53d6\u51b3\u4e8e\u200b\u60a8\u200b\u5982\u4f55\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u4e92\u52a8\u200b\uff0c\u200b\u5305\u62ec\u200b\uff1a</p> <ul> <li>\u200b\u4fdd\u62a4\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u3002   \u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u4f5c\u4e3a\u200b\u6211\u4eec\u200b\u4fdd\u6301\u200b\u670d\u52a1\u200b\u5b89\u5168\u200b\u7684\u200b\u52aa\u529b\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\uff0c\u200b\u5305\u62ec\u200b\u76d1\u63a7\u200b\u548c\u200b\u9884\u9632\u200b\u6b3a\u8bc8\u200b\u3002</li> <li>\u200b\u8bc6\u522b\u200b\u7528\u6237\u200b\u8d8b\u52bf\u200b\u3002   \u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5904\u7406\u200b\u6709\u5173\u200b\u60a8\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4ee5\u200b\u66f4\u597d\u200b\u5730\u200b\u4e86\u89e3\u200b\u5b83\u4eec\u200b\u7684\u200b\u4f7f\u7528\u200b\u60c5\u51b5\u200b\uff0c\u200b\u4ece\u800c\u200b\u6539\u8fdb\u200b\u5b83\u4eec\u200b\u3002</li> <li>\u200b\u4fdd\u5b58\u200b\u6216\u200b\u4fdd\u62a4\u200b\u4e2a\u4eba\u200b\u7684\u200b\u91cd\u8981\u200b\u5229\u76ca\u200b\u3002   \u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5728\u200b\u5fc5\u8981\u200b\u65f6\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4ee5\u200b\u4fdd\u5b58\u200b\u6216\u200b\u4fdd\u62a4\u200b\u4e2a\u4eba\u200b\u7684\u200b\u91cd\u8981\u200b\u5229\u76ca\u200b\uff0c\u200b\u4f8b\u5982\u200b\u4e3a\u4e86\u200b\u9632\u6b62\u200b\u4f24\u5bb3\u200b\u3002</li> </ul>"},{"location":"zh/about/privacy/#3","title":"3. \u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u6709\u200b\u4ec0\u4e48\u200b\u6cd5\u5f8b\u4f9d\u636e\u200b\uff1f","text":"<p>\u200b\u7b80\u800c\u8a00\u4e4b\u200b</p> <p>\u200b\u6211\u4eec\u200b\u53ea\u6709\u200b\u5728\u200b\u6211\u4eec\u200b\u8ba4\u4e3a\u200b\u5fc5\u8981\u200b\u4e14\u200b\u6709\u200b\u6709\u6548\u200b\u7684\u200b\u6cd5\u5f8b\u200b\u7406\u7531\u200b\uff08\u200b\u5373\u200b\u6cd5\u5f8b\u4f9d\u636e\u200b\uff09\u200b\u65f6\u624d\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff0c\u200b\u5982\u200b\u4e0e\u200b\u60a8\u200b\u7684\u200b\u540c\u610f\u200b\u3001\u200b\u9075\u5b88\u200b\u6cd5\u5f8b\u200b\u3001\u200b\u63d0\u4f9b\u200b\u670d\u52a1\u200b\u7ed9\u200b\u60a8\u200b\u8fdb\u5165\u200b\u6216\u200b\u5c65\u884c\u200b\u6211\u4eec\u200b\u7684\u200b\u5408\u540c\u200b\u4e49\u52a1\u200b\u3001\u200b\u4fdd\u62a4\u200b\u60a8\u200b\u7684\u200b\u6743\u5229\u200b\u6216\u200b\u6ee1\u8db3\u200b\u6211\u4eec\u200b\u5408\u6cd5\u200b\u7684\u200b\u4e1a\u52a1\u200b\u5229\u76ca\u200b\u3002</p> <p>\u200b\u6b27\u76df\u200b\u901a\u7528\u200b\u6570\u636e\u4fdd\u62a4\u200b\u6761\u4f8b\u200b\uff08GDPR\uff09\u200b\u548c\u200b\u82f1\u56fd\u200bGDPR\u200b\u8981\u6c42\u200b\u6211\u4eec\u200b\u89e3\u91ca\u200b\u6211\u4eec\u200b\u4f9d\u9760\u200b\u7684\u200b\u6709\u6548\u200b\u6cd5\u5f8b\u4f9d\u636e\u200b\u4ee5\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u3002 \u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f9d\u8d56\u200b\u4ee5\u4e0b\u200b\u6cd5\u5f8b\u4f9d\u636e\u200b\u6765\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff1a</p> <ul> <li>\u200b\u540c\u610f\u200b\u3002   \u200b\u5982\u679c\u200b\u60a8\u200b\u5df2\u200b\u7ed9\u200b\u6211\u4eec\u200b\u660e\u786e\u200b\u540c\u610f\u200b\u4f7f\u7528\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u7528\u4e8e\u200b\u67d0\u4e2a\u200b\u7279\u5b9a\u200b\u76ee\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u3002   \u200b\u60a8\u200b\u6709\u6743\u200b\u968f\u65f6\u200b\u64a4\u56de\u200b\u60a8\u200b\u7684\u200b\u540c\u610f\u200b\u3002   \u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u64a4\u56de\u200b\u60a8\u200b\u7684\u200b\u540c\u610f\u200b\u3002</li> <li>\u200b\u5408\u6cd5\u5229\u76ca\u200b\u3002   \u200b\u5f53\u200b\u6211\u4eec\u200b\u8ba4\u4e3a\u200b\u51fa\u4e8e\u200b\u6211\u4eec\u200b\u5408\u6cd5\u200b\u7684\u200b\u4e1a\u52a1\u200b\u5229\u76ca\u200b\u6765\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u662f\u200b\u5408\u7406\u200b\u5fc5\u8981\u200b\u7684\u200b\uff0c\u200b\u5e76\u4e14\u200b\u8fd9\u4e9b\u200b\u5229\u76ca\u200b\u4e0d\u200b\u8d85\u8fc7\u200b\u60a8\u200b\u7684\u200b\u5229\u76ca\u200b\u548c\u200b\u57fa\u672c\u6743\u5229\u200b\u4e0e\u200b\u81ea\u7531\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002   \u200b\u4f8b\u5982\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff0c\u200b\u7528\u4e8e\u200b\uff1a</li> <li>\u200b\u5206\u6790\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u5982\u4f55\u200b\u88ab\u200b\u4f7f\u7528\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u6539\u8fdb\u200b\u5b83\u4eec\u200b\u4ee5\u200b\u5438\u5f15\u200b\u548c\u200b\u4fdd\u7559\u200b\u7528\u6237\u200b</li> <li>\u200b\u8bca\u65ad\u200b\u95ee\u9898\u200b\u548c\u200b/\u200b\u6216\u200b\u9884\u9632\u200b\u6b3a\u8bc8\u200b\u6d3b\u52a8\u200b</li> <li>\u200b\u6cd5\u5f8b\u4e49\u52a1\u200b\u3002   \u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5728\u200b\u6211\u4eec\u200b\u8ba4\u4e3a\u200b\u5fc5\u987b\u200b\u9075\u5b88\u200b\u6211\u4eec\u200b\u7684\u200b\u6cd5\u5f8b\u4e49\u52a1\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4f8b\u5982\u200b\u4e0e\u200b\u6267\u6cd5\u200b\u673a\u6784\u200b\u6216\u200b\u76d1\u7ba1\u200b\u673a\u6784\u200b\u5408\u4f5c\u200b\u3001\u200b\u884c\u4f7f\u200b\u6216\u200b\u634d\u536b\u200b\u6211\u4eec\u200b\u7684\u200b\u6cd5\u5f8b\u200b\u6743\u5229\u200b\uff0c\u200b\u6216\u200b\u5728\u200b\u6211\u4eec\u200b\u53c2\u4e0e\u200b\u7684\u200b\u8bc9\u8bbc\u200b\u4e2d\u200b\u62ab\u9732\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u4f5c\u4e3a\u200b\u8bc1\u636e\u200b\u3002</li> <li>\u200b\u91cd\u8981\u200b\u5229\u76ca\u200b\u3002   \u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5728\u200b\u6211\u4eec\u200b\u8ba4\u4e3a\u200b\u5fc5\u987b\u200b\u4fdd\u62a4\u200b\u60a8\u200b\u6216\u200b\u7b2c\u4e09\u65b9\u200b\u7684\u200b\u91cd\u8981\u200b\u5229\u76ca\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4f8b\u5982\u200b\u6d89\u53ca\u200b\u6f5c\u5728\u200b\u5a01\u80c1\u200b\u4efb\u4f55\u4eba\u200b\u7684\u200b\u5b89\u5168\u200b\u7684\u200b\u60c5\u51b5\u200b\u3002</li> </ul> <p>\u200b\u5728\u200b\u52a0\u62ff\u5927\u200b\u5904\u7406\u200b\u7684\u200b\u540c\u610f\u200b</p> <p>\u200b\u5982\u679c\u200b\u60a8\u200b\u4f4d\u4e8e\u200b\u52a0\u62ff\u5927\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u5728\u200b\u9002\u7528\u6cd5\u5f8b\u200b\u4e0b\u200b\u5728\u200b\u67d0\u4e9b\u200b\u7279\u6b8a\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u65e0\u9700\u200b\u60a8\u200b\u7684\u200b\u540c\u610f\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5408\u6cd5\u200b\u5730\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5305\u62ec\u200b\u4f8b\u5982\u200b\uff1a</p> <ul> <li>\u200b\u5982\u679c\u200b\u6536\u96c6\u200b\u660e\u663e\u200b\u7b26\u5408\u200b\u4e2a\u4eba\u200b\u7684\u200b\u5229\u76ca\u200b\u4e14\u200b\u65e0\u6cd5\u200b\u53ca\u65f6\u200b\u83b7\u5f97\u200b\u540c\u610f\u200b</li> <li>\u200b\u7528\u4e8e\u200b\u8c03\u67e5\u200b\u548c\u200b\u6b3a\u8bc8\u200b\u68c0\u6d4b\u200b\u4e0e\u200b\u9884\u9632\u200b</li> <li>\u200b\u7528\u4e8e\u200b\u5546\u4e1a\u200b\u4ea4\u6613\u200b\uff0c\u200b\u524d\u63d0\u200b\u662f\u200b\u6ee1\u8db3\u200b\u67d0\u4e9b\u200b\u6761\u4ef6\u200b</li> <li>\u200b\u5982\u679c\u200b\u4fe1\u606f\u200b\u5305\u542b\u200b\u5728\u200b\u8bc1\u4eba\u200b\u58f0\u660e\u200b\u4e2d\u200b\uff0c\u200b\u4e14\u200b\u6536\u96c6\u200b\u5bf9\u4e8e\u200b\u8bc4\u4f30\u200b\u3001\u200b\u5904\u7406\u200b\u6216\u200b\u89e3\u51b3\u200b\u4fdd\u9669\u200b\u7d22\u8d54\u200b\u662f\u200b\u5fc5\u8981\u200b\u7684\u200b</li> <li>\u200b\u7528\u4e8e\u200b\u8bc6\u522b\u200b\u53d7\u4f24\u200b\u3001\u200b\u751f\u75c5\u200b\u6216\u200b\u5df2\u6545\u200b\u4eba\u58eb\u200b\u5e76\u200b\u4e0e\u200b\u8fd1\u4eb2\u200b\u6c9f\u901a\u200b</li> <li>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u6709\u200b\u5408\u7406\u200b\u7684\u200b\u7406\u7531\u200b\u76f8\u4fe1\u200b\u67d0\u4e2a\u200b\u4eba\u200b\u5df2\u7ecf\u200b\u3001\u200b\u6b63\u5728\u200b\u6216\u200b\u53ef\u80fd\u200b\u6210\u4e3a\u200b\u91d1\u878d\u200b\u6ee5\u7528\u200b\u7684\u200b\u53d7\u5bb3\u8005\u200b</li> <li>\u200b\u5982\u679c\u200b\u5408\u7406\u200b\u9884\u671f\u200b\u901a\u8fc7\u200b\u5f81\u5f97\u200b\u540c\u610f\u200b\u4ee5\u200b\u6536\u96c6\u200b\u548c\u200b\u4f7f\u7528\u200b\u4fe1\u606f\u200b\u4f1a\u200b\u635f\u5bb3\u200b\u4fe1\u606f\u200b\u7684\u200b\u53ef\u7528\u6027\u200b\u6216\u200b\u51c6\u786e\u6027\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6536\u96c6\u200b\u5bf9\u4e8e\u200b\u8c03\u67e5\u200b\u8fdd\u53cd\u200b\u534f\u8bae\u200b\u6216\u200b\u8fdd\u53cd\u200b\u52a0\u62ff\u5927\u200b\u6216\u7701\u200b\u6cd5\u5f8b\u200b\u7684\u200b\u76ee\u7684\u200b\u662f\u200b\u5408\u7406\u200b\u7684\u200b</li> <li>\u200b\u5982\u679c\u200b\u62ab\u9732\u200b\u662f\u200b\u4e3a\u4e86\u200b\u9075\u5b88\u200b\u4f20\u7968\u200b\u3001\u200b\u641c\u67e5\u200b\u4ee4\u200b\u3001\u200b\u6cd5\u9662\u200b\u547d\u4ee4\u200b\u6216\u200b\u4e0e\u200b\u8bb0\u5f55\u200b\u751f\u4ea7\u200b\u76f8\u5173\u200b\u7684\u200b\u6cd5\u9662\u200b\u89c4\u5219\u200b</li> <li>\u200b\u5982\u679c\u200b\u4fe1\u606f\u200b\u662f\u200b\u7531\u200b\u4e2a\u4eba\u200b\u5728\u200b\u5176\u200b\u5c31\u4e1a\u200b\u3001\u200b\u4e1a\u52a1\u200b\u6216\u200b\u4e13\u4e1a\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4ea7\u751f\u200b\u7684\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6536\u96c6\u200b\u4e0e\u200b\u4fe1\u606f\u200b\u4ea7\u751f\u200b\u7684\u200b\u76ee\u7684\u200b\u4e00\u81f4\u200b</li> <li>\u200b\u5982\u679c\u200b\u6536\u96c6\u200b\u4ec5\u200b\u7528\u4e8e\u200b\u65b0\u95fb\u200b\u3001\u200b\u827a\u672f\u200b\u6216\u200b\u6587\u5b66\u200b\u76ee\u7684\u200b</li> <li>\u200b\u5982\u679c\u200b\u4fe1\u606f\u200b\u662f\u200b\u516c\u5f00\u200b\u53ef\u7528\u200b\u7684\u200b\uff0c\u200b\u5e76\u4e14\u200b\u901a\u8fc7\u200b\u89c4\u5b9a\u200b\u6307\u5b9a\u200b</li> </ul>"},{"location":"zh/about/privacy/#4","title":"4. \u200b\u6211\u4eec\u200b\u4f55\u65f6\u200b\u4ee5\u53ca\u200b\u4e0e\u200b\u8c01\u200b\u5171\u4eab\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff1f","text":"<p>\u200b\u7b80\u800c\u8a00\u4e4b\u200b</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u5728\u200b\u672c\u8282\u200b\u63cf\u8ff0\u200b\u7684\u200b\u7279\u5b9a\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u548c\u200b/\u200b\u6216\u200b\u4e0e\u200b\u4ee5\u4e0b\u200b\u7b2c\u4e09\u65b9\u200b\u5171\u4eab\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5c06\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u7528\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u4e1a\u52a1\u200b\u76ee\u7684\u200b\uff0c\u200b\u5982\u200b\u8fdb\u884c\u200b\u5185\u90e8\u200b\u7814\u7a76\u200b\u4ee5\u200b\u8fdb\u884c\u200b\u6280\u672f\u5f00\u53d1\u200b\u548c\u200b\u5c55\u793a\u200b\u3002 \u200b\u8fd9\u200b\u4e0d\u200b\u88ab\u200b\u89c6\u4e3a\u200b\u201c\u200b\u51fa\u552e\u200b\u201d\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u4f9b\u5e94\u5546\u200b\u3001\u200b\u987e\u95ee\u200b\u548c\u200b\u5176\u4ed6\u200b\u7b2c\u4e09\u65b9\u200b\u670d\u52a1\u63d0\u4f9b\u5546\u200b\u3002 \u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4e0e\u200b\u4e3a\u200b\u6211\u4eec\u200b\u670d\u52a1\u200b\u6216\u200b\u4ee3\u8868\u200b\u6211\u4eec\u200b\u5de5\u4f5c\u200b\u5e76\u200b\u9700\u8981\u200b\u8bbf\u95ee\u200b\u6b64\u7c7b\u200b\u4fe1\u606f\u200b\u4ee5\u200b\u6267\u884c\u200b\u8be5\u200b\u5de5\u4f5c\u200b\u7684\u200b\u7b2c\u4e09\u65b9\u200b\u4f9b\u5e94\u5546\u200b\u3001\u200b\u670d\u52a1\u63d0\u4f9b\u5546\u200b\u3001\u200b\u627f\u5305\u5546\u200b\u6216\u200b\u4ee3\u7406\u200b\uff08\u201c\u200b\u7b2c\u4e09\u65b9\u200b\u201d\uff09\u200b\u5171\u4eab\u200b\u60a8\u200b\u7684\u200b\u6570\u636e\u200b\u3002 \u200b\u6211\u4eec\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b\u7b2c\u4e09\u65b9\u200b\u7b7e\u8ba2\u200b\u4e86\u200b\u5408\u540c\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5408\u540c\u200b\u65e8\u5728\u200b\u5e2e\u52a9\u200b\u4fdd\u62a4\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u3002 \u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u4ed6\u4eec\u200b\u4e0d\u80fd\u200b\u5728\u200b\u672a\u7ecf\u200b\u6211\u4eec\u200b\u6307\u793a\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u505a\u200b\u4efb\u4f55\u200b\u4e8b\u60c5\u200b\u3002 \u200b\u4ed6\u4eec\u200b\u4e5f\u200b\u4e0d\u4f1a\u200b\u4e0e\u200b\u6211\u4eec\u200b\u4ee5\u5916\u200b\u7684\u200b\u4efb\u4f55\u200b\u7ec4\u7ec7\u200b\u5171\u4eab\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u3002 \u200b\u4ed6\u4eec\u200b\u8fd8\u200b\u627f\u8bfa\u200b\u4fdd\u62a4\u200b\u4ed6\u4eec\u200b\u4ee3\u8868\u200b\u6211\u4eec\u200b\u6301\u6709\u200b\u7684\u200b\u6570\u636e\u200b\u5e76\u200b\u6309\u7167\u200b\u6211\u4eec\u200b\u7684\u200b\u6307\u793a\u200b\u4fdd\u7559\u200b\u8be5\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u5171\u4eab\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u7684\u200b\u7b2c\u4e09\u65b9\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u5e7f\u544a\u200b\u3001\u200b\u76f4\u9500\u200b\u548c\u200b\u6f5c\u5728\u200b\u5ba2\u6237\u200b\u751f\u6210\u200b</li> <li>Google AdSense</li> <li>\u200b\u4e91\u200b\u8ba1\u7b97\u200b\u670d\u52a1\u200b</li> <li>Microsoft Azure</li> <li>Amazon Web Services (AWS)</li> <li>Google Cloud Platform (GCP)</li> <li>\u200b\u901a\u4fe1\u200b\u548c\u200b\u5185\u5bb9\u200b\u4ea4\u4ed8\u200b\u7f51\u7edc\u200b (CDN) \u200b\u670d\u52a1\u200b</li> <li>Cloudflare</li> <li>\u200b\u5185\u5bb9\u200b\u4f18\u5316\u200b</li> <li>Google\u200b\u7ad9\u70b9\u200b\u641c\u7d22\u200b</li> <li>Google\u200b\u5b57\u4f53\u200b</li> <li>\u200b\u529f\u80fd\u200b\u548c\u200b\u57fa\u7840\u8bbe\u65bd\u200b\u4f18\u5316\u200b   GitHub\u200b\u9875\u9762\u200b</li> <li>\u200b\u7528\u6237\u200b\u8bc4\u8bba\u200b\u548c\u200b\u8bba\u575b\u200b</li> <li>Disqus</li> <li>GitHub\u200b\u8bae\u9898\u200b</li> <li>GitHub\u200b\u8ba8\u8bba\u200b</li> <li>\u200b\u7f51\u7edc\u200b\u548c\u200b\u79fb\u52a8\u200b\u5206\u6790\u200b</li> <li>Google Analytics</li> </ul> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u5728\u200b\u4ee5\u4e0b\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5171\u4eab\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff1a</p> <ul> <li>\u200b\u4e1a\u52a1\u200b\u8f6c\u79fb\u200b\u3002   \u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u5728\u200b\u8fdb\u884c\u200b\u4efb\u4f55\u200b\u5e76\u8d2d\u200b\u3001\u200b\u51fa\u552e\u200b\u516c\u53f8\u200b\u8d44\u4ea7\u200b\u3001\u200b\u878d\u8d44\u200b\u6216\u200b\u6536\u8d2d\u200b\u6211\u4eec\u200b\u5168\u90e8\u200b\u6216\u200b\u90e8\u5206\u200b\u4e1a\u52a1\u200b\u7684\u200b\u8c08\u5224\u200b\u4e2d\u200b\u5171\u4eab\u200b\u6216\u200b\u8f6c\u8ba9\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</li> </ul> <p>\u200b\u8fc7\u53bb\u200b\u5341\u4e8c\u200b\uff0812\uff09\u200b\u4e2a\u200b\u6708\u200b\u6211\u4eec\u200b\u51fa\u4e8e\u200b\u4e1a\u52a1\u200b\u76ee\u7684\u200b\u62ab\u9732\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u7c7b\u522b\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff1a</p> <p>\u200b\u65e0\u200b</p> <p>\u200b\u8fc7\u53bb\u200b\u5341\u4e8c\u200b\uff0812\uff09\u200b\u4e2a\u200b\u6708\u200b\u6211\u4eec\u200b\u51fa\u552e\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u7684\u200b\u7b2c\u4e09\u65b9\u200b\u7c7b\u522b\u200b\uff1a</p> <p>\u200b\u65e0\u200b</p> <p>\u200b\u8fc7\u53bb\u200b\u5341\u4e8c\u200b\uff0812\uff09\u200b\u4e2a\u200b\u6708\u200b\u6211\u4eec\u200b\u4e0e\u200b\u4e4b\u200b\u5171\u4eab\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u7684\u200b\u7b2c\u4e09\u65b9\u200b\u7c7b\u522b\u200b\uff1a</p> <ul> <li>\u200b\u5e7f\u544a\u200b\u3001\u200b\u76f4\u9500\u200b\u548c\u200b\u6f5c\u5728\u200b\u5ba2\u6237\u200b\u751f\u6210\u200b<ul> <li>Google AdSense</li> </ul> </li> <li>\u200b\u7f51\u7edc\u200b\u548c\u200b\u79fb\u52a8\u200b\u5206\u6790\u200b<ul> <li>Google Analytics</li> </ul> </li> </ul>"},{"location":"zh/about/privacy/#5-cookies","title":"5. \u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u4f7f\u7528\u200bcookies\u200b\u548c\u200b\u5176\u4ed6\u200b\u8ddf\u8e2a\u200b\u6280\u672f\u200b\uff1f","text":"<p>\u200b\u7b80\u800c\u8a00\u4e4b\u200b</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f7f\u7528\u200bcookies\u200b\u548c\u200b\u5176\u4ed6\u200b\u8ddf\u8e2a\u200b\u6280\u672f\u200b\u6765\u200b\u6536\u96c6\u200b\u548c\u200b\u5b58\u50a8\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5141\u8bb8\u200b\u7b2c\u4e09\u65b9\u200b\u548c\u200b\u670d\u52a1\u63d0\u4f9b\u5546\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u5728\u7ebf\u200b\u8ddf\u8e2a\u200b\u6280\u672f\u200b\u7528\u4e8e\u200b\u5206\u6790\u200b\u548c\u200b\u5e7f\u544a\u200b\uff0c\u200b\u5305\u62ec\u200b\u5e2e\u52a9\u200b\u7ba1\u7406\u200b\u548c\u200b\u5c55\u793a\u200b\u5e7f\u544a\u200b\uff0c\u200b\u6839\u636e\u200b\u60a8\u200b\u7684\u200b\u5174\u8da3\u200b\u5b9a\u5236\u200b\u5e7f\u544a\u200b\uff0c\u200b\u6216\u200b\u53d1\u9001\u200b\u9057\u5f03\u200b\u8d2d\u7269\u8f66\u200b\u63d0\u9192\u200b\uff08\u200b\u53d6\u51b3\u4e8e\u200b\u60a8\u200b\u7684\u200b\u6c9f\u901a\u200b\u504f\u597d\u200b\uff09\u3002 \u200b\u8fd9\u4e9b\u200b\u7b2c\u4e09\u65b9\u200b\u548c\u200b\u670d\u52a1\u63d0\u4f9b\u5546\u200b\u4f7f\u7528\u200b\u4ed6\u4eec\u200b\u7684\u200b\u6280\u672f\u200b\u4e3a\u200b\u60a8\u200b\u63d0\u4f9b\u200b\u5b9a\u5236\u200b\u7684\u200b\u4ea7\u54c1\u200b\u548c\u200b\u670d\u52a1\u200b\u5e7f\u544a\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5e7f\u544a\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u670d\u52a1\u200b\u6216\u200b\u5176\u4ed6\u200b\u7f51\u7ad9\u200b\u4e0a\u200b\u3002</p> <p>\u200b\u5728\u200b\u9002\u7528\u200b\u7684\u200b\u7f8e\u56fd\u200b\u5dde\u200b\u6cd5\u5f8b\u200b\u4e0b\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5728\u7ebf\u200b\u8ddf\u8e2a\u200b\u6280\u672f\u200b\u88ab\u200b\u89c6\u4e3a\u200b\u201c\u200b\u9500\u552e\u200b\u201d/\u201c\u200b\u5206\u4eab\u200b\u201d\uff08\u200b\u5305\u62ec\u200b\u76ee\u6807\u200b\u5e7f\u544a\u200b\uff0c\u200b\u6839\u636e\u200b\u9002\u7528\u6cd5\u5f8b\u200b\u5b9a\u4e49\u200b\uff09\u200b\u7684\u200b\u7a0b\u5ea6\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u70b9\u51fb\u200b\u672c\u9875\u200b\u9876\u90e8\u200b\u6216\u200b\u4e0b\u9762\u200b\u7684\u200b\u6309\u94ae\u200b\u6765\u200b\u9009\u62e9\u200b\u9000\u51fa\u200b\u8fd9\u4e9b\u200b\u5728\u7ebf\u200b\u8ddf\u8e2a\u200b\u6280\u672f\u200b\uff1a</p> <p>\u200b\u9690\u79c1\u200b\u63a7\u5236\u200b</p>"},{"location":"zh/about/privacy/#google-analytics","title":"Google Analytics","text":"<p>\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4e0e\u200bGoogle Analytics\u200b\u5171\u4eab\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4ee5\u200b\u8ddf\u8e2a\u200b\u548c\u200b\u5206\u6790\u200b\u670d\u52a1\u200b\u7684\u200b\u4f7f\u7528\u200b\u60c5\u51b5\u200b\u3002 \u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f7f\u7528\u200b\u7684\u200bGoogle Analytics\u200b\u5e7f\u544a\u200b\u529f\u80fd\u200b\u5305\u62ec\u200b\uff1a Google Analytics\u200b\u7684\u200b\u518d\u200b\u8425\u9500\u200b\u3001Google Display Network\u200b\u5370\u8c61\u200b\u62a5\u544a\u200b\u548c\u200bGoogle Analytics\u200b\u4eba\u53e3\u7edf\u8ba1\u200b\u548c\u200b\u5174\u8da3\u200b\u62a5\u544a\u200b\u3002 \u200b\u8981\u200b\u9009\u62e9\u200b\u9000\u51fa\u200b\u5728\u200b\u670d\u52a1\u200b\u4e2d\u200b\u901a\u8fc7\u200bGoogle Analytics\u200b\u8ddf\u8e2a\u200b\u60a8\u200b\uff0c\u200b\u8bf7\u200b\u8bbf\u95ee\u200bhttps://tools.google.com/dlpage/gaoptout\u3002 \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5e7f\u544a\u200b\u8bbe\u7f6e\u200b\u548c\u200b\u79fb\u52a8\u200b\u5e94\u7528\u200b\u7684\u200b\u5e7f\u544a\u200b\u8bbe\u7f6e\u200b\u6765\u200b\u9009\u62e9\u200b\u9000\u51fa\u200bGoogle Analytics\u200b\u5e7f\u544a\u200b\u529f\u80fd\u200b\u3002 \u200b\u5176\u4ed6\u200b\u9000\u51fa\u200b\u65b9\u5f0f\u200b\u5305\u62ec\u200bhttp://optout.networkadvertising.org/\u200b\u548c\u200bhttp://www.networkadvertising.org/mobile-choice\u3002 \u200b\u6709\u5173\u200bGoogle\u200b\u9690\u79c1\u200b\u505a\u6cd5\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u200b\u8bbf\u95ee\u200bGoogle\u200b\u9690\u79c1\u200b\u4e0e\u200b\u6761\u6b3e\u200b\u3002</p>"},{"location":"zh/about/privacy/#6","title":"6. \u200b\u6211\u4eec\u200b\u4fdd\u7559\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u591a\u4e45\u200b\uff1f","text":"<p>\u200b\u7b80\u800c\u8a00\u4e4b\u200b</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u6839\u636e\u200b\u672c\u200b\u9690\u79c1\u200b\u58f0\u660e\u200b\u4e2d\u200b\u6982\u8ff0\u200b\u7684\u200b\u76ee\u7684\u200b\u4fdd\u7559\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u9664\u975e\u200b\u6cd5\u5f8b\u200b\u53e6\u6709\u200b\u8981\u6c42\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ea\u4f1a\u200b\u5728\u200b\u672c\u200b\u9690\u79c1\u200b\u58f0\u660e\u200b\u4e2d\u200b\u6982\u8ff0\u200b\u7684\u200b\u76ee\u7684\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u65f6\u95f4\u200b\u5185\u200b\u4fdd\u7559\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff0c\u200b\u9664\u975e\u200b\u6cd5\u5f8b\u200b\u8981\u6c42\u200b\u6216\u200b\u5141\u8bb8\u200b\u66f4\u957f\u200b\u7684\u200b\u4fdd\u7559\u200b\u671f\u200b\uff08\u200b\u5982\u200b\u7a0e\u52a1\u200b\u3001\u200b\u4f1a\u8ba1\u200b\u6216\u200b\u5176\u4ed6\u200b\u6cd5\u5f8b\u200b\u8981\u6c42\u200b\uff09\u3002</p> <p>\u200b\u5f53\u200b\u6211\u4eec\u200b\u6ca1\u6709\u200b\u6301\u7eed\u200b\u7684\u200b\u5408\u6cd5\u200b\u4e1a\u52a1\u200b\u9700\u8981\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5220\u9664\u200b\u6216\u200b\u533f\u540d\u200b\u5316\u5b83\u200b\uff0c\u200b\u6216\u8005\u200b\uff0c\u200b\u5982\u679c\u200b\u8fd9\u200b\u4e0d\u200b\u53ef\u80fd\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u5df2\u200b\u5b58\u50a8\u200b\u5728\u200b\u5907\u4efd\u200b\u6863\u6848\u200b\u4e2d\u200b\uff09\uff0c\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b89\u5168\u200b\u5730\u200b\u5b58\u50a8\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u4e0e\u200b\u4efb\u4f55\u200b\u8fdb\u4e00\u6b65\u200b\u5904\u7406\u200b\u9694\u79bb\u200b\uff0c\u200b\u76f4\u5230\u200b\u5220\u9664\u200b\u6210\u4e3a\u200b\u53ef\u80fd\u200b\u3002</p>"},{"location":"zh/about/privacy/#7","title":"7. \u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u4fdd\u6301\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u5b89\u5168\u200b\uff1f","text":"<p>\u200b\u7b80\u800c\u8a00\u4e4b\u200b</p> <p>\u200b\u6211\u4eec\u200b\u65e8\u5728\u200b\u901a\u8fc7\u200b\u4e00\u7cfb\u5217\u200b\u7ec4\u7ec7\u200b\u548c\u200b\u6280\u672f\u200b\u5b89\u5168\u63aa\u65bd\u200b\u4fdd\u62a4\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u200b\u5b9e\u65bd\u200b\u9002\u5f53\u200b\u7684\u200b\u6280\u672f\u200b\u548c\u200b\u7ec4\u7ec7\u200b\u5b89\u5168\u63aa\u65bd\u200b\uff0c\u200b\u65e8\u5728\u200b\u4fdd\u62a4\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u4efb\u4f55\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u7684\u200b\u5b89\u5168\u200b\u3002 \u200b\u7136\u800c\u200b\uff0c\u200b\u5c3d\u7ba1\u200b\u6211\u4eec\u200b\u91c7\u53d6\u200b\u4e86\u200b\u4fdd\u969c\u200b\u63aa\u65bd\u200b\u5e76\u200b\u52aa\u529b\u200b\u786e\u4fdd\u60a8\u200b\u7684\u200b\u4fe1\u606f\u5b89\u5168\u200b\uff0c\u200b\u4efb\u4f55\u200b\u901a\u8fc7\u200b\u4e92\u8054\u7f51\u200b\u7684\u200b\u7535\u5b50\u200b\u4f20\u8f93\u200b\u6216\u200b\u4fe1\u606f\u200b\u5b58\u50a8\u6280\u672f\u200b\u90fd\u200b\u65e0\u6cd5\u200b\u4fdd\u8bc1\u200b\u662f\u200b100%\u200b\u5b89\u5168\u200b\u7684\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u65e0\u6cd5\u200b\u627f\u8bfa\u200b\u6216\u200b\u4fdd\u8bc1\u200b\u9ed1\u5ba2\u200b\u3001\u200b\u7f51\u7edc\u200b\u72af\u7f6a\u5206\u5b50\u200b\u6216\u200b\u5176\u4ed6\u200b\u672a\u7ecf\u200b\u6388\u6743\u200b\u7684\u200b\u7b2c\u4e09\u65b9\u200b\u4e0d\u4f1a\u200b\u7834\u574f\u200b\u6211\u4eec\u200b\u7684\u200b\u5b89\u5168\u63aa\u65bd\u200b\u5e76\u200b\u4e0d\u200b\u5f53\u5730\u200b\u6536\u96c6\u200b\u3001\u200b\u8bbf\u95ee\u200b\u3001\u200b\u7a83\u53d6\u200b\u6216\u200b\u4fee\u6539\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002 \u200b\u5c3d\u7ba1\u200b\u6211\u4eec\u200b\u5c06\u200b\u5c3d\u200b\u6700\u5927\u200b\u52aa\u529b\u200b\u4fdd\u62a4\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff0c\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u5230\u200b\u6211\u4eec\u200b\u670d\u52a1\u200b\u7684\u200b\u4f20\u8f93\u200b\u548c\u200b\u4ece\u200b\u6211\u4eec\u200b\u670d\u52a1\u200b\u7684\u200b\u4f20\u8f93\u200b\u4ecd\u7136\u200b\u662f\u200b\u60a8\u200b\u81ea\u5df1\u200b\u7684\u200b\u98ce\u9669\u200b\u3002 \u200b\u60a8\u200b\u5e94\u8be5\u200b\u53ea\u200b\u5728\u200b\u5b89\u5168\u200b\u7684\u200b\u73af\u5883\u200b\u4e2d\u200b\u8bbf\u95ee\u200b\u670d\u52a1\u200b\u3002</p>"},{"location":"zh/about/privacy/#8","title":"8. \u200b\u60a8\u200b\u6709\u200b\u54ea\u4e9b\u200b\u9690\u79c1\u200b\u6743\u5229\u200b\uff1f","text":"<p>\u200b\u7b80\u800c\u8a00\u4e4b\u200b</p> <p>\u200b\u6211\u4eec\u200b\u52aa\u529b\u200b\u5728\u200b\u6cd5\u5f8b\u200b\u5141\u8bb8\u200b\u7684\u200b\u6700\u5927\u200b\u8303\u56f4\u200b\u5185\u200b\u4fdd\u62a4\u200b\u60a8\u200b\u7684\u200b\u9690\u79c1\u200b\u6743\u5229\u200b\u548c\u200b\u9009\u62e9\u200b\u3002</p> <p>\u200b\u60a8\u200b\u5728\u200b\u67d0\u4e9b\u200b\u6570\u636e\u4fdd\u62a4\u200b\u6cd5\u4e0b\u200b\u6709\u200b\u6743\u5229\u200b\u3002 \u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u6743\u5229\u200b\u4e0d\u662f\u200b\u7edd\u5bf9\u200b\u7684\u200b\uff0c\u200b\u5728\u200b\u67d0\u4e9b\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6839\u636e\u200b\u6cd5\u5f8b\u200b\u62d2\u7edd\u200b\u60a8\u200b\u7684\u200b\u8bf7\u6c42\u200b\u3002 \u200b\u8fd9\u4e9b\u200b\u6743\u5229\u200b\u5305\u62ec\u200b\uff1a</p> <ul> <li>\u200b\u77e5\u60c5\u6743\u200b   \u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b</li> <li>\u200b\u8bbf\u95ee\u200b\u6743\u200b   \u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b</li> <li>\u200b\u66f4\u6b63\u200b\u6743\u200b   \u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u4e0d\u200b\u51c6\u786e\u200b\u4fe1\u606f\u200b</li> <li>\u200b\u8bf7\u6c42\u200b\u5220\u9664\u200b\u6743\u200b   \u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b</li> <li>\u200b\u83b7\u53d6\u200b\u526f\u672c\u200b\u6743\u200b   \u200b\u60a8\u200b\u4ee5\u524d\u200b\u4e0e\u200b\u6211\u4eec\u200b\u5171\u4eab\u200b\u7684\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b</li> <li>\u200b\u53cd\u200b\u6b67\u89c6\u200b\u6743\u200b   \u200b\u9488\u5bf9\u200b\u60a8\u200b\u884c\u4f7f\u200b\u60a8\u200b\u7684\u200b\u6743\u5229\u200b</li> <li>\u200b\u9009\u62e9\u200b\u9000\u51fa\u200b\u6743\u200b</li> <li>\u200b\u5982\u679c\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b\u7528\u4e8e\u200b\u76ee\u6807\u200b\u5e7f\u544a\u200b\uff08\u200b\u6216\u200b\u6839\u636e\u200b\u9002\u7528\u6cd5\u5f8b\u200b\u5b9a\u4e49\u200b\u7684\u200b\u201c\u200b\u5206\u4eab\u200b\u201d\uff09\uff0c\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b\u7684\u200b\u9500\u552e\u200b\uff0c\u200b\u6216\u200b\u4fc3\u8fdb\u200b\u5bf9\u200b\u60a8\u200b\u4ea7\u751f\u200b\u6cd5\u5f8b\u200b\u6216\u200b\u7c7b\u4f3c\u200b\u91cd\u5927\u200b\u6548\u679c\u200b\u7684\u200b\u51b3\u7b56\u200b\uff08\u201c\u200b\u5206\u6790\u200b\u201d\uff09\u200b\u7684\u200b\u5206\u6790\u200b</li> <li>\u200b\u6536\u96c6\u200b\u901a\u8fc7\u200b\u8bed\u97f3\u200b\u6216\u200b\u9762\u90e8\u200b\u8bc6\u522b\u200b\u529f\u80fd\u200b\u64cd\u4f5c\u200b\u6536\u96c6\u200b\u7684\u200b\u654f\u611f\u6570\u636e\u200b\u548c\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b</li> <li>\u200b\u83b7\u53d6\u200b\u6743\u200b</li> <li>\u200b\u5411\u200b\u6211\u4eec\u200b\u62ab\u9732\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b\u7684\u200b\u7b2c\u4e09\u65b9\u200b\u7c7b\u522b\u200b\u7684\u200b\u5217\u8868\u200b</li> <li>\u200b\u5411\u200b\u6211\u4eec\u200b\u62ab\u9732\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b\u7684\u200b\u7279\u5b9a\u200b\u7b2c\u4e09\u65b9\u200b\u7684\u200b\u5217\u8868\u200b</li> <li>\u200b\u9650\u5236\u200b\u4f7f\u7528\u200b\u548c\u200b\u62ab\u9732\u200b\u6743\u200b   \u200b\u654f\u611f\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b</li> </ul>"},{"location":"zh/about/privacy/#_5","title":"\u5982\u4f55\u200b\u884c\u4f7f\u200b\u60a8\u200b\u7684\u200b\u6743\u5229","text":"<p>\u200b\u60a8\u200b\u51e0\u4e4e\u200b\u4e0d\u200b\u53ef\u80fd\u200b\u884c\u4f7f\u200b\u4e0a\u8ff0\u200b\u6743\u5229\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u4e0d\u200b\u4ece\u200b\u60a8\u200b\u90a3\u91cc\u200b\u6536\u96c6\u200b\u4efb\u4f55\u200b\u53ef\u200b\u8bc6\u522b\u200b\u7684\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u65e0\u6cd5\u200b\u56de\u590d\u200b\u548c\u200b\u91c7\u53d6\u200b\u6570\u636e\u200b\u4e3b\u4f53\u200b\u8bbf\u95ee\u200b\u8bf7\u6c42\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u4e0d\u200b\u4fdd\u5b58\u200b\u4efb\u4f55\u200b\u53ef\u200b\u8bc6\u522b\u200b\u7684\u200b\u5173\u4e8e\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u65e0\u6cd5\u200b\u9a8c\u8bc1\u200b\u60a8\u200b\u7684\u200b\u8eab\u4efd\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u60a8\u200b\u8ba4\u4e3a\u200b\u6211\u4eec\u200b\u975e\u6cd5\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u8054\u7cfb\u200b\u60a8\u200b\u6240\u5728\u200b\u7ba1\u8f96\u533a\u200b\u7684\u200b\u76f8\u5173\u200b\u6570\u636e\u4fdd\u62a4\u200b\u76d1\u7ba1\u200b\u673a\u6784\u200b\u3001\u200b\u5dde\u200b\u603b\u200b\u68c0\u5bdf\u957f\u200b\u6216\u200b\u5176\u4ed6\u200b\u6709\u6743\u200b\u673a\u6784\u200b\u3002</p> \u200b\u5c45\u4f4f\u5730\u200b \u200b\u673a\u6784\u200b \u200b\u6b27\u6d32\u200b\u7ecf\u6d4e\u533a\u200b \u200b\u6210\u5458\u56fd\u200b\u7684\u200b\u6570\u636e\u4fdd\u62a4\u200b\u76d1\u7763\u673a\u6784\u200b \u200b\u82f1\u56fd\u200b \u200b\u4fe1\u606f\u200b\u4e13\u5458\u200b\u529e\u516c\u5ba4\u200b \u200b\u6fb3\u5927\u5229\u4e9a\u200b \u200b\u6fb3\u5927\u5229\u4e9a\u200b\u4fe1\u606f\u200b\u4e13\u5458\u200b\u529e\u516c\u5ba4\u200b \u200b\u65b0\u897f\u5170\u200b \u200b\u65b0\u897f\u5170\u200b\u9690\u79c1\u200b\u4e13\u5458\u200b\u529e\u516c\u5ba4\u200b \u200b\u52a0\u62ff\u5927\u200b \u200b\u52a0\u62ff\u5927\u200b\u9690\u79c1\u200b\u4e13\u5458\u200b\u529e\u516c\u5ba4\u200b \u200b\u7f8e\u56fd\u200b\u52a0\u5229\u798f\u5c3c\u4e9a\u5dde\u200b \u200b\u52a0\u5229\u798f\u5c3c\u4e9a\u200b\u9690\u79c1\u200b\u4fdd\u62a4\u200b\u673a\u6784\u200b \u200b\u745e\u58eb\u200b \u200b\u8054\u90a6\u200b\u6570\u636e\u4fdd\u62a4\u200b\u548c\u200b\u4fe1\u606f\u200b\u4e13\u5458\u200b \u200b\u5357\u975e\u200b \u200b\u4fe1\u606f\u200b\u76d1\u7ba1\u200b\u673a\u6784"},{"location":"zh/about/privacy/#_6","title":"\u64a4\u56de\u200b\u60a8\u200b\u7684\u200b\u540c\u610f","text":"<p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u4f9d\u8d56\u200b\u60a8\u200b\u7684\u200b\u540c\u610f\u200b\u6765\u200b\u5904\u7406\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u662f\u200b\u660e\u786e\u200b\u7684\u200b\u548c\u200b/\u200b\u6216\u200b\u6697\u793a\u200b\u7684\u200b\u540c\u610f\u200b\uff0c\u200b\u53d6\u51b3\u4e8e\u200b\u9002\u7528\u6cd5\u5f8b\u200b\uff0c\u200b\u60a8\u200b\u6709\u6743\u200b\u968f\u65f6\u200b\u64a4\u56de\u200b\u60a8\u200b\u7684\u200b\u540c\u610f\u200b\u3002 \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u70b9\u51fb\u200b\u672c\u9875\u200b\u9876\u90e8\u200b\u6216\u200b\u4e0b\u9762\u200b\u7684\u200b\u6309\u94ae\u200b\u968f\u65f6\u200b\u64a4\u56de\u200b\u60a8\u200b\u7684\u200b\u540c\u610f\u200b\uff1a</p> <p>\u200b\u9690\u79c1\u200b\u63a7\u5236\u200b</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u200b\u4e0d\u4f1a\u200b\u5f71\u54cd\u200b\u64a4\u56de\u200b\u4e4b\u524d\u200b\u7684\u200b\u5904\u7406\u200b\u7684\u200b\u5408\u6cd5\u6027\u200b\uff0c\u200b\u4e5f\u200b\u4e0d\u4f1a\u200b\u5f71\u54cd\u200b\u5f53\u200b\u9002\u7528\u6cd5\u5f8b\u200b\u5141\u8bb8\u200b\u65f6\u200b\uff0c\u200b\u57fa\u4e8e\u200b\u9664\u200b\u540c\u610f\u200b\u4e4b\u5916\u200b\u7684\u200b\u5408\u6cd5\u200b\u5904\u7406\u200b\u7406\u7531\u200b\u8fdb\u884c\u200b\u7684\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u4fe1\u606f\u200b\u7684\u200b\u5904\u7406\u200b\u3002</p>"},{"location":"zh/about/privacy/#cookies","title":"Cookies\u200b\u548c\u200b\u7c7b\u4f3c\u200b\u6280\u672f","text":"<p>\u200b\u5927\u591a\u6570\u200b\u7f51\u7edc\u200b\u6d4f\u89c8\u5668\u200b\u9ed8\u8ba4\u8bbe\u7f6e\u200b\u4e3a\u200b\u63a5\u53d7\u200bcookies\u3002 \u200b\u5982\u679c\u200b\u60a8\u200b\u613f\u610f\u200b\uff0c\u200b\u60a8\u200b\u901a\u5e38\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u8bbe\u7f6e\u200b\u60a8\u200b\u7684\u200b\u6d4f\u89c8\u5668\u200b\u4ee5\u200b\u5220\u9664\u200b\u6216\u200b\u62d2\u7edd\u200b\u6d4f\u89c8\u5668\u200bcookies\u3002 \u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u9009\u62e9\u200b\u5220\u9664\u200b\u6216\u200b\u62d2\u7edd\u200bcookies\uff0c\u200b\u8fd9\u200b\u5c06\u200b\u4e0d\u4f1a\u200b\u5f71\u54cd\u200b\u6211\u4eec\u200b\u670d\u52a1\u200b\u7684\u200b\u53ef\u7528\u6027\u200b\u548c\u200b\u529f\u80fd\u200b\u3002</p>"},{"location":"zh/about/privacy/#9","title":"9. \u200b\u4e0d\u200b\u8ffd\u8e2a\u200b\u529f\u80fd\u200b\u7684\u200b\u63a7\u5236","text":"<p>\u200b\u5927\u591a\u6570\u200b\u7f51\u7edc\u200b\u6d4f\u89c8\u5668\u200b\u548c\u200b\u4e00\u4e9b\u200b\u79fb\u52a8\u200b\u64cd\u4f5c\u7cfb\u7edf\u200b\u548c\u200b\u79fb\u52a8\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u5305\u62ec\u200b\u4e00\u4e2a\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u6fc0\u6d3b\u200b\u7684\u200b\u4e0d\u200b\u8ffd\u8e2a\u200b\uff08\u201cDNT\u201d\uff09\u200b\u529f\u80fd\u200b\u6216\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u4ee5\u200b\u8868\u8fbe\u200b\u60a8\u200b\u7684\u200b\u9690\u79c1\u200b\u504f\u597d\u200b\uff0c\u200b\u4e0d\u200b\u5e0c\u671b\u200b\u6709\u5173\u200b\u60a8\u200b\u7684\u200b\u5728\u7ebf\u200b\u6d4f\u89c8\u200b\u6d3b\u52a8\u200b\u7684\u200b\u6570\u636e\u200b\u88ab\u200b\u76d1\u63a7\u200b\u548c\u200b\u6536\u96c6\u200b\u3002 \u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u4e3a\u200b\u8bc6\u522b\u200b\u548c\u200b\u5b9e\u65bd\u200bDNT\u200b\u4fe1\u53f7\u200b\u5236\u5b9a\u200b\u7edf\u4e00\u200b\u7684\u200b\u6280\u672f\u6807\u51c6\u200b\u3002 \u200b\u867d\u7136\u200b\u6211\u4eec\u200b\u4e0d\u80fd\u200b\u627f\u8bfa\u200b\u5c0a\u91cd\u200b\u6bcf\u200b\u4e00\u4e2a\u200bDNT\u200b\u4fe1\u53f7\u200b\uff0c\u200b\u6211\u4eec\u200b\u529b\u6c42\u200b\u5c0a\u91cd\u200b\u6240\u6709\u200b\u5728\u6280\u672f\u4e0a\u200b\u53ef\u884c\u200b\u7684\u200b\u6b64\u7c7b\u200b\u8bf7\u6c42\u200b\u3002</p> <p>\u200b\u52a0\u5229\u798f\u5c3c\u4e9a\u200b\u6cd5\u5f8b\u200b\u8981\u6c42\u200b\u6211\u4eec\u200b\u544a\u8bc9\u60a8\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u54cd\u5e94\u200b\u7f51\u7edc\u200b\u6d4f\u89c8\u5668\u200b\u7684\u200bDNT\u200b\u4fe1\u53f7\u200b\u3002 \u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u4e0d\u80fd\u200b\u4fdd\u8bc1\u200b\u8bc6\u522b\u200b\u548c\u200b\u5c0a\u91cd\u200b\u6240\u6709\u200bDNT\u200b\u4fe1\u53f7\u200b\uff0c\u200b\u6211\u4eec\u200b\u76ee\u524d\u200b\u4e0d\u200b\u5bf9\u200b\u5b83\u4eec\u200b\u505a\u51fa\u200b\u54cd\u5e94\u200b\u3002</p>"},{"location":"zh/about/privacy/#10","title":"10. \u200b\u67d0\u4e9b\u200b\u7ba1\u8f96\u533a\u200b\u7684\u200b\u5c45\u6c11\u200b\u662f\u5426\u200b\u6709\u200b\u7279\u5b9a\u200b\u7684\u200b\u9690\u79c1\u200b\u6743\u5229\u200b\uff1f","text":"<p>\u200b\u5426\u200b\u3002</p> <p>\u200b\u6240\u6709\u200b\u7537\u5973\u200b\u751f\u800c\u5e73\u7b49\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5411\u200b\u6240\u6709\u200b\u4e2a\u4eba\u200b\u63d0\u4f9b\u200b\u76f8\u540c\u200b\u7684\u200b\u9690\u79c1\u200b\u6743\u5229\u200b\uff0c\u200b\u65e0\u8bba\u200b\u4ed6\u4eec\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u5982\u4f55\u200b\u3002</p> <p>\u200b\u8bf7\u200b\u653e\u5fc3\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ee5\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u88ab\u200b\u5bf9\u5f85\u200b\u7684\u200b\u76f8\u540c\u200b\u7684\u200b\u5c0a\u91cd\u200b\u548c\u200b\u5c0a\u4e25\u200b\u5bf9\u5f85\u200b\u60a8\u200b\u3002</p>"},{"location":"zh/about/privacy/#11","title":"11. \u200b\u60a8\u200b\u5982\u4f55\u200b\u67e5\u770b\u200b\u3001\u200b\u66f4\u65b0\u200b\u6216\u200b\u5220\u9664\u200b\u6211\u4eec\u200b\u6536\u96c6\u200b\u7684\u200b\u6570\u636e\u200b\uff1f","text":"<p>\u200b\u60a8\u200b\u51e0\u4e4e\u200b\u4e0d\u200b\u53ef\u80fd\u200b\u67e5\u770b\u200b\u3001\u200b\u66f4\u65b0\u200b\u6216\u200b\u5220\u9664\u200b\u6211\u4eec\u200b\u6536\u96c6\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u4e0d\u200b\u4ece\u200b\u60a8\u200b\u90a3\u91cc\u200b\u6536\u96c6\u200b\u4efb\u4f55\u200b\u53ef\u200b\u8bc6\u522b\u200b\u7684\u200b\u4e2a\u4eba\u200b\u6570\u636e\u200b\uff0c\u200b\u4e5f\u200b\u65e0\u6cd5\u200b\u786e\u5b9a\u200b\u54ea\u4e9b\u200b\u6570\u636e\u200b\u662f\u200b\u5c5e\u4e8e\u200b\u60a8\u200b\u7684\u200b\u3002</p>"},{"location":"zh/about/privacy/#12","title":"12. \u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u4f1a\u200b\u66f4\u65b0\u200b\u6b64\u200b\u58f0\u660e\u200b\uff1f","text":"<p>\u200b\u7b80\u800c\u8a00\u4e4b\u200b</p> <p>\u200b\u662f\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u66f4\u65b0\u200b\u6b64\u200b\u58f0\u660e\u200b\u4ee5\u200b\u4fdd\u6301\u200b\u4e0e\u200b\u76f8\u5173\u200b\u6cd5\u5f8b\u200b\u7684\u200b\u4e00\u81f4\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4e0d\u65f6\u200b\u66f4\u65b0\u200b\u6b64\u200b\u9690\u79c1\u200b\u58f0\u660e\u200b\u3002 \u200b\u66f4\u65b0\u200b\u540e\u200b\u7684\u200b\u7248\u672c\u200b\u5c06\u200b\u901a\u8fc7\u200b\u66f4\u65b0\u200b\u9876\u90e8\u200b\u7684\u200b\u201c\u200b\u6700\u540e\u200b\u4fee\u8ba2\u200b\u65e5\u671f\u200b\u201d\u200b\u6765\u200b\u8868\u793a\u200b\u3002 \u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u8fdb\u884c\u200b\u4efb\u4f55\u200b\u91cd\u5927\u200b\u66f4\u6539\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u5728\u200b\u672c\u9875\u200b\u53d1\u5e03\u200b\u65b0\u200b\u7684\u200b\u9690\u79c1\u200b\u58f0\u660e\u200b\u6765\u200b\u901a\u77e5\u200b\u60a8\u200b\u3002 \u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u4e0d\u200b\u6536\u96c6\u200b\u60a8\u200b\u7684\u200b\u4efb\u4f55\u200b\u8054\u7cfb\u200b\u4fe1\u606f\u200b\uff0c\u200b\u6211\u4eec\u200b\u65e0\u6cd5\u200b\u76f4\u63a5\u200b\u901a\u77e5\u200b\u60a8\u200b\u3002 \u200b\u6211\u4eec\u200b\u9f13\u52b1\u200b\u60a8\u200b\u7ecf\u5e38\u200b\u67e5\u770b\u200b\u672c\u200b\u9690\u79c1\u200b\u58f0\u660e\u200b\uff0c\u200b\u4ee5\u200b\u4e86\u89e3\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u4fdd\u62a4\u200b\u60a8\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p>"}]}